<!DOCTYPE html>
<html class="has-navbar-fixed-top">
<head>
    <meta charset="utf-8">
<title>The Practical Quant</title>
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/outdated-browser/1.1.5/outdatedbrowser.min.css">




<meta name="description" content="">





    <meta property="og:type" content="website">
<meta property="og:title" content="The Practical Quant">
<meta property="og:url" content="http://yoursite.com/index.html">
<meta property="og:site_name" content="The Practical Quant">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="Yumeng Li">
<meta name="twitter:card" content="summary">





<link rel="icon" href="/favicon.png">


<link rel="stylesheet" href="//fonts.googleapis.com/css?family=Ovo|Source+Code+Pro">
<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/bulma/0.6.2/css/bulma.min.css">


<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/lightgallery/1.6.8/css/lightgallery.min.css">
<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/justifiedGallery/3.6.5/css/justifiedGallery.min.css">


<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/atom-one-light.min.css">


<link rel="stylesheet" href="/css/style.css">


<script defer src="//use.fontawesome.com/releases/v5.0.8/js/all.js"></script>


    
    
    
    
    
    
    
    
    
    

    


<meta name="generator" content="Hexo 5.0.0"></head>
<body>
    
<nav class="navbar is-transparent is-fixed-top navbar-main" role="navigation" aria-label="main navigation">
    <div class="container">
        <div class="navbar-brand">
            <a class="navbar-item navbar-logo" href="/">
                
                    
                    THE PRACTICAL QUANT
                    
                
            </a>
            <div class="navbar-burger">
                <span></span>
                <span></span>
                <span></span>
            </div>
        </div>
        
        <div class="navbar-menu navbar-start">
            
            <a class="navbar-item "
               href="/archives">Archives</a>
            
            <a class="navbar-item "
               href="/categories">Category</a>
            
            <a class="navbar-item "
               target="_blank" rel="noopener" href="http://allaboutmacros.com/">Blog</a>
            
            <a class="navbar-item "
               target="_blank" rel="noopener" href="https://www.linkedin.com/in/emmelineli/">Linkedin</a>
            
        </div>
        
        <div class="navbar-menu navbar-end">
            
            <a class="navbar-item search" title="Search" href="javascript:;">
                <i class="fas fa-search"></i>
            </a>
            
            
            
            <a class="navbar-item" title="GitHub" target="_blank" rel="noopener" href="https://github.com/ppoffice/hexo-theme-minos">
                
                <i class="fab fa-github"></i>
                
            </a>
               
            
        </div>
    </div>
</nav>

    <section class="section">
    <div class="container">
    
        <article class="article content gallery" itemscope itemprop="blogPost">
    <h1 class="article-title is-size-3 is-size-4-mobile" itemprop="name">
        
            <a href="/2025/10/11/legacy/" itemprop="url">A Practical Walkthrough of Using Alternative Data to Evaluate Opportunities in Recession-Resilient Niche Sectors</a>
        
    </h1>
    <div class="article-meta columns is-variable is-1 is-multiline is-mobile is-size-7-mobile">
        <span class="column is-narrow">
            <time datetime="2025-10-11T16:00:00.000Z" itemprop="datePublished">Oct 11 2025</time>
        </span>
        
        <span class="column is-narrow article-category">
            <i class="far fa-folder"></i>
            <a class="article-category-link" href="/categories/Alternative-Data/">Alternative Data</a>
        </span>
        
        
        <span class="column is-narrow">
            
            
            35 minutes read (About 5183 words)
        </span>
        
    </div>
    <div class="article-entry is-size-6-mobile" itemprop="articleBody">
    
        <html><head></head><body><p>When I first look at these credit investment opportunities, the challenge was immediately clear. Our team was evaluating two legacy services chains in Florida, yet almost none of the analytical infrastructure one might expect&#x2014;standardized operating metrics, historical statements, market share data&#x2014;was available. Unlike more transparent service sectors, this industry provides no obvious datapoints to anchor intuition or build a conventional model.</p>
<p>But the nice thing about data work is: if you don&#x2019;t have intuition, you can build a framework.</p>
<p>This post walks through a simple, self-contained demo on how I used alternative data to compare two legacy service chains for a hypothetical private credit investment. All data here are simulated and anonymized; the goal is not to value real companies, but to show how to turn a &#x201C;no idea where to start&#x201D; situation into a structured, defensible analysis. My hope is that it provides inspiration for approaching private-credit opportunities in niche service industries where traditional information is limited.</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">chain_attributes.head()</span><br></pre></td></tr></tbody></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;</code></pre>
<p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>chain</th>
      <th>name</th>
      <th>address</th>
      <th>zip</th>
      <th>usps_zip_pref_state</th>
      <th>latitude</th>
      <th>longitude</th>
      <th>revenue</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>Eternal Rest Services</td>
      <td>Seawinds Funeral Home &amp; Crematory - Sebastian</td>
      <td>735 S Fleming St, Sebastian</td>
      <td>32958</td>
      <td>FL</td>
      <td>27.7913</td>
      <td>-80.4838</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>1</th>
      <td>Eternal Rest Services</td>
      <td>Lewis-Ray Mortuary Inc</td>
      <td>1595 S Hopkins Ave, Titusville</td>
      <td>32780</td>
      <td>FL</td>
      <td>28.5964</td>
      <td>-80.8076</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>2</th>
      <td>Eternal Rest Services</td>
      <td>Beth David Memorial Gardens / Levitt Weinstein...</td>
      <td>3201 N 72nd Ave, Hollywood</td>
      <td>33024</td>
      <td>FL</td>
      <td>26.0373</td>
      <td>-80.2325</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>3</th>
      <td>Eternal Rest Services</td>
      <td>Sorensen Funeral Home</td>
      <td>3180 30th Ave N, St. Petersburg</td>
      <td>33713</td>
      <td>FL</td>
      <td>27.7988</td>
      <td>-82.6770</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>4</th>
      <td>Eternal Rest Services</td>
      <td>Brewer &amp; Sons Funeral Homes - Spring Hill Chapel</td>
      <td>4450 Commercial Way, Spring Hill</td>
      <td>34606</td>
      <td>FL</td>
      <td>28.4939</td>
      <td>-82.5962</td>
      <td>NaN</td>
    </tr>
  </tbody>
</table>
</div>



<h2 id="Confronting-an-opaque-sector-with-minimal-initial-intuition"><a href="#Confronting-an-opaque-sector-with-minimal-initial-intuition" class="headerlink" title="Confronting an opaque sector with minimal initial intuition"></a>Confronting an opaque sector with minimal initial intuition</h2><p>My first attempt at framing the problem was straightforward but ultimately insufficient. I began with the assumption that local population size should correlate with demand, and therefore revenue, in a necessity-based service. The logic felt self-evident.</p>
<p>Yet once I tested this idea against the reference dataset where one comparable chain provided actual revenue data&#x2014;the relationship collapsed almost immediately. Dense ZIP codes did not necessarily produce higher revenue, and several low-population regions performed unexpectedly well. This mismatch signaled that the core driver of performance was not simply &#x201C;how many people live nearby.&#x201D; It forced a re-examination of what demand actually means in this context.</p>
<h2 id="Refining-the-definition-of-demand"><a href="#Refining-the-definition-of-demand" class="headerlink" title="Refining the definition of demand"></a>Refining the definition of demand</h2><p>A more nuanced view is that not all population contributes equally to demand. To approximate the actual service need in each market, I began assembling a dataset that did not exist in any ready-made form. I pulled <strong>age-bucket distributions from the American Community Survey at the ZIP-code level</strong>, then matched each location to its corresponding county in order to merge in <strong>CDC crude mortality rates by age group</strong>. Only after aligning these disparate geographies&#x2014;coordinates &#x2192; ZIP &#x2192; county&#x2014;was it possible to compute a more meaningful proxy: the expected number of annual events within each location&#x2019;s true catchment area.</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd</span><br><span class="line"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np</span><br><span class="line"><span class="hljs-keyword">import</span> statsmodels.formula.api <span class="hljs-keyword">as</span> smf</span><br><span class="line"><span class="hljs-keyword">import</span> statsmodels.api <span class="hljs-keyword">as</span> sm</span><br><span class="line"><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt </span><br><span class="line"><span class="hljs-keyword">import</span> cartopy.crs <span class="hljs-keyword">as</span> ccrs</span><br><span class="line"><span class="hljs-keyword">import</span> cartopy.feature <span class="hljs-keyword">as</span> cfeature</span><br><span class="line"><span class="hljs-keyword">from</span> math <span class="hljs-keyword">import</span> radians, cos, sin, asin, sqrt</span><br></pre></td></tr></tbody></table></figure>

<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-comment"># Load the datasets</span></span><br><span class="line">chain_attributes = pd.read_csv(<span class="hljs-string">&apos;chain_attributes.csv&apos;</span>,dtype={<span class="hljs-string">&apos;zip&apos;</span>: str})</span><br><span class="line">zipcode_county_mapping = pd.read_csv(<span class="hljs-string">&apos;zipcode_county_mapping.csv&apos;</span>,dtype={<span class="hljs-string">&apos;zipcode&apos;</span>: str})</span><br><span class="line"><span class="hljs-comment"># the American Community Survey (ACS). Downloaded from https://data.census.gov/table/ACSDP5Y2023.DP05?t=Older+Population&amp;g=040XX00US12$8600000</span></span><br><span class="line">dp03 = pd.read_csv(<span class="hljs-string">&apos;DP03_by_county.csv&apos;</span>)</span><br><span class="line">dp05 = pd.read_csv(<span class="hljs-string">&apos;DP05_by_county.csv&apos;</span>)</span><br><span class="line"><span class="hljs-comment">#CDC crude mortality rates by age group by county. Downloaded from https://wonder.cdc.gov/controller/datarequest/D76;jsessionid=6FAF78721C8F4B417A0EF2BBEDE8</span></span><br><span class="line">crude = pd.read_csv(<span class="hljs-string">&apos;crude_rate.csv&apos;</span>,dtype={<span class="hljs-string">&apos;county code&apos;</span>: str})</span><br></pre></td></tr></tbody></table></figure>


<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-comment"># clean up ACS data</span></span><br><span class="line">dp = pd.merge(dp03, dp05, on=[<span class="hljs-string">&apos;Geography&apos;</span>, <span class="hljs-string">&apos;Geographic Area Name&apos;</span>], how=<span class="hljs-string">&apos;outer&apos;</span>)</span><br><span class="line">dp.columns = [<span class="hljs-string">&apos;Geography&apos;</span>, <span class="hljs-string">&apos;Geographic Area Name&apos;</span>, <span class="hljs-string">&apos;Median household income&apos;</span>,<span class="hljs-string">&apos;Mean household income&apos;</span>] + [<span class="hljs-string">&apos;Population &apos;</span>+ str(age) <span class="hljs-keyword">for</span> age <span class="hljs-keyword">in</span> dp.columns[<span class="hljs-number">4</span>:]]</span><br><span class="line">dp[<span class="hljs-string">&apos;county&apos;</span>] = dp.iloc[:, <span class="hljs-number">1</span>].apply(<span class="hljs-keyword">lambda</span> x: x.split(<span class="hljs-string">&apos;, &apos;</span>)[<span class="hljs-number">0</span>].replace(<span class="hljs-string">&apos; County&apos;</span>, <span class="hljs-string">&apos;&apos;</span>))</span><br><span class="line">dp[<span class="hljs-string">&apos;state&apos;</span>] = dp.iloc[:, <span class="hljs-number">1</span>].apply(<span class="hljs-keyword">lambda</span> x: x.split(<span class="hljs-string">&apos;, &apos;</span>)[<span class="hljs-number">1</span>])</span><br><span class="line">dp[<span class="hljs-string">&apos;county code&apos;</span>] = dp.iloc[:, <span class="hljs-number">0</span>].apply(<span class="hljs-keyword">lambda</span> x: x.split(<span class="hljs-string">&apos;US&apos;</span>)[<span class="hljs-number">1</span>])</span><br><span class="line"></span><br><span class="line"><span class="hljs-comment"># clean up crude rate data</span></span><br><span class="line"><span class="hljs-comment"># Pivot the data to reorganize by county and county code, with Ten-Year Age Groups and Crude Rates as columns</span></span><br><span class="line">pivoted_crude = crude.pivot_table(index=[<span class="hljs-string">&apos;county code&apos;</span>], </span><br><span class="line">                                columns=<span class="hljs-string">&apos;Ten-Year Age Groups&apos;</span>, </span><br><span class="line">                                values=<span class="hljs-string">&apos;Crude Rate Per 100,000&apos;</span>, </span><br><span class="line">                                aggfunc=<span class="hljs-string">&apos;first&apos;</span>)</span><br><span class="line">pivoted_crude.reset_index(inplace=<span class="hljs-literal">True</span>)</span><br><span class="line">pivoted_crude.columns = [<span class="hljs-string">&apos;county code&apos;</span>] + [<span class="hljs-string">&apos;Crude Rate &apos;</span>+ str(age) <span class="hljs-keyword">for</span> age <span class="hljs-keyword">in</span> pivoted_crude.columns[<span class="hljs-number">1</span>:]]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="hljs-comment"># Merge attributes with zip-county map</span></span><br><span class="line">data = pd.merge(chain_attributes, zipcode_county_mapping, left_on=<span class="hljs-string">&apos;zip&apos;</span>, right_on=<span class="hljs-string">&apos;zipcode&apos;</span>, how=<span class="hljs-string">&apos;left&apos;</span>)</span><br><span class="line"><span class="hljs-comment"># Merge data with DP03 and DP05 based on the state/county columns</span></span><br><span class="line">data = pd.merge(data, dp, on=[<span class="hljs-string">&apos;state&apos;</span>,<span class="hljs-string">&apos;county&apos;</span>], how=<span class="hljs-string">&apos;left&apos;</span>)</span><br><span class="line"><span class="hljs-comment"># Merge data with crude rate based on the county column</span></span><br><span class="line">data = pd.merge(data, pivoted_crude, on=[<span class="hljs-string">&apos;county code&apos;</span>], how=<span class="hljs-string">&apos;left&apos;</span>)</span><br><span class="line"></span><br><span class="line"><span class="hljs-comment"># Check and display the rows that were not merged</span></span><br><span class="line">unmerged_rows = data[data[<span class="hljs-string">&apos;Geography&apos;</span>].isna()]</span><br><span class="line"><span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> unmerged_rows.empty:</span><br><span class="line">    print(<span class="hljs-string">f&quot;There are <span class="hljs-subst">{len(unmerged_rows)}</span> rows that did not merge successfully.&quot;</span>)</span><br><span class="line"><span class="hljs-keyword">else</span>:</span><br><span class="line">    print(<span class="hljs-string">&quot;All rows have been successfully merged.&quot;</span>)</span><br></pre></td></tr></tbody></table></figure>

<pre><code>All rows have been successfully merged.</code></pre>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-comment"># Drop columns that will not be used</span></span><br><span class="line">df = data.drop([<span class="hljs-string">&apos;state_fips&apos;</span>,<span class="hljs-string">&apos;state_abbr&apos;</span>, <span class="hljs-string">&apos;zipcode&apos;</span>,<span class="hljs-string">&apos;Geography&apos;</span>, <span class="hljs-string">&apos;Geographic Area Name&apos;</span>], axis=<span class="hljs-number">1</span>)</span><br><span class="line">df = df.drop([<span class="hljs-string">&apos;Population Under 5 years&apos;</span>, <span class="hljs-string">&apos;Population 5 to 9 years&apos;</span>,</span><br><span class="line">               <span class="hljs-string">&apos;Population 10 to 14 years&apos;</span>, <span class="hljs-string">&apos;Population 15 to 19 years&apos;</span>,</span><br><span class="line">               <span class="hljs-string">&apos;Population 20 to 24 years&apos;</span>, <span class="hljs-string">&apos;Population 25 to 34 years&apos;</span>,</span><br><span class="line">               <span class="hljs-string">&apos;Population 35 to 44 years&apos;</span>, <span class="hljs-string">&apos;Population 45 to 54 years&apos;</span>,</span><br><span class="line">               <span class="hljs-string">&apos;Population 55 to 59 years&apos;</span>, <span class="hljs-string">&apos;Population 60 to 64 years&apos;</span>,</span><br><span class="line">               <span class="hljs-string">&apos;Crude Rate 1-4 years&apos;</span>, <span class="hljs-string">&apos;Crude Rate 15-24 years&apos;</span>,</span><br><span class="line">               <span class="hljs-string">&apos;Crude Rate 25-34 years&apos;</span>, <span class="hljs-string">&apos;Crude Rate 35-44 years&apos;</span>,</span><br><span class="line">               <span class="hljs-string">&apos;Crude Rate 45-54 years&apos;</span>, <span class="hljs-string">&apos;Crude Rate 5-14 years&apos;</span>,</span><br><span class="line">               <span class="hljs-string">&apos;Crude Rate 55-64 years&apos;</span>,<span class="hljs-string">&apos;Crude Rate &lt; 1 year&apos;</span>,<span class="hljs-string">&apos;Crude Rate Not Stated&apos;</span>], axis=<span class="hljs-number">1</span>)</span><br><span class="line"><span class="hljs-comment"># correct data types</span></span><br><span class="line">df[[<span class="hljs-string">&apos;Crude Rate 65-74 years&apos;</span>,<span class="hljs-string">&apos;Crude Rate 75-84 years&apos;</span>, <span class="hljs-string">&apos;Crude Rate 85+ years&apos;</span>,<span class="hljs-string">&apos;Median household income&apos;</span>, <span class="hljs-string">&apos;Mean household income&apos;</span>]]= df[[<span class="hljs-string">&apos;Crude Rate 65-74 years&apos;</span>,<span class="hljs-string">&apos;Crude Rate 75-84 years&apos;</span>, <span class="hljs-string">&apos;Crude Rate 85+ years&apos;</span>,<span class="hljs-string">&apos;Median household income&apos;</span>, <span class="hljs-string">&apos;Mean household income&apos;</span>]].astype(<span class="hljs-string">&apos;float&apos;</span>)</span><br></pre></td></tr></tbody></table></figure>

<p>I translated demographic structure into an estimate of actual annual events. Mortality data from the CDC are reported as crude rates per 100,000 people for each age bracket, so the calculation effectively becomes a weighted sum of age-specific risk. For each ZIP, I multiplied the population in each senior age group by the corresponding crude mortality rate, scaled the result back to actual counts, and then aggregated across age brackets. The resulting field&#x2014;Estimated deaths&#x2014;provides a first-order approximation of the true service demand in each catchment area.</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-comment"># estimated death = population * crude rate</span></span><br><span class="line">df[<span class="hljs-string">&apos;Estimated deaths 65 to 74 years&apos;</span>] = ((df[<span class="hljs-string">&apos;Population 65 to 74 years&apos;</span>] * df[<span class="hljs-string">&apos;Crude Rate 65-74 years&apos;</span>]) / <span class="hljs-number">100000</span>).round(<span class="hljs-number">0</span>)</span><br><span class="line">df[<span class="hljs-string">&apos;Estimated deaths 75 to 84 years&apos;</span>] = ((df[<span class="hljs-string">&apos;Population 75 to 84 years&apos;</span>] * df[<span class="hljs-string">&apos;Crude Rate 75-84 years&apos;</span>]) / <span class="hljs-number">100000</span>).round(<span class="hljs-number">0</span>)</span><br><span class="line">df[<span class="hljs-string">&apos;Estimated deaths 85 years and over&apos;</span>] = ((df[<span class="hljs-string">&apos;Population 85 years and over&apos;</span>] * df[<span class="hljs-string">&apos;Crude Rate 85+ years&apos;</span>]) / <span class="hljs-number">100000</span>).round(<span class="hljs-number">0</span>)</span><br><span class="line">df[<span class="hljs-string">&apos;Estimated deaths&apos;</span>] = (df[<span class="hljs-string">&apos;Estimated deaths 65 to 74 years&apos;</span>] + df[<span class="hljs-string">&apos;Estimated deaths 75 to 84 years&apos;</span>]+ df[<span class="hljs-string">&apos;Estimated deaths 85 years and over&apos;</span>]).round(<span class="hljs-number">0</span>)</span><br></pre></td></tr></tbody></table></figure>

<p>This proxy proved substantially closer to the underlying demand function than raw population counts when comparing with one chain in a different state but with actual revenue data. But while the measure improved explanatory power, it still did not explain why sites with nearly identical expected-event profiles performed so differently. Revenue dispersion within the reference chain remained wide. It became clear that another structural factor&#x2014;something demographic data alone could not capture&#x2014;was driving the variation.</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">df[[<span class="hljs-string">&apos;zip&apos;</span>,  <span class="hljs-string">&apos;Population 65 to 74 years&apos;</span>,<span class="hljs-string">&apos;Crude Rate 65-74 years&apos;</span>, <span class="hljs-string">&apos;Population 75 to 84 years&apos;</span>,<span class="hljs-string">&apos;Crude Rate 75-84 years&apos;</span>,</span><br><span class="line">       <span class="hljs-string">&apos;Population 85 years and over&apos;</span>, <span class="hljs-string">&apos;Crude Rate 85+ years&apos;</span>, <span class="hljs-string">&apos;Estimated deaths&apos;</span>, <span class="hljs-string">&apos;revenue&apos;</span>]][<span class="hljs-number">50</span>:<span class="hljs-number">60</span>]</span><br></pre></td></tr></tbody></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;</code></pre>
<p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>zip</th>
      <th>Population 65 to 74 years</th>
      <th>Crude Rate 65-74 years</th>
      <th>Population 75 to 84 years</th>
      <th>Crude Rate 75-84 years</th>
      <th>Population 85 years and over</th>
      <th>Crude Rate 85+ years</th>
      <th>Estimated deaths</th>
      <th>revenue</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>50</th>
      <td>28105</td>
      <td>82170</td>
      <td>1667.1</td>
      <td>35797</td>
      <td>4417.0</td>
      <td>14314</td>
      <td>13249.3</td>
      <td>4848.0</td>
      <td>1443785.90</td>
    </tr>
    <tr>
      <th>51</th>
      <td>28715</td>
      <td>33804</td>
      <td>1673.3</td>
      <td>15735</td>
      <td>4515.7</td>
      <td>7033</td>
      <td>14083.6</td>
      <td>2267.0</td>
      <td>1142504.96</td>
    </tr>
    <tr>
      <th>52</th>
      <td>28301</td>
      <td>25938</td>
      <td>2324.2</td>
      <td>12208</td>
      <td>5129.5</td>
      <td>4598</td>
      <td>13700.4</td>
      <td>1859.0</td>
      <td>933859.23</td>
    </tr>
    <tr>
      <th>53</th>
      <td>27292</td>
      <td>18861</td>
      <td>2247.5</td>
      <td>9673</td>
      <td>5438.5</td>
      <td>3306</td>
      <td>15263.5</td>
      <td>1455.0</td>
      <td>1104161.22</td>
    </tr>
    <tr>
      <th>54</th>
      <td>27604</td>
      <td>89427</td>
      <td>1405.2</td>
      <td>39811</td>
      <td>4244.9</td>
      <td>14816</td>
      <td>13564.1</td>
      <td>4957.0</td>
      <td>902405.14</td>
    </tr>
    <tr>
      <th>55</th>
      <td>30134</td>
      <td>11119</td>
      <td>2198.6</td>
      <td>4615</td>
      <td>5267.5</td>
      <td>1732</td>
      <td>15275.1</td>
      <td>752.0</td>
      <td>1046596.19</td>
    </tr>
    <tr>
      <th>56</th>
      <td>30046</td>
      <td>68686</td>
      <td>1472.3</td>
      <td>28960</td>
      <td>4319.0</td>
      <td>7830</td>
      <td>14309.8</td>
      <td>3382.0</td>
      <td>783832.30</td>
    </tr>
    <tr>
      <th>57</th>
      <td>30092</td>
      <td>68686</td>
      <td>1472.3</td>
      <td>28960</td>
      <td>4319.0</td>
      <td>7830</td>
      <td>14309.8</td>
      <td>3382.0</td>
      <td>1305180.42</td>
    </tr>
    <tr>
      <th>58</th>
      <td>31404</td>
      <td>29473</td>
      <td>1966.1</td>
      <td>14405</td>
      <td>4595.0</td>
      <td>4936</td>
      <td>13413.5</td>
      <td>1903.0</td>
      <td>1264724.73</td>
    </tr>
    <tr>
      <th>59</th>
      <td>30060</td>
      <td>64020</td>
      <td>1532.7</td>
      <td>28231</td>
      <td>4505.9</td>
      <td>9642</td>
      <td>14546.6</td>
      <td>3656.0</td>
      <td>946321.92</td>
    </tr>
  </tbody>
</table>
</div>




<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-comment">### plot comparison of revenue vs. estimated death</span></span><br><span class="line">df_legacy_fl = df[(df[<span class="hljs-string">&apos;chain&apos;</span>] == <span class="hljs-string">&apos;Legacy Memorial Services&apos;</span>) &amp; (df[<span class="hljs-string">&apos;state&apos;</span>] == <span class="hljs-string">&apos;Florida&apos;</span>)].copy()</span><br><span class="line"><span class="hljs-comment"># Sort (descending) by revenue, so the highest-revenue location appears first</span></span><br><span class="line">df_legacy_fl.sort_values(by=<span class="hljs-string">&apos;revenue&apos;</span>, ascending=<span class="hljs-literal">False</span>, inplace=<span class="hljs-literal">True</span>)</span><br><span class="line">df_legacy_fl.reset_index(drop=<span class="hljs-literal">True</span>, inplace=<span class="hljs-literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="hljs-comment"># Create the Plot</span></span><br><span class="line">x = np.arange(len(df_legacy_fl))  <span class="hljs-comment"># x-coordinates for each home</span></span><br><span class="line">fig, ax1 = plt.subplots(figsize=(<span class="hljs-number">10</span>, <span class="hljs-number">4</span>))</span><br><span class="line"></span><br><span class="line"><span class="hljs-comment"># Bar Chart for Revenue </span></span><br><span class="line">bar_width = <span class="hljs-number">0.6</span></span><br><span class="line">ax1.bar(x, df_legacy_fl[<span class="hljs-string">&apos;revenue&apos;</span>], width=bar_width, label=<span class="hljs-string">&apos;Revenue&apos;</span>)</span><br><span class="line">ax1.set_ylabel(<span class="hljs-string">&quot;Revenue (USD)&quot;</span>)</span><br><span class="line">ax1.set_xlabel(<span class="hljs-string">&quot;Home (Sorted by Revenue)&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="hljs-comment"># For aesthetic labeling, show home indices or names:</span></span><br><span class="line">ax1.set_xticks(x)</span><br><span class="line">ax1.set_xticklabels(x + <span class="hljs-number">1</span>) </span><br><span class="line"></span><br><span class="line"><span class="hljs-comment"># Line Plot for Estimated Death</span></span><br><span class="line">ax2 = ax1.twinx()  <span class="hljs-comment"># create a twin axis sharing the same x-axis</span></span><br><span class="line">ax2.plot(x, df_legacy_fl[<span class="hljs-string">&apos;Estimated deaths&apos;</span>], color = <span class="hljs-string">&apos;black&apos;</span>, marker=<span class="hljs-string">&apos;o&apos;</span>, linewidth=<span class="hljs-number">1</span>, label=<span class="hljs-string">&apos;Estimated Death&apos;</span>)</span><br><span class="line">ax2.set_ylabel(<span class="hljs-string">&quot;Estimated Death&quot;</span>)</span><br><span class="line"></span><br><span class="line">plt.title(<span class="hljs-string">&quot;Comparison of Revenue vs. Estimated Death\n(Legacy Memorial Services - Florida Homes)&quot;</span>)</span><br><span class="line">plt.tight_layout()</span><br><span class="line">plt.show()</span><br></pre></td></tr></tbody></table></figure>



<img src="http://yumeng-li.github.io/legacy_files/legacy_16_0.png">



<h2 id="Competition-determines-revenue-more-than-raw-demand"><a href="#Competition-determines-revenue-more-than-raw-demand" class="headerlink" title="Competition determines revenue more than raw demand"></a>Competition determines revenue more than raw demand</h2><p>The turning point occurred when I spatially mapped all sites and overlaid nearby competitors. Only then did the inconsistencies resolve.</p>
<p>Two locations with nearly identical demographic and demand characteristics behaved very differently when their competitive environments diverged. A near-monopoly footprint materially outperformed markets with multiple service providers.</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-comment">### plot locations on FL map to see the competitive dynamic</span></span><br><span class="line">df_fl = df[df[<span class="hljs-string">&apos;state&apos;</span>] == <span class="hljs-string">&apos;Florida&apos;</span>]</span><br><span class="line">colors = {</span><br><span class="line">    <span class="hljs-string">&apos;Eternal Rest Services&apos;</span>: <span class="hljs-string">&apos;red&apos;</span>,</span><br><span class="line">    <span class="hljs-string">&apos;Forever Peace Homes&apos;</span>: <span class="hljs-string">&apos;blue&apos;</span>,</span><br><span class="line">    <span class="hljs-string">&apos;Legacy Memorial Services&apos;</span>: <span class="hljs-string">&apos;green&apos;</span></span><br><span class="line">}</span><br><span class="line"><span class="hljs-keyword">if</span> <span class="hljs-string">&apos;chain&apos;</span> <span class="hljs-keyword">in</span> df_fl.columns:</span><br><span class="line">    df_fl[<span class="hljs-string">&apos;color&apos;</span>] = df_fl[<span class="hljs-string">&apos;chain&apos;</span>].map(colors)</span><br><span class="line"><span class="hljs-keyword">else</span>:</span><br><span class="line">    df_fl[<span class="hljs-string">&apos;color&apos;</span>] = <span class="hljs-string">&apos;purple&apos;</span></span><br><span class="line"></span><br><span class="line"><span class="hljs-comment"># Create a Matplotlib figure with a Cartopy projection</span></span><br><span class="line">plt.figure(figsize=(<span class="hljs-number">10</span>, <span class="hljs-number">8</span>))</span><br><span class="line">ax = plt.axes(projection=ccrs.PlateCarree())</span><br><span class="line"></span><br><span class="line"><span class="hljs-comment"># Set the geographic extent to cover Florida</span></span><br><span class="line">ax.set_extent([<span class="hljs-number">-88</span>, <span class="hljs-number">-80</span>, <span class="hljs-number">24</span>, <span class="hljs-number">32</span>], crs=ccrs.PlateCarree())</span><br><span class="line"></span><br><span class="line"><span class="hljs-comment"># Add map features for context: coastlines, national borders, and state boundaries</span></span><br><span class="line">ax.add_feature(cfeature.COASTLINE)</span><br><span class="line">ax.add_feature(cfeature.BORDERS, linestyle=<span class="hljs-string">&apos;:&apos;</span>)</span><br><span class="line">ax.add_feature(cfeature.STATES, edgecolor=<span class="hljs-string">&apos;gray&apos;</span>)</span><br><span class="line"></span><br><span class="line"><span class="hljs-comment"># Plot the Florida home locations</span></span><br><span class="line"><span class="hljs-keyword">if</span> <span class="hljs-string">&apos;chain&apos;</span> <span class="hljs-keyword">in</span> df_fl.columns:</span><br><span class="line">    <span class="hljs-keyword">for</span> chain, group <span class="hljs-keyword">in</span> df_fl.groupby(<span class="hljs-string">&apos;chain&apos;</span>):</span><br><span class="line">        ax.scatter(</span><br><span class="line">            group[<span class="hljs-string">&apos;longitude&apos;</span>],</span><br><span class="line">            group[<span class="hljs-string">&apos;latitude&apos;</span>],</span><br><span class="line">            label=chain,</span><br><span class="line">            color=colors.get(chain, <span class="hljs-string">&apos;purple&apos;</span>),</span><br><span class="line">            s=<span class="hljs-number">100</span>,</span><br><span class="line">            alpha=<span class="hljs-number">0.7</span>,</span><br><span class="line">            edgecolor=<span class="hljs-string">&apos;k&apos;</span>,</span><br><span class="line">            transform=ccrs.PlateCarree()</span><br><span class="line">        )</span><br><span class="line"><span class="hljs-keyword">else</span>:</span><br><span class="line">    ax.scatter(</span><br><span class="line">        df_fl[<span class="hljs-string">&apos;longitude&apos;</span>],</span><br><span class="line">        df_fl[<span class="hljs-string">&apos;latitude&apos;</span>],</span><br><span class="line">        label=<span class="hljs-string">&apos;Funeral Home&apos;</span>,</span><br><span class="line">        color=<span class="hljs-string">&apos;purple&apos;</span>,</span><br><span class="line">        s=<span class="hljs-number">100</span>,</span><br><span class="line">        alpha=<span class="hljs-number">0.7</span>,</span><br><span class="line">        edgecolor=<span class="hljs-string">&apos;k&apos;</span>,</span><br><span class="line">        transform=ccrs.PlateCarree()</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">plt.title(<span class="hljs-string">&quot;Funeral Home Locations in Florida&quot;</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></tbody></table></figure>

<pre><code>/var/folders/8t/vzks7t793bq9w_c_km9_4f2h0000gn/T/ipykernel_63287/4228247440.py:16: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  df_fl[&apos;color&apos;] = df_fl[&apos;chain&apos;].map(colors)</code></pre>
<img src="http://yumeng-li.github.io/legacy_files/legacy_18_0.png">



<p>The map below visualizes the reference chain across Florida, and it quickly illustrates why demographic data alone could not explain revenue dispersion. Each bubble represents a county, with bubble size proportional to estimated annual events (derived from senior population &#xD7; county-level mortality rates) and bubble color reflecting actual 2023 revenue for the nearest location.</p>
<p>What immediately stands out is the lack of a clean relationship between demand potential and realized performance. Several counties with sizeable senior populations and high expected-event estimates (large bubbles) exhibit only mid-range revenues. Conversely, some moderately sized markets but without any competitors nearby outperform expectations despite smaller demand pools.</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-comment">### further plot estimated death and revenue for &quot;Legacy Memorial Services&quot;</span></span><br><span class="line">fig = plt.figure(figsize=(<span class="hljs-number">12</span>, <span class="hljs-number">10</span>))</span><br><span class="line">ax = plt.axes(projection=ccrs.PlateCarree())</span><br><span class="line"></span><br><span class="line">ax.set_extent([<span class="hljs-number">-88</span>, <span class="hljs-number">-78</span>, <span class="hljs-number">24</span>, <span class="hljs-number">32</span>], crs=ccrs.PlateCarree())</span><br><span class="line"></span><br><span class="line">ax.add_feature(cfeature.COASTLINE)</span><br><span class="line">ax.add_feature(cfeature.BORDERS, linestyle=<span class="hljs-string">&apos;:&apos;</span>)</span><br><span class="line">ax.add_feature(cfeature.STATES, edgecolor=<span class="hljs-string">&apos;black&apos;</span>)</span><br><span class="line"></span><br><span class="line">chains = df_fl[<span class="hljs-string">&apos;chain&apos;</span>].unique()</span><br><span class="line">chain_colors = {</span><br><span class="line">    <span class="hljs-string">&quot;Eternal Rest Services&quot;</span>: <span class="hljs-string">&quot;red&quot;</span>,</span><br><span class="line">    <span class="hljs-string">&quot;Forever Peace Homes&quot;</span>: <span class="hljs-string">&quot;blue&quot;</span></span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="hljs-keyword">for</span> chain <span class="hljs-keyword">in</span> chains:</span><br><span class="line">    <span class="hljs-keyword">if</span> chain != <span class="hljs-string">&quot;Legacy Memorial Services&quot;</span>:</span><br><span class="line">        <span class="hljs-comment"># For the other chains, plot points in a fixed color.</span></span><br><span class="line">        chain_df = df_fl[df_fl[<span class="hljs-string">&apos;chain&apos;</span>] == chain]</span><br><span class="line">        ax.scatter(chain_df[<span class="hljs-string">&apos;longitude&apos;</span>], chain_df[<span class="hljs-string">&apos;latitude&apos;</span>],</span><br><span class="line">                   color=chain_colors.get(chain, <span class="hljs-string">&quot;gray&quot;</span>),</span><br><span class="line">                   s=<span class="hljs-number">50</span>,</span><br><span class="line">                   label=chain,</span><br><span class="line">                   transform=ccrs.PlateCarree())</span><br><span class="line">        </span><br><span class="line"><span class="hljs-comment"># For &quot;Legacy Memorial Services&quot;, use the estimated death as the marker size and revenue for its color.</span></span><br><span class="line">legacy_df = df_fl[df_fl[<span class="hljs-string">&apos;chain&apos;</span>] == <span class="hljs-string">&quot;Legacy Memorial Services&quot;</span>]</span><br><span class="line"></span><br><span class="line"><span class="hljs-comment"># Set a scale factor for the estimated death to get marker sizes (adjust as needed)</span></span><br><span class="line">size_factor = <span class="hljs-number">0.3</span></span><br><span class="line">marker_sizes = legacy_df[<span class="hljs-string">&apos;Estimated deaths&apos;</span>] * size_factor</span><br><span class="line"></span><br><span class="line"><span class="hljs-comment"># Scatter the Legacy points with a colormap for revenue.</span></span><br><span class="line">legacy_scatter = ax.scatter(legacy_df[<span class="hljs-string">&apos;longitude&apos;</span>], legacy_df[<span class="hljs-string">&apos;latitude&apos;</span>],</span><br><span class="line">                            c=legacy_df[<span class="hljs-string">&apos;revenue&apos;</span>], s=marker_sizes,</span><br><span class="line">                            cmap=<span class="hljs-string">&apos;RdYlGn&apos;</span>, alpha=<span class="hljs-number">0.6</span>, edgecolor=<span class="hljs-string">&apos;k&apos;</span>,</span><br><span class="line">                            label=<span class="hljs-string">&quot;Legacy Memorial Services&quot;</span>,</span><br><span class="line">                            transform=ccrs.PlateCarree())</span><br><span class="line"></span><br><span class="line">cbar = plt.colorbar(legacy_scatter, ax=ax, orientation=<span class="hljs-string">&apos;vertical&apos;</span>, shrink=<span class="hljs-number">0.7</span>)</span><br><span class="line">cbar.set_label(<span class="hljs-string">&quot;Revenue&quot;</span>)</span><br><span class="line">plt.title(<span class="hljs-string">&quot;Legacy Memorial Services\n(bubble size = Estimated Death of County, bubble color = Revenue)&quot;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></tbody></table></figure>



<img src="http://yumeng-li.github.io/legacy_files/legacy_20_0.png">



<p>This led to a critical insight:</p>
<p><strong>Performance is driven less by total demand and more by demand per available provider.</strong></p>
<p>To quantify this, I constructed a measure that ultimately became central to the analysis:<br>&#x201C;death share,&#x201D; defined as expected annual events divided by the number of competitors within a defined radius.</p>
<p>While conceptually simple, it captured competitive intensity more effectively than any other variable and proved to be the strongest empirical predictor of revenue.</p>
<p>First, I reconstructed local market structure directly from geospatial coordinates. The first step was implementing a haversine function, a standard method for calculating great-circle distances between two points on the Earth&#x2019;s surface. With this, each site could be compared to every other location in the dataset to determine whether it falls within a reasonable competitive radius.</p>
<p>For this analysis, I used a ten-mile radius as a practical proxy for the area in which consumers are likely to consider alternatives. By converting that radius to kilometers and iterating through all pairs of coordinates, I was able to count, for each site, how many competing providers operate within that local catchment. The resulting competitors_within_10_miles field became a foundational feature: it quantifies the local supply environment and captures the structural headwinds&#x2014;or advantages&#x2014;that shape a site&#x2019;s revenue potential.</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-comment"># Define the haversine function to calculate distance between two coordinates</span></span><br><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">haversine_distance</span>(<span class="hljs-params">lat1, lon1, lat2, lon2</span>):</span></span><br><span class="line">    <span class="hljs-string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="hljs-string">    Calculate the great-circle distance between two points on Earth using the haversine formula.</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    Parameters:</span></span><br><span class="line"><span class="hljs-string">    lat1, lon1 : float</span></span><br><span class="line"><span class="hljs-string">        Latitude and longitude of the first point in decimal degrees.</span></span><br><span class="line"><span class="hljs-string">    lat2, lon2 : float</span></span><br><span class="line"><span class="hljs-string">        Latitude and longitude of the second point in decimal degrees.</span></span><br><span class="line"><span class="hljs-string">    </span></span><br><span class="line"><span class="hljs-string">    Returns:</span></span><br><span class="line"><span class="hljs-string">    float</span></span><br><span class="line"><span class="hljs-string">        Distance between the two points in kilometers.</span></span><br><span class="line"><span class="hljs-string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="hljs-comment"># Convert decimal degrees to radians</span></span><br><span class="line">    lat1, lon1, lat2, lon2 = map(radians, [lat1, lon1, lat2, lon2])</span><br><span class="line">    </span><br><span class="line">    <span class="hljs-comment"># Compute differences</span></span><br><span class="line">    dlat = lat2 - lat1</span><br><span class="line">    dlon = lon2 - lon1</span><br><span class="line">    </span><br><span class="line">    <span class="hljs-comment"># Haversine formula</span></span><br><span class="line">    a = sin(dlat / <span class="hljs-number">2</span>)**<span class="hljs-number">2</span> + cos(lat1) * cos(lat2) * sin(dlon / <span class="hljs-number">2</span>)**<span class="hljs-number">2</span></span><br><span class="line">    c = <span class="hljs-number">2</span> * asin(sqrt(a))</span><br><span class="line">    </span><br><span class="line">    <span class="hljs-comment"># Earth&apos;s radius in kilometers</span></span><br><span class="line">    km = <span class="hljs-number">6371</span> * c</span><br><span class="line">    <span class="hljs-keyword">return</span> km</span><br><span class="line"></span><br><span class="line"><span class="hljs-comment"># Specify the search radius</span></span><br><span class="line">search_radius_miles = <span class="hljs-number">10</span></span><br><span class="line"><span class="hljs-comment"># Convert the search radius from miles to kilometers</span></span><br><span class="line">search_radius_km = search_radius_miles * <span class="hljs-number">1.60934</span></span><br><span class="line"></span><br><span class="line"><span class="hljs-comment"># Initialize a list to hold the count of competitors for each location</span></span><br><span class="line">competitor_counts = []</span><br><span class="line"></span><br><span class="line"><span class="hljs-comment"># Loop through each location in the DataFrame</span></span><br><span class="line"><span class="hljs-keyword">for</span> i, row_i <span class="hljs-keyword">in</span> df.iterrows():</span><br><span class="line">    lat_i = row_i[<span class="hljs-string">&apos;latitude&apos;</span>]</span><br><span class="line">    lon_i = row_i[<span class="hljs-string">&apos;longitude&apos;</span>]</span><br><span class="line">    count_competitors = <span class="hljs-number">0</span></span><br><span class="line">    </span><br><span class="line">    <span class="hljs-comment"># Compare location i with every other location j in the DataFrame</span></span><br><span class="line">    <span class="hljs-keyword">for</span> j, row_j <span class="hljs-keyword">in</span> df.iterrows():</span><br><span class="line">        <span class="hljs-keyword">if</span> i == j:</span><br><span class="line">            <span class="hljs-keyword">continue</span>  <span class="hljs-comment"># Skip comparing the location with itself.</span></span><br><span class="line">        lat_j = row_j[<span class="hljs-string">&apos;latitude&apos;</span>]</span><br><span class="line">        lon_j = row_j[<span class="hljs-string">&apos;longitude&apos;</span>]</span><br><span class="line">        </span><br><span class="line">        <span class="hljs-comment"># Calculate the distance between the two locations</span></span><br><span class="line">        distance_km = haversine_distance(lat_i, lon_i, lat_j, lon_j)</span><br><span class="line">        </span><br><span class="line">        <span class="hljs-comment"># If the distance is within the search radius, count it as a competitor</span></span><br><span class="line">        <span class="hljs-keyword">if</span> distance_km &lt;= search_radius_km:</span><br><span class="line">            count_competitors += <span class="hljs-number">1</span></span><br><span class="line">            </span><br><span class="line">    competitor_counts.append(count_competitors)</span><br><span class="line"></span><br><span class="line"><span class="hljs-comment"># Add the computed competitor counts to the DataFrame</span></span><br><span class="line">df[<span class="hljs-string">&apos;competitors_within_10_miles&apos;</span>] = competitor_counts</span><br></pre></td></tr></tbody></table></figure>


<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df[[<span class="hljs-string">&apos;name&apos;</span>,<span class="hljs-string">&apos;competitors_within_10_miles&apos;</span>]].head(<span class="hljs-number">10</span>)</span><br></pre></td></tr></tbody></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;</code></pre>
<p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>name</th>
      <th>competitors_within_10_miles</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>Seawinds Funeral Home &amp; Crematory - Sebastian</td>
      <td>1</td>
    </tr>
    <tr>
      <th>1</th>
      <td>Lewis-Ray Mortuary Inc</td>
      <td>1</td>
    </tr>
    <tr>
      <th>2</th>
      <td>Beth David Memorial Gardens / Levitt Weinstein...</td>
      <td>3</td>
    </tr>
    <tr>
      <th>3</th>
      <td>Sorensen Funeral Home</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>Brewer &amp; Sons Funeral Homes - Spring Hill Chapel</td>
      <td>0</td>
    </tr>
    <tr>
      <th>5</th>
      <td>Fairchild Funeral Home-Crmtry</td>
      <td>0</td>
    </tr>
    <tr>
      <th>6</th>
      <td>Memorial Plan Flagler Memorial Park</td>
      <td>3</td>
    </tr>
    <tr>
      <th>7</th>
      <td>All County Funeral Home &amp; Crematory</td>
      <td>1</td>
    </tr>
    <tr>
      <th>8</th>
      <td>Affordable Cremations by Baldwin Brothers</td>
      <td>3</td>
    </tr>
    <tr>
      <th>9</th>
      <td>Neptune Society</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
</div>



<p>Once competition was quantified, the next logical step was to translate local demand into something economically meaningful: how much of that demand a given site could reasonably capture. Estimated annual events alone are insufficient because two locations with identical demand can perform very differently if one operates in a near-monopoly environment while the other is surrounded by multiple providers. To reflect this, I constructed a simple but powerful measure of implied market share.</p>
<p>For each age group&#x2014;and for total expected events&#x2014;I divided the estimated annual deaths by the number of competitors within a ten-mile radius (plus one, to include the site itself). This produces an approximate &#x201C;demand-per-provider&#x201D; metric, or what I refer to analytically as death share. While coarse by design, this adjustment captures the structural effect of market fragmentation: the more providers sharing the same catchment, the smaller the realistic opportunity set for each individual location.</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-comment"># estimate maket share for each location by dividing estimated death by number of competitors in 10 miles</span></span><br><span class="line">df[<span class="hljs-string">&apos;Estimated deaths share 65 to 74 years&apos;</span>] = (df[<span class="hljs-string">&apos;Estimated deaths 65 to 74 years&apos;</span>]/(df[<span class="hljs-string">&apos;competitors_within_10_miles&apos;</span>]+<span class="hljs-number">1</span>)).round(<span class="hljs-number">0</span>)</span><br><span class="line">df[<span class="hljs-string">&apos;Estimated deaths share 75 to 84 years&apos;</span>] = (df[<span class="hljs-string">&apos;Estimated deaths 75 to 84 years&apos;</span>]/(df[<span class="hljs-string">&apos;competitors_within_10_miles&apos;</span>]+<span class="hljs-number">1</span>)).round(<span class="hljs-number">0</span>)</span><br><span class="line">df[<span class="hljs-string">&apos;Estimated deaths share 85 years and over&apos;</span>] = (df[<span class="hljs-string">&apos;Estimated deaths 85 years and over&apos;</span>]/(df[<span class="hljs-string">&apos;competitors_within_10_miles&apos;</span>]+<span class="hljs-number">1</span>)).round(<span class="hljs-number">0</span>)</span><br><span class="line">df[<span class="hljs-string">&apos;Estimated deaths share&apos;</span>] = (df[<span class="hljs-string">&apos;Estimated deaths&apos;</span>]/(df[<span class="hljs-string">&apos;competitors_within_10_miles&apos;</span>]+<span class="hljs-number">1</span>)).round(<span class="hljs-number">0</span>)</span><br></pre></td></tr></tbody></table></figure>

<h2 id="Learning-the-revenue-function-from-a-reference-chain"><a href="#Learning-the-revenue-function-from-a-reference-chain" class="headerlink" title="Learning the revenue function from a reference chain"></a>Learning the revenue function from a reference chain</h2><p>Because neither of the two target chains disclosed revenue, the only way to infer the economic structure of the industry was to learn it from the reference operator with known revenue across its Florida locations. The goal was not to build a sophisticated forecasting model, but rather to identify a stable, interpretable mapping between local market characteristics and realized revenue.</p>
<p>I began by isolating the Florida subset of Legacy Memorial and merging in the features I had engineered earlier&#x2014;competitive density, household income, and various age-adjusted demand metrics. To understand how much revenue each location generated relative to its local opportunity set, I calculated a simple ratio: revenue per unit of estimated demand share. Conceptually, this expresses how much value a site captures from each unit of its adjusted demand pool.</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-comment"># filter &apos;Legacy Memorial Services&apos; homes in FL</span></span><br><span class="line">df_lms_fl = df[(df[<span class="hljs-string">&apos;state&apos;</span>] == <span class="hljs-string">&apos;Florida&apos;</span>)&amp;(df[<span class="hljs-string">&apos;chain&apos;</span>] == <span class="hljs-string">&apos;Legacy Memorial Services&apos;</span>)].copy()</span><br><span class="line">df_lms_fl[[<span class="hljs-string">&apos;chain&apos;</span>, <span class="hljs-string">&apos;name&apos;</span>,<span class="hljs-string">&apos;zip&apos;</span>,<span class="hljs-string">&apos;county&apos;</span>,<span class="hljs-string">&apos;revenue&apos;</span>,<span class="hljs-string">&apos;competitors_within_10_miles&apos;</span>, <span class="hljs-string">&apos;Mean household income&apos;</span>,</span><br><span class="line">           <span class="hljs-string">&apos;Estimated deaths share 65 to 74 years&apos;</span>,<span class="hljs-string">&apos;Estimated deaths share 75 to 84 years&apos;</span>,</span><br><span class="line">           <span class="hljs-string">&apos;Estimated deaths share 85 years and over&apos;</span>, <span class="hljs-string">&apos;Estimated deaths share&apos;</span>]]</span><br><span class="line"><span class="hljs-comment"># calculate rev generated per estimated deaths share (market share)</span></span><br><span class="line">df_lms_fl[<span class="hljs-string">&apos;rev per death&apos;</span>] = (df_lms_fl[<span class="hljs-string">&apos;revenue&apos;</span>]/df_lms_fl[<span class="hljs-string">&apos;Estimated deaths share&apos;</span>]).round(<span class="hljs-number">0</span>)</span><br><span class="line">df_lms_fl[[<span class="hljs-string">&apos;chain&apos;</span>, <span class="hljs-string">&apos;name&apos;</span>,<span class="hljs-string">&apos;zip&apos;</span>,<span class="hljs-string">&apos;county&apos;</span>,<span class="hljs-string">&apos;revenue&apos;</span>,<span class="hljs-string">&apos;competitors_within_10_miles&apos;</span>, <span class="hljs-string">&apos;Mean household income&apos;</span>,</span><br><span class="line">           <span class="hljs-string">&apos;Estimated deaths share 65 to 74 years&apos;</span>,<span class="hljs-string">&apos;Estimated deaths share 75 to 84 years&apos;</span>,</span><br><span class="line">           <span class="hljs-string">&apos;Estimated deaths share 85 years and over&apos;</span>, <span class="hljs-string">&apos;Estimated deaths share&apos;</span>,<span class="hljs-string">&apos;rev per death&apos;</span>]]</span><br></pre></td></tr></tbody></table></figure>

<p>At this point, one location&#x2014;Baldwin Fairchild Funeral Home in Seminole County&#x2014;stood out as a clear outlier. Its implied revenue-per-share figure was substantially higher than that of other sites. A closer look revealed that Seminole is the smallest county in Florida, meaning that computing expected events strictly within county boundaries understates the true catchment area. In practice, residents often cross county lines for this category of services, especially when adjacent counties are significantly larger.</p>
<p>To correct for this geographical artifact, I expanded the demand base for Seminole locations by incorporating senior population from neighboring Orange County. This adjustment created a more realistic representation of the actual market size accessible to those sites.</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-comment"># add the population of the closest neighbor county Orange to locations in Seminole</span></span><br><span class="line">pop_cols = [<span class="hljs-string">&apos;Population 65 to 74 years&apos;</span>, <span class="hljs-string">&apos;Population 75 to 84 years&apos;</span>,<span class="hljs-string">&apos;Population 85 years and over&apos;</span>, </span><br><span class="line">            <span class="hljs-string">&apos;Estimated deaths 65 to 74 years&apos;</span>, <span class="hljs-string">&apos;Estimated deaths 75 to 84 years&apos;</span>,<span class="hljs-string">&apos;Estimated deaths 85 years and over&apos;</span>, <span class="hljs-string">&apos;Estimated deaths&apos;</span>]</span><br><span class="line">df.loc[<span class="hljs-number">45</span>, pop_cols] += df.loc[<span class="hljs-number">39</span>, pop_cols]</span><br><span class="line">df.loc[<span class="hljs-number">9</span>, pop_cols] += df.loc[<span class="hljs-number">39</span>, pop_cols]</span><br><span class="line">df.loc[df[<span class="hljs-string">&apos;county&apos;</span>] == <span class="hljs-string">&apos;Seminole&apos;</span>, pop_cols]</span><br></pre></td></tr></tbody></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;</code></pre>
<p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Population 65 to 74 years</th>
      <th>Population 75 to 84 years</th>
      <th>Population 85 years and over</th>
      <th>Estimated deaths 65 to 74 years</th>
      <th>Estimated deaths 75 to 84 years</th>
      <th>Estimated deaths 85 years and over</th>
      <th>Estimated deaths</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>9</th>
      <td>157525</td>
      <td>74102</td>
      <td>29533</td>
      <td>2545.0</td>
      <td>3104.0</td>
      <td>3701.0</td>
      <td>9350.0</td>
    </tr>
    <tr>
      <th>45</th>
      <td>157525</td>
      <td>74102</td>
      <td>29533</td>
      <td>2545.0</td>
      <td>3104.0</td>
      <td>3701.0</td>
      <td>9350.0</td>
    </tr>
  </tbody>
</table>
</div>



<p>After correcting the denominator for Seminole, the revenue-per-share metric became remarkably consistent across the reference chain&#x2019;s locations. This stability provided exactly what I needed: a proportionality constant linking adjusted demand to realized revenue.</p>
<p>In other words, the reference chain supplied a set of empirical weights that could be transplanted onto the two target portfolios. With these weights in hand, I could estimate the economic potential of each site&#x2014;even in the absence of disclosed financials&#x2014;and ultimately compare the attractiveness of the two acquisition candidates.</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-comment"># recalculate estimated deaths share based on updated data</span></span><br><span class="line">df[<span class="hljs-string">&apos;Estimated deaths share 65 to 74 years&apos;</span>] = (df[<span class="hljs-string">&apos;Estimated deaths 65 to 74 years&apos;</span>]/(df[<span class="hljs-string">&apos;competitors_within_10_miles&apos;</span>]+<span class="hljs-number">1</span>)).round(<span class="hljs-number">0</span>)</span><br><span class="line">df[<span class="hljs-string">&apos;Estimated deaths share 75 to 84 years&apos;</span>] = (df[<span class="hljs-string">&apos;Estimated deaths 75 to 84 years&apos;</span>]/(df[<span class="hljs-string">&apos;competitors_within_10_miles&apos;</span>]+<span class="hljs-number">1</span>)).round(<span class="hljs-number">0</span>)</span><br><span class="line">df[<span class="hljs-string">&apos;Estimated deaths share 85 years and over&apos;</span>] = (df[<span class="hljs-string">&apos;Estimated deaths 85 years and over&apos;</span>]/(df[<span class="hljs-string">&apos;competitors_within_10_miles&apos;</span>]+<span class="hljs-number">1</span>)).round(<span class="hljs-number">0</span>)</span><br><span class="line">df[<span class="hljs-string">&apos;Estimated deaths share&apos;</span>] = (df[<span class="hljs-string">&apos;Estimated deaths&apos;</span>]/(df[<span class="hljs-string">&apos;competitors_within_10_miles&apos;</span>]+<span class="hljs-number">1</span>)).round(<span class="hljs-number">0</span>)</span><br><span class="line"></span><br><span class="line">df_lms_fl = df[(df[<span class="hljs-string">&apos;state&apos;</span>] == <span class="hljs-string">&apos;Florida&apos;</span>)&amp;(df[<span class="hljs-string">&apos;chain&apos;</span>] == <span class="hljs-string">&apos;Legacy Memorial Services&apos;</span>)].copy()</span><br><span class="line">df_lms_fl[<span class="hljs-string">&apos;rev per death&apos;</span>] = (df_lms_fl[<span class="hljs-string">&apos;revenue&apos;</span>]/df_lms_fl[<span class="hljs-string">&apos;Estimated deaths share&apos;</span>]).round(<span class="hljs-number">0</span>)</span><br><span class="line">df_lms_fl[[<span class="hljs-string">&apos;chain&apos;</span>, <span class="hljs-string">&apos;name&apos;</span>,<span class="hljs-string">&apos;zip&apos;</span>,<span class="hljs-string">&apos;county&apos;</span>,<span class="hljs-string">&apos;revenue&apos;</span>,<span class="hljs-string">&apos;competitors_within_10_miles&apos;</span>, <span class="hljs-string">&apos;Mean household income&apos;</span>, <span class="hljs-string">&apos;Estimated deaths share&apos;</span>,<span class="hljs-string">&apos;rev per death&apos;</span>]]</span><br><span class="line"><span class="hljs-comment"># now rev generated per estimated deaths share is consistant across different locations</span></span><br></pre></td></tr></tbody></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;</code></pre>
<p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>chain</th>
      <th>name</th>
      <th>zip</th>
      <th>county</th>
      <th>revenue</th>
      <th>competitors_within_10_miles</th>
      <th>Mean household income</th>
      <th>Estimated deaths share</th>
      <th>rev per death</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>40</th>
      <td>Legacy Memorial Services</td>
      <td>Lohman Funeral Home Ormond</td>
      <td>32174</td>
      <td>Volusia</td>
      <td>1423140.28</td>
      <td>0</td>
      <td>87233.0</td>
      <td>5765.0</td>
      <td>247.0</td>
    </tr>
    <tr>
      <th>41</th>
      <td>Legacy Memorial Services</td>
      <td>Russell Allen Wright, Sr. Mortuary LLC</td>
      <td>32405</td>
      <td>Bay</td>
      <td>729434.16</td>
      <td>0</td>
      <td>92641.0</td>
      <td>1348.0</td>
      <td>541.0</td>
    </tr>
    <tr>
      <th>42</th>
      <td>Legacy Memorial Services</td>
      <td>Gendron Funeral &amp; Cremation Services Inc.</td>
      <td>33901</td>
      <td>Lee</td>
      <td>904327.89</td>
      <td>3</td>
      <td>102290.0</td>
      <td>1618.0</td>
      <td>559.0</td>
    </tr>
    <tr>
      <th>43</th>
      <td>Legacy Memorial Services</td>
      <td>Caballero Rivero Hialeah</td>
      <td>33010</td>
      <td>Miami-Dade</td>
      <td>1060401.51</td>
      <td>3</td>
      <td>102498.0</td>
      <td>4090.0</td>
      <td>259.0</td>
    </tr>
    <tr>
      <th>44</th>
      <td>Legacy Memorial Services</td>
      <td>Alexander Funeral Home Inc</td>
      <td>33881</td>
      <td>Polk</td>
      <td>1175491.50</td>
      <td>1</td>
      <td>84031.0</td>
      <td>2875.0</td>
      <td>409.0</td>
    </tr>
    <tr>
      <th>45</th>
      <td>Legacy Memorial Services</td>
      <td>Baldwin Fairchild Funeral Home</td>
      <td>32714</td>
      <td>Seminole</td>
      <td>1189375.91</td>
      <td>2</td>
      <td>111625.0</td>
      <td>3117.0</td>
      <td>382.0</td>
    </tr>
    <tr>
      <th>46</th>
      <td>Legacy Memorial Services</td>
      <td>Sound Choice Cremation</td>
      <td>34232</td>
      <td>Sarasota</td>
      <td>1221708.98</td>
      <td>0</td>
      <td>120022.0</td>
      <td>5263.0</td>
      <td>232.0</td>
    </tr>
    <tr>
      <th>47</th>
      <td>Legacy Memorial Services</td>
      <td>Christian Family Funeral</td>
      <td>32503</td>
      <td>Escambia</td>
      <td>1142915.31</td>
      <td>1</td>
      <td>88025.0</td>
      <td>1314.0</td>
      <td>870.0</td>
    </tr>
    <tr>
      <th>48</th>
      <td>Legacy Memorial Services</td>
      <td>Smith-Youngs Funeral Home, Inc.</td>
      <td>33756</td>
      <td>Pinellas</td>
      <td>1056734.64</td>
      <td>3</td>
      <td>101629.0</td>
      <td>2372.0</td>
      <td>446.0</td>
    </tr>
    <tr>
      <th>49</th>
      <td>Legacy Memorial Services</td>
      <td>Riverside Gordon Memorial Chapels</td>
      <td>33162</td>
      <td>Miami-Dade</td>
      <td>989213.10</td>
      <td>4</td>
      <td>102498.0</td>
      <td>3272.0</td>
      <td>302.0</td>
    </tr>
  </tbody>
</table>
</div>



<p>By combining demographic variables, event-based demand estimates, income proxies, and competition measures, I allowed a simple regression model to reveal which characteristics truly mattered.</p>
<p>The results were:</p>
<ol>
<li><p>Death share was the dominant explanatory factor.</p>
</li>
<li><p>Senior population contributed largely through its relationship to death share.</p>
</li>
<li><p>Household income, despite being intuitively appealing as a proxy for pricing power, exerted minimal influence in this dataset.</p>
</li>
</ol>
<p>The muted role of income is less a substantive conclusion about the industry and more a feature of the simplified, simulated structure of this exercise. In this notebook, each location relies on county-level income data without adjusting for actual catchment boundaries. counties are administratively convenient but economically arbitrary; they often fail to reflect the true service footprint, especially in suburban or cross-county markets where consumers may travel several miles beyond their residential ZIP.</p>
<p>In real underwriting work, analysts typically aggregate population and income across multi-ZIP catchments or construct custom polygons based on drive-time radii. That process&#x2014;while far more laborious&#x2014;yields a materially more accurate representation of the household base each site can access. Under such a framework, income variability tends to matter more: higher-income catchments support differentiated service tiers, greater upsell elasticity, and more stable spend per event. Here, because the analysis relies on a simplified approximation, income contributes little incremental signal beyond what competition already explains. </p>
<p>The exercise nevertheless confirms a useful takeaway: when pricing dispersion is narrow and supply is unevenly distributed, market structure dominates household characteristics. The regression results remain directionally consistent with qualitative research showing that, in this sector, proximity and provider availability outweigh income-driven preference formation.</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">df[<span class="hljs-string">&apos;income&apos;</span>] = df[<span class="hljs-string">&apos;Mean household income&apos;</span>]</span><br><span class="line">df[<span class="hljs-string">&apos;death_share&apos;</span>] = df[<span class="hljs-string">&apos;Estimated deaths share&apos;</span>]</span><br><span class="line">df_lms_fl = df[(df[<span class="hljs-string">&apos;state&apos;</span>] == <span class="hljs-string">&apos;Florida&apos;</span>)&amp;(df[<span class="hljs-string">&apos;chain&apos;</span>] == <span class="hljs-string">&apos;Legacy Memorial Services&apos;</span>)].copy()</span><br><span class="line"></span><br><span class="line"><span class="hljs-comment"># Set up predictors (X) and target (y)</span></span><br><span class="line">features = [<span class="hljs-string">&apos;income&apos;</span>, <span class="hljs-string">&apos;death_share&apos;</span>]</span><br><span class="line">X = df_lms_fl[features]</span><br><span class="line">y = df_lms_fl[<span class="hljs-string">&apos;revenue&apos;</span>]</span><br><span class="line"></span><br><span class="line"><span class="hljs-comment"># Add a constant term to include an intercept in the model</span></span><br><span class="line">X = sm.add_constant(X)</span><br><span class="line"></span><br><span class="line"><span class="hljs-comment"># Fit the OLS Model</span></span><br><span class="line">formula = <span class="hljs-string">&apos;revenue ~  death_share + income&apos;</span></span><br><span class="line">model = smf.ols(formula, data=df_lms_fl).fit()</span><br><span class="line"><span class="hljs-comment"># Print the model summary</span></span><br><span class="line">print(model.summary())</span><br><span class="line"></span><br><span class="line"><span class="hljs-comment"># Predict Revenue for Other Chains</span></span><br><span class="line">X = df[features]</span><br><span class="line">X = sm.add_constant(X)</span><br><span class="line"><span class="hljs-comment"># Predict revenue using the fitted model</span></span><br><span class="line">df[<span class="hljs-string">&apos;predicted_revenue&apos;</span>] = model.predict(X).round()</span><br></pre></td></tr></tbody></table></figure>

<pre><code>                            OLS Regression Results                            
==============================================================================
Dep. Variable:                revenue   R-squared:                       0.597
Model:                            OLS   Adj. R-squared:                  0.481
Method:                 Least Squares   F-statistic:                     5.178
Date:                Sat, 12 Apr 2025   Prob (F-statistic):             0.0417
Time:                        00:05:45   Log-Likelihood:                -130.67
No. Observations:                  10   AIC:                             267.3
Df Residuals:                       7   BIC:                             268.2
Df Model:                           2                                         
Covariance Type:            nonrobust                                         
===============================================================================
                  coef    std err          t      P&gt;|t|      [0.025      0.975]
-------------------------------------------------------------------------------
Intercept    1.219e+06   4.01e+05      3.042      0.019    2.71e+05    2.17e+06
death_share    98.2885     30.567      3.215      0.015      26.009     170.568
income         -4.3804      4.182     -1.048      0.330     -14.268       5.508
==============================================================================
Omnibus:                        0.129   Durbin-Watson:                   1.711
Prob(Omnibus):                  0.938   Jarque-Bera (JB):                0.230
Skew:                          -0.196   Prob(JB):                        0.891
Kurtosis:                       2.369   Cond. No.                     9.25e+05
==============================================================================

Notes:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
[2] The condition number is large, 9.25e+05. This might indicate that there are
strong multicollinearity or other numerical problems.


/opt/anaconda3/lib/python3.12/site-packages/scipy/stats/_axis_nan_policy.py:531: UserWarning: kurtosistest only valid for n&gt;=20 ... continuing anyway, n=10
  res = hypotest_fun_out(*samples, **kwds)</code></pre>
<h2 id="Applying-the-model-to-the-two-target-chains"><a href="#Applying-the-model-to-the-two-target-chains" class="headerlink" title="Applying the model to the two target chains"></a>Applying the model to the two target chains</h2><figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df_other = df.loc[df[<span class="hljs-string">&apos;chain&apos;</span>] != <span class="hljs-string">&apos;Legacy Memorial Services&apos;</span>,[<span class="hljs-string">&apos;chain&apos;</span>, <span class="hljs-string">&apos;name&apos;</span>,<span class="hljs-string">&apos;zip&apos;</span>,<span class="hljs-string">&apos;county&apos;</span>,<span class="hljs-string">&apos;Mean household income&apos;</span>, <span class="hljs-string">&apos;Estimated deaths share&apos;</span>, <span class="hljs-string">&apos;predicted_revenue&apos;</span>]]</span><br></pre></td></tr></tbody></table></figure>

<p>With a stable revenue-per-share relationship established from the reference chain, I applied this calibrated mapping to the two target portfolios. For each location, I used the engineered features developed earlier (estimated senior demand, competitive density, and adjusted death share) to compute its implied revenue contribution.</p>
<p>This step effectively converted a qualitative location footprint into a quantifiable economic profile, allowing the two portfolios to be compared on a like-for-like basis despite the absence of financial disclosures.</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">total_revenue_by_chain = df_other.groupby(<span class="hljs-string">&apos;chain&apos;</span>)[<span class="hljs-string">&apos;predicted_revenue&apos;</span>].sum()</span><br><span class="line">total_revenue_by_chain</span><br></pre></td></tr></tbody></table></figure>




<pre><code>chain
Eternal Rest Services    22158799.0
Forever Peace Homes      22932606.0
Name: predicted_revenue, dtype: float64</code></pre>
<p>Stepping back from the individual charts, a consistent story emerges across both portfolios. The relative attractiveness of each chain is driven less by headline population numbers and far more by where each location sits within the competitive and demographic landscape. Sites with moderate density and limited competition reliably rise to the top of the revenue distribution; those with constrained catchment areas or fragmented competitive markets tend to fall behind.</p>
<p>Although Eternal Rest Services benefits from several monopoly-like locations, many of these assets are situated in counties with shallow senior populations, limiting their ability to translate exclusivity into sustainable economics. Conversely, Forever Peace Homes competes in denser and occasionally more contested areas, yet still delivers stronger revenue because its best locations sit at the intersection of healthy demand and manageable competition. The Miami cluster illustrates this clearly: even in a tighter competitive environment, the underlying demographics and service need create enough economic weight to outperform less populated regions further north.</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><span class="line">chains_of_interest = [<span class="hljs-string">&quot;Eternal Rest Services&quot;</span>, <span class="hljs-string">&quot;Forever Peace Homes&quot;</span>]</span><br><span class="line">fig, axs = plt.subplots(nrows=<span class="hljs-number">4</span>, ncols=<span class="hljs-number">2</span>, figsize=(<span class="hljs-number">16</span>, <span class="hljs-number">18</span>), sharex=<span class="hljs-literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="hljs-comment"># Define age groups and colors for the stacked plot</span></span><br><span class="line">age_groups = [<span class="hljs-string">&apos;Population 65 to 74 years&apos;</span>, <span class="hljs-string">&apos;Population 75 to 84 years&apos;</span>,<span class="hljs-string">&apos;Population 85 years and over&apos;</span>]</span><br><span class="line">age_colors = [<span class="hljs-string">&apos;#a6cee3&apos;</span>, <span class="hljs-string">&apos;#1f78b4&apos;</span>, <span class="hljs-string">&apos;#b2df8a&apos;</span>]</span><br><span class="line"></span><br><span class="line"><span class="hljs-keyword">for</span> col_idx, chain <span class="hljs-keyword">in</span> enumerate(chains_of_interest):</span><br><span class="line">    <span class="hljs-comment"># Filter for the current chain and sort by actual revenue descending</span></span><br><span class="line">    df_chain = df[df[<span class="hljs-string">&apos;chain&apos;</span>] == chain].copy()</span><br><span class="line">    df_chain = df_chain.sort_values(by=<span class="hljs-string">&apos;predicted_revenue&apos;</span>, ascending=<span class="hljs-literal">False</span>).reset_index(drop=<span class="hljs-literal">True</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="hljs-comment"># Create an index for homes (rank order)</span></span><br><span class="line">    x = np.arange(len(df_chain))</span><br><span class="line">    </span><br><span class="line">    <span class="hljs-comment"># -----------------------------</span></span><br><span class="line">    <span class="hljs-comment"># Row 1: Bar Chart for Predicted Revenue</span></span><br><span class="line">    <span class="hljs-comment"># -----------------------------</span></span><br><span class="line">    axs[<span class="hljs-number">0</span>, col_idx].bar(x, df_chain[<span class="hljs-string">&apos;predicted_revenue&apos;</span>], color=<span class="hljs-string">&apos;green&apos;</span>)</span><br><span class="line">    axs[<span class="hljs-number">0</span>, col_idx].set_title(<span class="hljs-string">f&quot;<span class="hljs-subst">{chain}</span> - Predicted Revenue&quot;</span>)</span><br><span class="line">    axs[<span class="hljs-number">0</span>, col_idx].set_ylabel(<span class="hljs-string">&quot;Predicted Revenue&quot;</span>)</span><br><span class="line">    axs[<span class="hljs-number">0</span>, col_idx].set_xticks(x)</span><br><span class="line">    axs[<span class="hljs-number">0</span>, col_idx].set_xticklabels(x + <span class="hljs-number">1</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="hljs-comment"># -----------------------------</span></span><br><span class="line">    <span class="hljs-comment"># Row 2: Bar Chart for Estimated Death Share</span></span><br><span class="line">    <span class="hljs-comment"># -----------------------------</span></span><br><span class="line">    axs[<span class="hljs-number">1</span>, col_idx].bar(x, df_chain[<span class="hljs-string">&apos;Estimated deaths share&apos;</span>], color=<span class="hljs-string">&apos;red&apos;</span>)</span><br><span class="line">    axs[<span class="hljs-number">1</span>, col_idx].set_title(<span class="hljs-string">f&quot;<span class="hljs-subst">{chain}</span> - Estimated Death Share&quot;</span>)</span><br><span class="line">    axs[<span class="hljs-number">1</span>, col_idx].set_ylabel(<span class="hljs-string">&quot;Death Share&quot;</span>)</span><br><span class="line">    axs[<span class="hljs-number">1</span>, col_idx].set_xticks(x)</span><br><span class="line">    axs[<span class="hljs-number">1</span>, col_idx].set_xticklabels(x + <span class="hljs-number">1</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="hljs-comment"># -----------------------------</span></span><br><span class="line">    <span class="hljs-comment"># Row 3: Bar Chart for Competitors Within 10 Miles</span></span><br><span class="line">    <span class="hljs-comment"># -----------------------------</span></span><br><span class="line">    axs[<span class="hljs-number">2</span>, col_idx].bar(x, df_chain[<span class="hljs-string">&apos;competitors_within_10_miles&apos;</span>], color=<span class="hljs-string">&apos;orange&apos;</span>)</span><br><span class="line">    axs[<span class="hljs-number">2</span>, col_idx].set_title(<span class="hljs-string">f&quot;<span class="hljs-subst">{chain}</span> - Competitors (10 Miles)&quot;</span>)</span><br><span class="line">    axs[<span class="hljs-number">2</span>, col_idx].set_ylabel(<span class="hljs-string">&quot;Competitors&quot;</span>)</span><br><span class="line">    axs[<span class="hljs-number">2</span>, col_idx].set_xticks(x)</span><br><span class="line">    axs[<span class="hljs-number">2</span>, col_idx].set_xticklabels(x + <span class="hljs-number">1</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="hljs-comment"># -----------------------------</span></span><br><span class="line">    <span class="hljs-comment"># Row 4: Stacked Bar Chart for Population Demographics with Median Age Line</span></span><br><span class="line">    <span class="hljs-comment"># -----------------------------</span></span><br><span class="line">    bottom = np.zeros(len(df_chain))</span><br><span class="line">    <span class="hljs-keyword">for</span> age_group, color <span class="hljs-keyword">in</span> zip(age_groups, age_colors):</span><br><span class="line">        values = df_chain[age_group].values</span><br><span class="line">        axs[<span class="hljs-number">3</span>, col_idx].bar(x, values, bottom=bottom, color=color, label=age_group)</span><br><span class="line">        bottom += values</span><br><span class="line">    axs[<span class="hljs-number">3</span>, col_idx].set_title(<span class="hljs-string">f&quot;<span class="hljs-subst">{chain}</span> - Population Demographics&quot;</span>)</span><br><span class="line">    axs[<span class="hljs-number">3</span>, col_idx].set_ylabel(<span class="hljs-string">&quot;Population Count&quot;</span>)</span><br><span class="line">    axs[<span class="hljs-number">3</span>, col_idx].set_xticks(x)</span><br><span class="line">    axs[<span class="hljs-number">3</span>, col_idx].set_xticklabels(x + <span class="hljs-number">1</span>)</span><br><span class="line">    axs[<span class="hljs-number">3</span>, col_idx].legend(loc=<span class="hljs-string">&apos;upper left&apos;</span>, fontsize=<span class="hljs-string">&apos;small&apos;</span>)</span><br><span class="line"></span><br><span class="line">    ax2 = axs[<span class="hljs-number">3</span>, col_idx].twinx()</span><br><span class="line">    ax2.plot(x, df_chain[<span class="hljs-string">&apos;Population Median age (years)&apos;</span>], marker=<span class="hljs-string">&apos;o&apos;</span>, color=<span class="hljs-string">&apos;black&apos;</span>, linewidth=<span class="hljs-number">1</span>, label=<span class="hljs-string">&apos;Median Age&apos;</span>)</span><br><span class="line">    ax2.set_ylabel(<span class="hljs-string">&quot;Median Age&quot;</span>, color=<span class="hljs-string">&apos;black&apos;</span>)</span><br><span class="line">    ax2.tick_params(axis=<span class="hljs-string">&apos;y&apos;</span>, labelcolor=<span class="hljs-string">&apos;black&apos;</span>)</span><br><span class="line">    ax2.legend(loc=<span class="hljs-string">&apos;upper right&apos;</span>, fontsize=<span class="hljs-string">&apos;small&apos;</span>)</span><br><span class="line"><span class="hljs-keyword">for</span> ax <span class="hljs-keyword">in</span> axs[<span class="hljs-number">3</span>, :]:</span><br><span class="line">    ax.set_xlabel(<span class="hljs-string">&quot;Home (Rank Order)&quot;</span>)</span><br><span class="line"></span><br><span class="line">plt.tight_layout(rect=[<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0.96</span>])</span><br><span class="line">plt.suptitle(<span class="hljs-string">&quot;Comparison of Key Attributes by Home for Each Chain&quot;</span>, fontsize=<span class="hljs-number">22</span>, y=<span class="hljs-number">1</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></tbody></table></figure>



<img src="http://yumeng-li.github.io/legacy_files/legacy_39_0.png">



<h2 id="Building-structure-out-of-ambiguity"><a href="#Building-structure-out-of-ambiguity" class="headerlink" title="Building structure out of ambiguity"></a>Building structure out of ambiguity</h2><p>This simplified, simulated exercise obviously abstracts away many real-world complexities&#x2014;pricing tiers, referral patterns, operational differences, and localized brand effects. But the analytical path is instructive: by breaking the problem into demand, competition, and share of market opportunity, a coherent investment framework emerges even in a niche sector with sparse standardized data. In practice, the analysis would be enriched by catchment-level population aggregation, drive-time modeling, and income-segmented demand estimation, all of which tend to sharpen revenue mapping further.</p>
<p>Yet even in this lightweight demonstration, the conclusion is clear:<br>the economics of legacy services are fundamentally spatial, and the winning portfolio is the one that consistently occupies strong demographic basins with structurally advantaged competitive positions.</p>
<p>This project began as a blank slate&#x2014;no intuition, no industry metrics, no obvious path forward&#x2014;but ended with a defensible, data-driven view of portfolio quality. That is ultimately the value of alternative data in private-credit underwriting: when traditional signals are absent, rigorous structure can still be built from the ground up.</p>
</body></html>
    
    </div>
    
    
</article>




    
        <article class="article content gallery" itemscope itemprop="blogPost">
    <h1 class="article-title is-size-3 is-size-4-mobile" itemprop="name">
        
            <a href="/2022/01/11/VECM2/" itemprop="url">Predicting Post-Pandemic Consumption Behaviors Given Potential Paths of Monetary Policy, Part 2 -- Cointegration, VAR/VECM and Forecast</a>
        
    </h1>
    <div class="article-meta columns is-variable is-1 is-multiline is-mobile is-size-7-mobile">
        <span class="column is-narrow">
            <time datetime="2022-01-11T17:00:00.000Z" itemprop="datePublished">Jan 11 2022</time>
        </span>
        
        <span class="column is-narrow article-category">
            <i class="far fa-folder"></i>
            <a class="article-category-link" href="/categories/Time-Series-Analysis/">Time Series Analysis</a>
        </span>
        
        
        <span class="column is-narrow">
            
            
            26 minutes read (About 3833 words)
        </span>
        
    </div>
    <div class="article-entry is-size-6-mobile" itemprop="articleBody">
    
        <html><head></head><body><p>In the last blog, we went through the economic intuitions behind this consumption model, conducted some preliminary analysis in search for structural breaks and checked the stationarity of the endogenous variables using ADF and KPSS tests. Since we have found that none of the variables can be considered stationary, we can therefore ask whether the variables form a cointegrated system with a given number of &#x201C;common trends&#x201D;. Intuitively, I would argue that there exists one cointegration equation among the three variables as in the long run, one&#x2019;s consumption and net wealth should add up to the one&#x2019;s income.</p>
<p>So let&#x2019;s keep going down to the path, and see how we can test for cointegration and include it in a VECM.</p>
<img src="http://yumeng-li.github.io/VECM_files/flowchart.png">



<h2 id="Cointegration-Test"><a href="#Cointegration-Test" class="headerlink" title="Cointegration Test"></a>Cointegration Test</h2><p>Given that all variables are consistent with the unit root hypothesis and possibly cointegrated, we can use <strong>Johansen&#x2019;s trace statistic</strong> to test for the existence of a common trend i.e., a long-run cointegration relationship among the three endogenous variables.</p>
<p>First, we should estimate the VAR in levels so as to find out the optimal lags to use for the cointegration test. You might want to do this in first differences depending on your preferences, but basically the objective here is to run a preliminary VAR without worrying about whether the variables are cointegrated, which is <strong>obviously an issue given that they all have unit roots</strong>.</p>
<h3 id="VAR-in-Levels"><a href="#VAR-in-Levels" class="headerlink" title="VAR in Levels"></a>VAR in Levels</h3><p>I&#x2019;m going to allow for 10 lags in the first instance just to get the process going, and these are the variables in the VAR as I include the dummy variables sb_1975_4, sb_2008_3 to allow for the possibility of a structural breaks.</p>
<p>The BIC and HQ statistics suggest two lags for the optimal number of lags, whereas the AIC and FPE statistics suggest three. We choose the VAR model with three lags to allow for the possibility.</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train = train.iloc[<span class="hljs-number">1</span>:]</span><br></pre></td></tr></tbody></table></figure>


<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">model = VAR(endog=train[[<span class="hljs-string">&apos;ln_rc&apos;</span>,<span class="hljs-string">&apos;ln_rdy&apos;</span>,<span class="hljs-string">&apos;ln_rnw&apos;</span>]], exog =train[[<span class="hljs-string">&apos;sb_1975_4&apos;</span>,<span class="hljs-string">&apos;sb_2008_3&apos;</span>,<span class="hljs-string">&apos;const&apos;</span>]])</span><br><span class="line">res = model.select_order(<span class="hljs-number">10</span>)</span><br><span class="line">res.summary()</span><br></pre></td></tr></tbody></table></figure>




<table class="simpletable">
<caption>VAR Order Selection (* highlights the minimums)</caption>
<tbody><tr>
   <td></td>      <th>AIC</th>         <th>BIC</th>         <th>FPE</th>        <th>HQIC</th>    
</tr>
<tr>
  <th>0</th>  <td>    -17.58</td>  <td>    -17.39</td>  <td> 2.310e-08</td>  <td>    -17.51</td>
</tr>
<tr>
  <th>1</th>  <td>    -27.99</td>  <td>    -27.66</td>  <td> 6.975e-13</td>  <td>    -27.86</td>
</tr>
<tr>
  <th>2</th>  <td>    -28.19</td>  <td>    -27.71*</td> <td> 5.696e-13</td>  <td>    -28.00*</td>
</tr>
<tr>
  <th>3</th>  <td>    -28.20*</td> <td>    -27.58</td>  <td> 5.664e-13*</td> <td>    -27.95</td>
</tr>
<tr>
  <th>4</th>  <td>    -28.17</td>  <td>    -27.40</td>  <td> 5.829e-13</td>  <td>    -27.86</td>
</tr>
<tr>
  <th>5</th>  <td>    -28.13</td>  <td>    -27.22</td>  <td> 6.067e-13</td>  <td>    -27.76</td>
</tr>
<tr>
  <th>6</th>  <td>    -28.12</td>  <td>    -27.07</td>  <td> 6.144e-13</td>  <td>    -27.69</td>
</tr>
<tr>
  <th>7</th>  <td>    -28.07</td>  <td>    -26.87</td>  <td> 6.488e-13</td>  <td>    -27.58</td>
</tr>
<tr>
  <th>8</th>  <td>    -28.02</td>  <td>    -26.68</td>  <td> 6.810e-13</td>  <td>    -27.48</td>
</tr>
<tr>
  <th>9</th>  <td>    -28.02</td>  <td>    -26.53</td>  <td> 6.807e-13</td>  <td>    -27.42</td>
</tr>
<tr>
  <th>10</th> <td>    -27.97</td>  <td>    -26.33</td>  <td> 7.217e-13</td>  <td>    -27.31</td>
</tr>
</tbody></table>



<h3 id="Johansen-Cointegration-Test"><a href="#Johansen-Cointegration-Test" class="headerlink" title="Johansen Cointegration Test"></a>Johansen Cointegration Test</h3><p>Then, we conduct the Johansen&#x2019;s test with two lags. One has to ensure that the lag interval for the<br>differenced endogenous variables is <strong>1 smaller</strong> than the number of lags used in VAR in levels by moving to the corresponding VECM form.</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">vec_rank = vecm.select_coint_rank(endog=train[[<span class="hljs-string">&apos;ln_rc&apos;</span>,<span class="hljs-string">&apos;ln_rdy&apos;</span>,<span class="hljs-string">&apos;ln_rnw&apos;</span>]], det_order = <span class="hljs-number">0</span>, k_ar_diff = <span class="hljs-number">2</span>, method = <span class="hljs-string">&apos;trace&apos;</span>, signif=<span class="hljs-number">0.05</span>)</span><br><span class="line">print(vec_rank.summary())</span><br><span class="line">print(<span class="hljs-string">&apos;The rank to choose according to the Johansen test: &apos;</span>+ str(vec_rank.rank))</span><br></pre></td></tr></tbody></table></figure>

<pre><code>Johansen cointegration test using trace test statistic with 5% significance level
=====================================
r_0 r_1 test statistic critical value
-------------------------------------
  0   3          33.75          29.80
  1   3          11.44          15.49
-------------------------------------
The rank to choose according to the Johansen test: 1</code></pre>
<p>The results suggest that we can reject the null hypothesis of no cointegration (or zero cointegrating vectors) using a 5% significance level. Moreover, we cannot reject the null hypotheses of at most 1 cointegrating vectors versus the alternative of 2. Therefore, we assume that there exists one (and only one) cointegrating vector.</p>
<h2 id="VECM-Estimation"><a href="#VECM-Estimation" class="headerlink" title="VECM Estimation"></a>VECM Estimation</h2><p>Since there is a cointegration relationship existing, we should construct an ECM equation (e.g., an error correction model) that can explain the actual behavior of consumption during the sample period based on the long-run specification.</p>
<p>In developing the ECM model, we can include additional exogenous variables in the VECM to explain short-run dynamics of consumption. Besides the structural break around 1976 and the inequality gap, here are a bunch of other variables that we should consider. Remember the the determinants we discussed at the beginning?</p>
<ul>
<li>Disposable Income (in the current period) (+)</li>
<li>Expectations (consumer confidence indexes, employment growth) (+)</li>
<li>Wealth (stock market performance, housing prices) (+)</li>
<li>Uncertainty (precautionary saving) (-)</li>
<li>Availability of Credit (+)</li>
<li>Real Interest Rate (?)<ul>
<li>Substitution effect (-)</li>
<li>Income effect (+)</li>
</ul>
</li>
</ul>
<h3 id="Explore-Exogenous-Variables"><a href="#Explore-Exogenous-Variables" class="headerlink" title="Explore Exogenous Variables"></a>Explore Exogenous Variables</h3><p>In developing the ECM model, we should include additional exogenous variables in the VECM to explain short-run dynamics of consumption.</p>
<h4 id="Expectations"><a href="#Expectations" class="headerlink" title="Expectations"></a>Expectations</h4><p>Here we use the unemployment rate and the consumer confidence index as exogenous variables. These variables are proxies for changes in the level of income uncertainty facing the household sector.</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">f, (ax1, ax2) = plt.subplots(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>,figsize=(<span class="hljs-number">14</span>,<span class="hljs-number">4</span>))</span><br><span class="line">ax1.plot(train[<span class="hljs-string">&apos;unemp&apos;</span>])</span><br><span class="line">ax2.plot(train[<span class="hljs-string">&apos;unemp&apos;</span>].diff().dropna())</span><br><span class="line">ax1.set_title(<span class="hljs-string">&apos;unemp&apos;</span>)</span><br><span class="line">ax2.set_title(<span class="hljs-string">&apos;diff_unemp&apos;</span>)</span><br></pre></td></tr></tbody></table></figure>


<img src="http://yumeng-li.github.io/VECM_files/VECM_61_1.png">


<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">f, (ax1, ax2) = plt.subplots(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>,figsize=(<span class="hljs-number">14</span>,<span class="hljs-number">4</span>))</span><br><span class="line">ax1.plot(train[<span class="hljs-string">&apos;consumer_confidence&apos;</span>])</span><br><span class="line">ax2.plot(np.log(train[<span class="hljs-string">&apos;consumer_confidence&apos;</span>]))</span><br><span class="line">ax1.set_title(<span class="hljs-string">&apos;consumer_confidence&apos;</span>)</span><br><span class="line">ax2.set_title(<span class="hljs-string">&apos;ln_consumer_confidence&apos;</span>)</span><br></pre></td></tr></tbody></table></figure>



<img src="http://yumeng-li.github.io/VECM_files/VECM_62_1.png">



<p>Let&#x2019;s then conduct the stationary tests for the change in the unemployment rate and the logarithm of the consumer confidence index, to check whether they are separately consistent with the stationarity assumption.</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">augmented_dickey_fuller_test(train[<span class="hljs-string">&apos;unemp&apos;</span>].diff().dropna())</span><br><span class="line">kpss_test(train[<span class="hljs-string">&apos;unemp&apos;</span>].diff().dropna())</span><br></pre></td></tr></tbody></table></figure>

<pre><code>ADF Statistic: -5.032319
ADF p-value: 0.000019
stationary - null hypothesis of a unit root can be rejected at a 5% significant level
KPSS Statistic: 0.048204
KPSS p-value: 0.100000
stationary - null hypothesis of stationary cannot be rejected at a 5% significant level</code></pre>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">augmented_dickey_fuller_test(np.log(train[<span class="hljs-string">&apos;consumer_confidence&apos;</span>]))</span><br><span class="line">kpss_test(np.log(train[<span class="hljs-string">&apos;consumer_confidence&apos;</span>]))</span><br></pre></td></tr></tbody></table></figure>

<pre><code>ADF Statistic: -3.450606
ADF p-value: 0.009350
stationary - null hypothesis of a unit root can be rejected at a 5% significant level
KPSS Statistic: 0.134007
KPSS p-value: 0.100000
stationary - null hypothesis of stationary cannot be rejected at a 5% significant level</code></pre>
<h4 id="Availability-of-Credit"><a href="#Availability-of-Credit" class="headerlink" title="Availability of Credit"></a>Availability of Credit</h4><p>Evidently, the availability and cost of credit will also affect decisions of households to consume. So the behavior of monetary aggregates in terms of the availability of credit and the interest rates in terms of the cost of credit will matter for our projections for private consumption.</p>
<p>(Honestly, it would be ridiculous to ignore the crazy amount of money that the fed has been pumping into the economy since 2008 and its consequential impact. The hopeless belief in the Keynesian&#x2019;s monetary policy.)</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">f, (ax1, ax2) = plt.subplots(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>,figsize=(<span class="hljs-number">14</span>,<span class="hljs-number">4</span>))</span><br><span class="line">ax1.plot(train[<span class="hljs-string">&apos;m2&apos;</span>])</span><br><span class="line">ax2.plot(np.log(train[<span class="hljs-string">&apos;m2&apos;</span>]).diff())</span><br><span class="line">ax1.set_title(<span class="hljs-string">&apos;real m2&apos;</span>)</span><br><span class="line">ax2.set_title(<span class="hljs-string">&apos;diff real m2&apos;</span>)</span><br></pre></td></tr></tbody></table></figure>



<img src="http://yumeng-li.github.io/VECM_files/VECM_67_1.png">



<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">f, (ax1, ax2) = plt.subplots(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>,figsize=(<span class="hljs-number">14</span>,<span class="hljs-number">4</span>))</span><br><span class="line">ax1.plot(train[<span class="hljs-string">&apos;dff&apos;</span>])</span><br><span class="line">ax2.plot(train[<span class="hljs-string">&apos;dff&apos;</span>].diff())</span><br><span class="line">ax1.set_title(<span class="hljs-string">&apos;Federal Funds Effective Rate&apos;</span>)</span><br><span class="line">ax2.set_title(<span class="hljs-string">&apos;Diff Federal Funds Effective Rate&apos;</span>)</span><br></pre></td></tr></tbody></table></figure>



<img src="http://yumeng-li.github.io/VECM_files/VECM_68_1.png">



<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">augmented_dickey_fuller_test(np.log(train[<span class="hljs-string">&apos;m2&apos;</span>]).diff().dropna())</span><br><span class="line">kpss_test(np.log(train[<span class="hljs-string">&apos;m2&apos;</span>]).diff().dropna())</span><br></pre></td></tr></tbody></table></figure>

<pre><code>ADF Statistic: -5.040178
ADF p-value: 0.000018
stationary - null hypothesis of a unit root can be rejected at a 5% significant level
KPSS Statistic: 0.172896
KPSS p-value: 0.100000
stationary - null hypothesis of stationary cannot be rejected at a 5% significant level</code></pre>
<p>Let&#x2019;s adding these new variables to the dataframe.</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">df[<span class="hljs-string">&apos;d_unemp&apos;</span>] = df[<span class="hljs-string">&apos;unemp&apos;</span>].diff()</span><br><span class="line">df[<span class="hljs-string">&apos;ln_consumer_confidence&apos;</span>] = np.log(df[<span class="hljs-string">&apos;consumer_confidence&apos;</span>])</span><br><span class="line">df[<span class="hljs-string">&apos;d_ln_m2&apos;</span>] = np.log(df[<span class="hljs-string">&apos;m2&apos;</span>]).diff()</span><br><span class="line">df[<span class="hljs-string">&apos;d_dff&apos;</span>] = df[<span class="hljs-string">&apos;dff&apos;</span>].diff()</span><br><span class="line">df[<span class="hljs-string">&apos;spread_10y3m&apos;</span>] = df[<span class="hljs-string">&apos;yield_10y&apos;</span>]-df[<span class="hljs-string">&apos;yield_3m&apos;</span>]</span><br><span class="line"></span><br><span class="line">train,test = df[<span class="hljs-number">1</span>:cut], df[cut:]</span><br></pre></td></tr></tbody></table></figure>

<h3 id="Model-Estimation"><a href="#Model-Estimation" class="headerlink" title="Model Estimation"></a>Model Estimation</h3><p>Based on all previous analysis, we can eventually run a VECM. Based on the long-run equation, we specify the form of the model with three endogenous variables log(rc), log(rdy) and log(rnw), and allow for five additional exogenous variables.</p>
<p>The lag intervals for the differenced endogenous variables are 2 given that we estimated the VAR with 3 lags in levels.</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">endog = [<span class="hljs-string">&apos;ln_rc&apos;</span>,<span class="hljs-string">&apos;ln_rdy&apos;</span>,<span class="hljs-string">&apos;ln_rnw&apos;</span>]</span><br><span class="line">exog = [<span class="hljs-string">&apos;sb_1975_4&apos;</span>,<span class="hljs-string">&apos;sb_2008_3&apos;</span>,<span class="hljs-string">&apos;d_unemp&apos;</span>,<span class="hljs-string">&apos;ln_consumer_confidence&apos;</span>, <span class="hljs-string">&apos;d_ln_m2&apos;</span>, <span class="hljs-string">&apos;d_dff&apos;</span>]</span><br></pre></td></tr></tbody></table></figure>


<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">vec = vecm.VECM(endog = train[endog],</span><br><span class="line">                exog =train[exog],</span><br><span class="line">                k_ar_diff = <span class="hljs-number">2</span>, coint_rank = <span class="hljs-number">1</span>, deterministic = <span class="hljs-string">&quot;co&quot;</span>, missing= <span class="hljs-string">&apos;drop&apos;</span>)</span><br><span class="line">vecm_fit = vec.fit()</span><br></pre></td></tr></tbody></table></figure>

<p>The coefficients in the long-run equation are both statistically significantly different from zero (see the first three tables below), and most importantly, consistent with our economic priors i.e., income and wealth have positive coefficients on consumption (see parameters for ln_rdy and ln_rnw in the first table).</p>
<p>For the exogenous variables, the negative coefficient on the structural breaks indicates that the increasing inequality may compress the consumption of households. Positive and significant coefficient on real m2 complies with our assumption that expanding monetary policy through quantitative ease may stimulate consumption in the short term.</p>
<p>Also, the coefficient on the ECM term is <strong>negative</strong> and significant with respect to real consumption, and significant and <strong>positive</strong> with respect to disposable income, both using a 5% level of significance, which are reasonable since both consumption and income are endogenous variables and dependent on each other, a rational household would react to disequilibrium in consumption relative to its long-run path though simultaneous adjustments - lower their spending (negative coefficient) and increase their income (positive coefficient).</p>
<p>From the &#x201C;cointegration relations&#x201D; table, we can verify our idea that in the long run, amount that one&#x2019;s consumption and wealth accumulated would be equal to the income one earns.</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vecm_fit.summary().as_latex</span><br></pre></td></tr></tbody></table></figure>




<pre><code>&lt;bound method Summary.as_latex of &lt;class &apos;statsmodels.iolib.summary.Summary&apos;&gt;
&quot;&quot;&quot;
Det. terms outside the coint. relation &amp; lagged endog. parameters for equation ln_rc
==============================================================================
                 coef    std err          z      P&gt;|z|      [0.025      0.975]
------------------------------------------------------------------------------
const         -0.6625      0.154     -4.310      0.000      -0.964      -0.361
exog1         -0.0027      0.001     -3.309      0.001      -0.004      -0.001
exog2         -0.0052      0.001     -4.940      0.000      -0.007      -0.003
exog3         -0.0069      0.001     -4.782      0.000      -0.010      -0.004
exog4          0.1386      0.032      4.279      0.000       0.075       0.202
exog5          0.1697      0.033      5.189      0.000       0.106       0.234
exog6          0.0006      0.000      1.463      0.144      -0.000       0.001
L1.ln_rc      -0.2099      0.068     -3.108      0.002      -0.342      -0.078
L1.ln_rdy      0.0742      0.043      1.739      0.082      -0.009       0.158
L1.ln_rnw      0.0485      0.022      2.248      0.025       0.006       0.091
L2.ln_rc      -0.0623      0.063     -0.986      0.324      -0.186       0.062
L2.ln_rdy      0.0077      0.042      0.184      0.854      -0.074       0.090
L2.ln_rnw      0.0039      0.022      0.176      0.860      -0.039       0.047
Det. terms outside the coint. relation &amp; lagged endog. parameters for equation ln_rdy
==============================================================================
                 coef    std err          z      P&gt;|z|      [0.025      0.975]
------------------------------------------------------------------------------
const         -0.2331      0.251     -0.928      0.353      -0.725       0.259
exog1         -0.0025      0.001     -1.879      0.060      -0.005       0.000
exog2         -0.0031      0.002     -1.777      0.076      -0.006       0.000
exog3         -0.0069      0.002     -2.937      0.003      -0.011      -0.002
exog4          0.0640      0.053      1.210      0.226      -0.040       0.168
exog5          0.1607      0.053      3.007      0.003       0.056       0.265
exog6         -0.0003      0.001     -0.445      0.656      -0.002       0.001
L1.ln_rc       0.1241      0.110      1.124      0.261      -0.092       0.340
L1.ln_rdy     -0.1826      0.070     -2.620      0.009      -0.319      -0.046
L1.ln_rnw      0.0519      0.035      1.469      0.142      -0.017       0.121
L2.ln_rc      -0.1472      0.103     -1.426      0.154      -0.350       0.055
L2.ln_rdy      0.0505      0.068      0.738      0.460      -0.084       0.184
L2.ln_rnw     -0.0868      0.036     -2.421      0.015      -0.157      -0.017
Det. terms outside the coint. relation &amp; lagged endog. parameters for equation ln_rnw
==============================================================================
                 coef    std err          z      P&gt;|z|      [0.025      0.975]
------------------------------------------------------------------------------
const         -1.4968      0.494     -3.028      0.002      -2.466      -0.528
exog1          0.0003      0.003      0.097      0.923      -0.005       0.005
exog2         -0.0009      0.003     -0.260      0.795      -0.007       0.006
exog3         -0.0024      0.005     -0.514      0.607      -0.011       0.007
exog4          0.3093      0.104      2.968      0.003       0.105       0.513
exog5          0.1839      0.105      1.748      0.080      -0.022       0.390
exog6         -0.0021      0.001     -1.576      0.115      -0.005       0.001
L1.ln_rc       0.1244      0.217      0.573      0.567      -0.301       0.550
L1.ln_rdy     -0.2364      0.137     -1.723      0.085      -0.505       0.033
L1.ln_rnw      0.0856      0.069      1.232      0.218      -0.051       0.222
L2.ln_rc       0.2163      0.203      1.065      0.287      -0.182       0.615
L2.ln_rdy     -0.2192      0.135     -1.629      0.103      -0.483       0.044
L2.ln_rnw      0.0711      0.071      1.008      0.314      -0.067       0.209
               Loading coefficients (alpha) for equation ln_rc                
==============================================================================
                 coef    std err          z      P&gt;|z|      [0.025      0.975]
------------------------------------------------------------------------------
ec1           -0.0605      0.020     -2.993      0.003      -0.100      -0.021
               Loading coefficients (alpha) for equation ln_rdy              
==============================================================================
                 coef    std err          z      P&gt;|z|      [0.025      0.975]
------------------------------------------------------------------------------
ec1            0.0887      0.033      2.684      0.007       0.024       0.153
               Loading coefficients (alpha) for equation ln_rnw              
==============================================================================
                 coef    std err          z      P&gt;|z|      [0.025      0.975]
------------------------------------------------------------------------------
ec1           -0.1382      0.065     -2.126      0.033      -0.266      -0.011
          Cointegration relations for loading-coefficients-column 1          
==============================================================================
                 coef    std err          z      P&gt;|z|      [0.025      0.975]
------------------------------------------------------------------------------
beta.1         1.0000          0          0      0.000       1.000       1.000
beta.2        -1.1175      0.073    -15.311      0.000      -1.261      -0.974
beta.3         0.0555      0.058      0.963      0.336      -0.057       0.168
==============================================================================
&quot;&quot;&quot;&gt;</code></pre>
<h2 id="VECM-Forecast"><a href="#VECM-Forecast" class="headerlink" title="VECM Forecast"></a>VECM Forecast</h2><h3 id="Out-of-sample-Forecast"><a href="#Out-of-sample-Forecast" class="headerlink" title="Out-of-sample Forecast"></a>Out-of-sample Forecast</h3><p>As we are comfortable with the model estimated, let&#x2019;s do some forecasting with the out-of-sample data and evaluate how the model works.</p>
<p>To forecast for time beyond the sample period pre-2017, we need to provide futures shocks post-2017 i.e., the exogenous variables, which includes the dramatic economic downturn during the Covid recession. Let&#x2019;s plot them out and get a feel.</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">f, (ax1, ax2, ax3) = plt.subplots(<span class="hljs-number">1</span>,<span class="hljs-number">3</span>,figsize=(<span class="hljs-number">18</span>,<span class="hljs-number">4</span>))</span><br><span class="line">ax1.plot(test[<span class="hljs-string">&apos;d_unemp&apos;</span>])</span><br><span class="line">ax2.plot(test[<span class="hljs-string">&apos;d_ln_m2&apos;</span>])</span><br><span class="line">ax3.plot(test[<span class="hljs-string">&apos;d_dff&apos;</span>])</span><br><span class="line">ax1.set_title(<span class="hljs-string">&apos;Diff Unemployment Rate&apos;</span>)</span><br><span class="line">ax2.set_title(<span class="hljs-string">&apos;Diff Real M2&apos;</span>)</span><br><span class="line">ax3.set_title(<span class="hljs-string">&apos;Diff Fed Fund Rate&apos;</span>)</span><br><span class="line">f.autofmt_xdate()</span><br></pre></td></tr></tbody></table></figure>

<img src="http://yumeng-li.github.io/VECM_files/VECM_80_0.png">


<p>When forecasting, I include the confidence intervals for the predictions as well. Remember, point predictions won&#x2019;t help you much with gauging the uncertainty of forecasts.</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">mid, lower_1se, upper_1se = vecm_fit.predict(steps=len(test),alpha = <span class="hljs-number">0.32</span>, exog_fc =test[exog])</span><br><span class="line">_, lower_2se, upper_2se = vecm_fit.predict(steps=len(test),alpha = <span class="hljs-number">0.05</span>, exog_fc =test[exog])</span><br><span class="line"></span><br><span class="line">pred_mid = pd.DataFrame(np.exp(mid), columns=[<span class="hljs-string">&apos;pred_rc&apos;</span>,<span class="hljs-string">&apos;pred_rdy&apos;</span>,<span class="hljs-string">&apos;pred_rnw&apos;</span>], index = test.index)</span><br><span class="line">pred_lower_1se = pd.DataFrame(np.exp(lower_1se), columns=[<span class="hljs-string">&apos;lower_1se_rc&apos;</span>,<span class="hljs-string">&apos;lower_1se_rdy&apos;</span>,<span class="hljs-string">&apos;lower_1se_rnw&apos;</span>], index = test.index)</span><br><span class="line">pred_upper_1se = pd.DataFrame(np.exp(upper_1se), columns=[<span class="hljs-string">&apos;upper_1se_rc&apos;</span>,<span class="hljs-string">&apos;upper_1se_rdy&apos;</span>,<span class="hljs-string">&apos;upper_1se_rnw&apos;</span>], index = test.index)</span><br><span class="line">pred_lower_2se = pd.DataFrame(np.exp(lower_2se), columns=[<span class="hljs-string">&apos;lower_2se_rc&apos;</span>,<span class="hljs-string">&apos;lower_2se_rdy&apos;</span>,<span class="hljs-string">&apos;lower_2se_rnw&apos;</span>], index = test.index)</span><br><span class="line">pred_upper_2se = pd.DataFrame(np.exp(upper_2se), columns=[<span class="hljs-string">&apos;upper_2se_rc&apos;</span>,<span class="hljs-string">&apos;upper_2se_rdy&apos;</span>,<span class="hljs-string">&apos;upper_2se_rnw&apos;</span>], index = test.index)</span><br><span class="line">ptest = pd.concat([test, pred_mid, pred_lower_1se, pred_upper_1se, pred_lower_2se, pred_upper_2se],axis = <span class="hljs-number">1</span>)</span><br></pre></td></tr></tbody></table></figure>

<p>The saving rate (percentage points) can be derived from total nominal consumption and<br>nominal household disposable income using the following formula:</p>
<p>$saving\ rate=100*(rdy - (rc + (government\ transfers + interest\ payments/gdp\ deflator))) /rdy$</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ptest[<span class="hljs-string">&apos;pred_saving_rate&apos;</span>]=<span class="hljs-number">100</span>*(ptest.pred_rdy-(ptest.pred_rc+(ptest.gov_transfers+ptest.interest_payments/ptest.c_deflator)))/ptest.pred_rdy</span><br></pre></td></tr></tbody></table></figure>

<p>Now let&#x2019;s see how the model predicts,</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">plot_point_prediction</span>(<span class="hljs-params">test</span>):</span></span><br><span class="line">    fig, axes = plt.subplots(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>, figsize=(<span class="hljs-number">12</span>,<span class="hljs-number">8</span>))</span><br><span class="line">    count=<span class="hljs-number">0</span></span><br><span class="line">    <span class="hljs-keyword">for</span> endog <span class="hljs-keyword">in</span> [<span class="hljs-string">&apos;rc&apos;</span>,<span class="hljs-string">&apos;rdy&apos;</span>,<span class="hljs-string">&apos;rnw&apos;</span>,<span class="hljs-string">&apos;saving_rate&apos;</span>]:</span><br><span class="line">        col=count %<span class="hljs-number">2</span></span><br><span class="line">        row=count //<span class="hljs-number">2</span></span><br><span class="line">        ax=axes[row][col]</span><br><span class="line">        ax.set_title(endog)</span><br><span class="line">        ax.plot(test[endog], color=<span class="hljs-string">&apos;Black&apos;</span>,linewidth=<span class="hljs-number">1</span>, label=<span class="hljs-string">&quot;Observed&quot;</span>)</span><br><span class="line">        ax.plot(test[<span class="hljs-string">&apos;pred_&apos;</span>+endog],color=<span class="hljs-string">&apos;grey&apos;</span>,linestyle=<span class="hljs-string">&apos;--&apos;</span>,linewidth=<span class="hljs-number">1</span>, label=<span class="hljs-string">&quot;Forecast&quot;</span>)  </span><br><span class="line">        ax.legend()</span><br><span class="line">        count+=<span class="hljs-number">1</span></span><br><span class="line">    fig.autofmt_xdate()</span><br></pre></td></tr></tbody></table></figure>


<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">plot_point_prediction(ptest)</span><br></pre></td></tr></tbody></table></figure>

<img src="http://yumeng-li.github.io/VECM_files/VECM_87_0.png">



<p>The results are not so bad. Actually pretty dope pre-pandemic. For a model that is trying to predict for consumption three years out, its predictive accuracy is out-of-expectation.</p>
<p>Though not surprisingly, the forecasts are over predicting actual real disposable income, real net wealth and therefore real consumption expenditures. This reflects the I(1) nature of the variables involved, the limited serial correlation in their innovation sequences, and the fact that all variables were trending upwards before the pandemic. Because of their I(1) nature, a dynamic forecast will effectively use the last known value (four years ago) of these variables to generate out-of-sample forecasts. Consequently, the forecasting model does not do good job of predicting the impact of covid19 on real consumption, and its predictions for the saving ratio appear to be systematically below the actual saving outcome.</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">plot_forecast_with_confidence_intervals</span>(<span class="hljs-params">df,test</span>):</span></span><br><span class="line">    fig, axes = plt.subplots(<span class="hljs-number">3</span>, <span class="hljs-number">1</span>, figsize=(<span class="hljs-number">10</span>,<span class="hljs-number">10</span>))</span><br><span class="line">    count=<span class="hljs-number">0</span></span><br><span class="line">    <span class="hljs-keyword">for</span> endog <span class="hljs-keyword">in</span> [<span class="hljs-string">&apos;rc&apos;</span>,<span class="hljs-string">&apos;rdy&apos;</span>,<span class="hljs-string">&apos;rnw&apos;</span>]:</span><br><span class="line">        col=count</span><br><span class="line">        ax=axes[col]</span><br><span class="line">        ax.set_title(endog)</span><br><span class="line">        ax.plot(df[endog], color=<span class="hljs-string">&apos;Black&apos;</span>,linewidth=<span class="hljs-number">1.2</span>,label=<span class="hljs-string">&quot;Observed&quot;</span>)</span><br><span class="line">        ax.plot(test[<span class="hljs-string">&apos;pred_&apos;</span>+endog],<span class="hljs-string">&apos;k--&apos;</span>,linewidth=<span class="hljs-number">0.7</span>,label=<span class="hljs-string">&quot;Forecast&quot;</span>)</span><br><span class="line">        ax.fill_between(test[<span class="hljs-string">&apos;pred_&apos;</span>+endog].index,test[<span class="hljs-string">&apos;lower_1se_&apos;</span>+endog],test[<span class="hljs-string">&apos;upper_1se_&apos;</span>+endog],alpha=<span class="hljs-number">0.07</span>,color=<span class="hljs-string">&quot;k&quot;</span>)</span><br><span class="line">        ax.fill_between(test[<span class="hljs-string">&apos;pred_&apos;</span>+endog].index,test[<span class="hljs-string">&apos;lower_2se_&apos;</span>+endog],test[<span class="hljs-string">&apos;upper_2se_&apos;</span>+endog],alpha=<span class="hljs-number">0.07</span>,color=<span class="hljs-string">&quot;k&quot;</span>)      </span><br><span class="line">        ax.legend(loc =<span class="hljs-string">&apos;upper left&apos;</span>)</span><br><span class="line">        count+=<span class="hljs-number">1</span></span><br></pre></td></tr></tbody></table></figure>

<p>With that being said, the previous assessment considers only a single point prediction using the baseline model. A fairer assessment of the model can be made considering the <strong>confidential intervals for prediction</strong>, which suggest that the underlying model is useful for predicting the outcome of the consumption and net wealth.</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">plot_forecast_with_confidence_intervals(df.iloc[<span class="hljs-number">200</span>:],ptest)</span><br></pre></td></tr></tbody></table></figure>

<img src="http://yumeng-li.github.io/VECM_files/VECM_91_0.png">



<p>While there is obvious (downward) bias in the model&#x2019;s predictions, it should be noted that this partly a reflection of the initial starting point of the forecast. If one begins the forecast after the pandemic, the resulting confidential intervals would be much closer to what observed. In this way, a <strong>static forecast</strong> which is <strong>one-period ahead forecasting</strong> would do a much better job. This is also how the model should be used in practice. As always, predicting future is difficult, trying to predict mutiple years out is meaningless.</p>
<h3 id="Long-run-Forecasts-Based-on-Potential-Paths-of-Monetary-Policy"><a href="#Long-run-Forecasts-Based-on-Potential-Paths-of-Monetary-Policy" class="headerlink" title="Long-run Forecasts Based on Potential Paths of Monetary Policy"></a>Long-run Forecasts Based on Potential Paths of Monetary Policy</h3><p>Now it has come to a very interesting point. Let&#x2019;s fit a VECM with the complete data from 1962 to 2021Q2, and use it to forecast for 2021Q3 onwards using two simulated forward paths of the Fed&#x2019;s monetary policy. Correct assumptions would help us to get more accurate estimates for the long-run consumption, disposal income and net wealth of the American households after the pandemic.</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">endog = [<span class="hljs-string">&apos;ln_rc&apos;</span>,<span class="hljs-string">&apos;ln_rdy&apos;</span>,<span class="hljs-string">&apos;ln_rnw&apos;</span>]</span><br><span class="line">exog = [<span class="hljs-string">&apos;sb_1975_4&apos;</span>,<span class="hljs-string">&apos;sb_2008_3&apos;</span>,<span class="hljs-string">&apos;d_unemp&apos;</span>,<span class="hljs-string">&apos;ln_consumer_confidence&apos;</span>, <span class="hljs-string">&apos;d_ln_m2&apos;</span>,<span class="hljs-string">&apos;d_dff&apos;</span>]</span><br><span class="line">vec = vecm.VECM(endog = df[endog],</span><br><span class="line">                exog =df[exog],</span><br><span class="line">                k_ar_diff = <span class="hljs-number">2</span>, coint_rank = <span class="hljs-number">1</span>, deterministic = <span class="hljs-string">&quot;co&quot;</span>, missing= <span class="hljs-string">&apos;drop&apos;</span>)</span><br><span class="line">vecm_fit = vec.fit()</span><br></pre></td></tr></tbody></table></figure>

<p>Let&#x2019;s generate two paths for the forward monetary shocks. One path assumes that fed keeps its current pacing of assets purchasing i.e., no tapering, the other path assumes four interest rate hikes in 2022, 2023 and 2024 assuming the same pace as the period after the financial crisis from 2014Q4 to 2017Q3, with all the other exogenous variables being the same, specifically, current value for unemployment rate and the historical average for consumer confidence.</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">forecast_range = pd.date_range(start=<span class="hljs-string">&apos;04/01/2021&apos;</span>, end =<span class="hljs-string">&apos;01/01/2024&apos;</span>,freq=<span class="hljs-string">&apos;QS&apos;</span> )</span><br><span class="line">shocks = {}</span><br><span class="line">shocks = pd.DataFrame(shocks, index = forecast_range, columns = exog)</span><br><span class="line">shocks[<span class="hljs-string">&apos;sb_1975_4&apos;</span>] = <span class="hljs-number">1</span></span><br><span class="line">shocks[<span class="hljs-string">&apos;sb_2008_3&apos;</span>] = <span class="hljs-number">1</span></span><br><span class="line">shocks[<span class="hljs-string">&apos;d_unemp&apos;</span>] = <span class="hljs-number">0</span></span><br><span class="line">shocks[<span class="hljs-string">&apos;ln_consumer_confidence&apos;</span>] = df[<span class="hljs-string">&apos;ln_consumer_confidence&apos;</span>].mean()</span><br><span class="line"></span><br><span class="line">no_hikes_shocks = shocks.copy()</span><br><span class="line">no_hikes_shocks[<span class="hljs-string">&apos;d_ln_m2&apos;</span>] = df.iloc[<span class="hljs-number">-4</span>:][<span class="hljs-string">&apos;d_ln_m2&apos;</span>].mean()</span><br><span class="line">no_hikes_shocks[<span class="hljs-string">&apos;d_dff&apos;</span>] = <span class="hljs-number">0.0</span></span><br><span class="line"></span><br><span class="line">four_hikes_shocks =shocks.copy()</span><br><span class="line">four_hikes_shocks[[<span class="hljs-string">&apos;d_ln_m2&apos;</span>,<span class="hljs-string">&apos;d_dff&apos;</span>]] = df.loc[<span class="hljs-string">&apos;2014-10-01&apos;</span>:<span class="hljs-string">&apos;2017-07-01&apos;</span>,[<span class="hljs-string">&apos;d_ln_m2&apos;</span>,<span class="hljs-string">&apos;d_dff&apos;</span>]].values</span><br></pre></td></tr></tbody></table></figure>

<p>Here&#x2019;s how the money aggregates is going to be like if assuming no tapering until 2024, which would be of course totally insane&#x2026;</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-comment"># change of log(real m2) and change of fed fund rate</span></span><br><span class="line">no_hikes_shocks[[<span class="hljs-string">&apos;d_ln_m2&apos;</span>,<span class="hljs-string">&apos;d_dff&apos;</span>]]</span><br></pre></td></tr></tbody></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;</code></pre>
<p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>d_ln_m2</th>
      <th>d_dff</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>2021-04-01</th>
      <td>0.02268</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>2021-07-01</th>
      <td>0.02268</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>2021-10-01</th>
      <td>0.02268</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>2022-01-01</th>
      <td>0.02268</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>2022-04-01</th>
      <td>0.02268</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>2022-07-01</th>
      <td>0.02268</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>2022-10-01</th>
      <td>0.02268</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>2023-01-01</th>
      <td>0.02268</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>2023-04-01</th>
      <td>0.02268</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>2023-07-01</th>
      <td>0.02268</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>2023-10-01</th>
      <td>0.02268</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>2024-01-01</th>
      <td>0.02268</td>
      <td>0.0</td>
    </tr>
  </tbody>
</table>
</div>



<p>Here&#x2019;s a more likely path assumes that the fed would stop expanding its balance sheet from the forth quarter of 2021 and hike once in 2022 and three times at the end 2023 and beginning of 2024. This path is quite aggressive.</p>
<p>(By the way, when central banks expend their balance sheets i.e., increase their net asset position, they literally buy assets or reduce their liability through money creation, which means writing checks to themselves.)</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-comment"># hike schedule - historical simulation using 2014-2017 data</span></span><br><span class="line"><span class="hljs-comment">## change of log(real m2) and change of fed fund rate</span></span><br><span class="line">four_hikes_shocks[[<span class="hljs-string">&apos;d_ln_m2&apos;</span>,<span class="hljs-string">&apos;d_dff&apos;</span>]]</span><br></pre></td></tr></tbody></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;</code></pre>
<p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>d_ln_m2</th>
      <th>d_dff</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>2021-04-01</th>
      <td>0.015029</td>
      <td>0.01</td>
    </tr>
    <tr>
      <th>2021-07-01</th>
      <td>0.025704</td>
      <td>0.01</td>
    </tr>
    <tr>
      <th>2021-10-01</th>
      <td>0.003989</td>
      <td>0.02</td>
    </tr>
    <tr>
      <th>2022-01-01</th>
      <td>0.007853</td>
      <td>0.00</td>
    </tr>
    <tr>
      <th>2022-04-01</th>
      <td>0.013799</td>
      <td>0.03</td>
    </tr>
    <tr>
      <th>2022-07-01</th>
      <td>0.022938</td>
      <td>0.20</td>
    </tr>
    <tr>
      <th>2022-10-01</th>
      <td>0.009184</td>
      <td>0.01</td>
    </tr>
    <tr>
      <th>2023-01-01</th>
      <td>0.011144</td>
      <td>0.02</td>
    </tr>
    <tr>
      <th>2023-04-01</th>
      <td>0.009073</td>
      <td>0.06</td>
    </tr>
    <tr>
      <th>2023-07-01</th>
      <td>0.008134</td>
      <td>0.25</td>
    </tr>
    <tr>
      <th>2023-10-01</th>
      <td>0.010906</td>
      <td>0.25</td>
    </tr>
    <tr>
      <th>2024-01-01</th>
      <td>0.005544</td>
      <td>0.20</td>
    </tr>
  </tbody>
</table>
</div>




<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">mid_no_hikes = vecm_fit.predict(steps=<span class="hljs-number">12</span>,exog_fc =no_hikes_shocks[exog])</span><br><span class="line">pred_mid_no_hikes = pd.DataFrame(np.exp(mid_no_hikes), columns=[<span class="hljs-string">&apos;pred_rc&apos;</span>,<span class="hljs-string">&apos;pred_rdy&apos;</span>,<span class="hljs-string">&apos;pred_rnw&apos;</span>], index = forecast_range)</span><br><span class="line"></span><br><span class="line">mid, lower_1se, upper_1se = vecm_fit.predict(steps=<span class="hljs-number">12</span>,alpha = <span class="hljs-number">0.32</span>, exog_fc =four_hikes_shocks[exog])</span><br><span class="line">_, lower_2se, upper_2se = vecm_fit.predict(steps=<span class="hljs-number">12</span>,alpha = <span class="hljs-number">0.05</span>, exog_fc =four_hikes_shocks[exog])</span><br><span class="line"></span><br><span class="line">pred_mid = pd.DataFrame(np.exp(mid), columns=[<span class="hljs-string">&apos;pred_rc&apos;</span>,<span class="hljs-string">&apos;pred_rdy&apos;</span>,<span class="hljs-string">&apos;pred_rnw&apos;</span>], index = forecast_range)</span><br><span class="line">pred_lower_1se = pd.DataFrame(np.exp(lower_1se), columns=[<span class="hljs-string">&apos;lower_1se_rc&apos;</span>,<span class="hljs-string">&apos;lower_1se_rdy&apos;</span>,<span class="hljs-string">&apos;lower_1se_rnw&apos;</span>], index = forecast_range)</span><br><span class="line">pred_upper_1se = pd.DataFrame(np.exp(upper_1se), columns=[<span class="hljs-string">&apos;upper_1se_rc&apos;</span>,<span class="hljs-string">&apos;upper_1se_rdy&apos;</span>,<span class="hljs-string">&apos;upper_1se_rnw&apos;</span>], index = forecast_range)</span><br><span class="line">pred_lower_2se = pd.DataFrame(np.exp(lower_2se), columns=[<span class="hljs-string">&apos;lower_2se_rc&apos;</span>,<span class="hljs-string">&apos;lower_2se_rdy&apos;</span>,<span class="hljs-string">&apos;lower_2se_rnw&apos;</span>], index = forecast_range)</span><br><span class="line">pred_upper_2se = pd.DataFrame(np.exp(upper_2se), columns=[<span class="hljs-string">&apos;upper_2se_rc&apos;</span>,<span class="hljs-string">&apos;upper_2se_rdy&apos;</span>,<span class="hljs-string">&apos;upper_2se_rnw&apos;</span>], index = forecast_range)</span><br><span class="line">pred = pd.concat([pred_mid, pred_lower_1se, pred_upper_1se, pred_lower_2se, pred_upper_2se],axis = <span class="hljs-number">1</span>)</span><br></pre></td></tr></tbody></table></figure>


<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">plot_forecasts_given_different_paths</span>(<span class="hljs-params">df,pred1,pred2</span>):</span></span><br><span class="line">    fig, axes = plt.subplots(<span class="hljs-number">3</span>, <span class="hljs-number">1</span>, figsize=(<span class="hljs-number">10</span>,<span class="hljs-number">10</span>))</span><br><span class="line">    count=<span class="hljs-number">0</span></span><br><span class="line">    <span class="hljs-keyword">for</span> endog <span class="hljs-keyword">in</span> [<span class="hljs-string">&apos;rc&apos;</span>,<span class="hljs-string">&apos;rdy&apos;</span>,<span class="hljs-string">&apos;rnw&apos;</span>]:</span><br><span class="line">        col=count</span><br><span class="line">        ax=axes[col]</span><br><span class="line">        ax.set_title(endog)</span><br><span class="line">        ax.plot(df[endog], color=<span class="hljs-string">&apos;Black&apos;</span>,linewidth=<span class="hljs-number">1.2</span>,label=<span class="hljs-string">&quot;Observed&quot;</span>)</span><br><span class="line">        ax.plot(pred1[<span class="hljs-string">&apos;pred_&apos;</span>+endog],<span class="hljs-string">&apos;--&apos;</span>,color=<span class="hljs-string">&apos;tab:blue&apos;</span>, linewidth=<span class="hljs-number">1.5</span>,label=<span class="hljs-string">&quot;Forecast - no hikes&quot;</span>)</span><br><span class="line">        ax.plot(pred2[<span class="hljs-string">&apos;pred_&apos;</span>+endog],<span class="hljs-string">&apos;--&apos;</span>,color=<span class="hljs-string">&apos;tab:red&apos;</span>, linewidth=<span class="hljs-number">1.5</span>,label=<span class="hljs-string">&quot;Forecast - four hikes&quot;</span>)</span><br><span class="line">        ax.fill_between(pred2[<span class="hljs-string">&apos;pred_&apos;</span>+endog].index,pred2[<span class="hljs-string">&apos;lower_1se_&apos;</span>+endog],pred2[<span class="hljs-string">&apos;upper_1se_&apos;</span>+endog],alpha=<span class="hljs-number">0.07</span>,color=<span class="hljs-string">&quot;tab:red&quot;</span>)</span><br><span class="line">        ax.fill_between(pred2[<span class="hljs-string">&apos;pred_&apos;</span>+endog].index,pred2[<span class="hljs-string">&apos;lower_2se_&apos;</span>+endog],pred2[<span class="hljs-string">&apos;upper_2se_&apos;</span>+endog],alpha=<span class="hljs-number">0.07</span>,color=<span class="hljs-string">&quot;tab:red&quot;</span>)</span><br><span class="line">        ax.legend(loc =<span class="hljs-string">&apos;upper left&apos;</span>)</span><br><span class="line">        ax.axvline(x=datetime(<span class="hljs-number">2022</span>, <span class="hljs-number">7</span>, <span class="hljs-number">1</span>), color=<span class="hljs-string">&apos;tab:red&apos;</span>)</span><br><span class="line">        ax.axvline(x=datetime(<span class="hljs-number">2023</span>, <span class="hljs-number">7</span>, <span class="hljs-number">1</span>), color=<span class="hljs-string">&apos;tab:red&apos;</span>)</span><br><span class="line">        ax.axvline(x=datetime(<span class="hljs-number">2023</span>, <span class="hljs-number">10</span>, <span class="hljs-number">1</span>), color=<span class="hljs-string">&apos;tab:red&apos;</span>)</span><br><span class="line">        ax.axvline(x=datetime(<span class="hljs-number">2024</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>), color=<span class="hljs-string">&apos;tab:red&apos;</span>)</span><br><span class="line">        count+=<span class="hljs-number">1</span></span><br></pre></td></tr></tbody></table></figure>

<p>In March 2020, in response to the economic impact of the pandemic, the Fed acted quickly having learned its lesson from the financial crisis and cut short-term interest rates to zero. It also restarted its large-scale asset purchases and since June 2020, the Fed has been buying 80 billion of Treasury securities and 40 billion of mortgage-backed securities each month, with its balance sheet swelling from 4.4 trillion to 8.6 trillion. The extreme monetary support combined with a huge fiscal stimulus package holds the U.S economy and shields American household incomes. Though plots won&#x2019;t be shown here, the model forecasts a much lower rdy and rnw if not taking into account the rapid increase of money base. Therefore, in the last 20 months, we all see the quick recovery of consumption and a rapid growth of asset prices, which is also reflected in the steeper slope of the third graph. However, as the economy rebounded, it may no longer need such extreme measures of support and keeping them in place could do more harm than good. For example, low mortgage rates have fueled a boom in house prices, but the problems that now afflict the economy are mostly supply issues while demand, which the bond buys most directly affects. Therefore, the policy makers may start to hit the brakes. That&#x2019;s also why we are seeing Fed officials talking about tapering the pace of its bond purchases and lately laying the groundwork for higher rates.</p>
<h3 id="1-Growth-rates-return-to-normal"><a href="#1-Growth-rates-return-to-normal" class="headerlink" title="1. Growth rates return to normal"></a>1. Growth rates return to normal</h3><p><strong>First it&#x2019;s not surprising that real consumption, disposable income and household net wealth are projected to grow at milder rates comparing to those when the economy was bottoming out from the slump. As the stimulus aids wear off over time, one would expect the growth rates to normalize to pre-pandemic levels.</strong></p>
<h3 id="2-Growth-differences-taking-expansionary-and-tight-monetary-policy"><a href="#2-Growth-differences-taking-expansionary-and-tight-monetary-policy" class="headerlink" title="2. Growth differences taking expansionary and tight monetary policy"></a>2. Growth differences taking expansionary and tight monetary policy</h3><p><strong>Secondly, assuming a tighter monetary policy to come, we project that the growth rate of real consumption throughout 2023 would slow down to 2.2% from 3.0%. The real income and net wealth of households would decrease from 3.9% and 9.1% to 2.4% and 6.5% respectively. At the end of 2023, all three measures would be lower than those come from the first path - that the Fed sticks to its current rate target and asset buying pace. In fact the amounts would be lowered by 1.8%, 3.3% and 4.8% respectively.</strong></p>
<h3 id="3-Foreseeable-shallow-decline-in-mid-2022"><a href="#3-Foreseeable-shallow-decline-in-mid-2022" class="headerlink" title="3. Foreseeable shallow decline in mid 2022"></a>3. Foreseeable shallow decline in mid 2022</h3><p><strong>Thirdly, we also notice that in the middle of 2022, the private consumption and household income would show a temporary and shallow decline and then grow at an annual rate again coming close to the path observed just before the pandemic.</strong> Likely this is caused by the tightening of policy in 2022. So far, Powell has announced that the fed will begin tapering this month, the first step toward pulling back on the massive amount of help it had been providing markets and the economy. One the fiscal side, though public programs to support households and businesses have been the most powerful engine of recovery from the hit, governments of the major economies now are hitting the brakes. The money they&#x2019; ll pull out of their economies in 2022 amounts to 2.5 percentage points of the world&#x2019;s GDP, five times bigger than anything that happened during the turn to austerity after the 2008 crisis, according to UBS estimates. The fiscal tightening - the withdrawal of pandemic spending will likely have more impact on the global economy next year.</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">plot_forecasts_given_different_paths(df.iloc[<span class="hljs-number">220</span>:],pred_mid_no_hikes,pred)</span><br></pre></td></tr></tbody></table></figure>

<img src="http://yumeng-li.github.io/VECM_files/VECM_105_0.png">

</body></html>
    
    </div>
    
    
</article>




    
        <article class="article content gallery" itemscope itemprop="blogPost">
    <h1 class="article-title is-size-3 is-size-4-mobile" itemprop="name">
        
            <a href="/2021/11/16/VECM/" itemprop="url">Predicting Post-Pandemic Consumption Behaviors Given Potential Paths of Monetary Policy, Part 1 -- Economic Theories, Structural Breaks and Stationarity</a>
        
    </h1>
    <div class="article-meta columns is-variable is-1 is-multiline is-mobile is-size-7-mobile">
        <span class="column is-narrow">
            <time datetime="2021-11-16T17:00:00.000Z" itemprop="datePublished">Nov 16 2021</time>
        </span>
        
        <span class="column is-narrow article-category">
            <i class="far fa-folder"></i>
            <a class="article-category-link" href="/categories/Time-Series-Analysis/">Time Series Analysis</a>
        </span>
        
        
        <span class="column is-narrow">
            
            
            29 minutes read (About 4355 words)
        </span>
        
    </div>
    <div class="article-entry is-size-6-mobile" itemprop="articleBody">
    
        <html><head></head><body><p>It&#x2019;s official on Nov.3rd that the Federal Reserve is winding down its aggressive pandemic-era asset purchasing, or as what we call &#x201C;tapering&#x201D;. The announcement actually didn&#x2019;t come out as a surprise since Fed officials had been laying the groundwork for policy tightening as the economy gradually recovered from the Covid19 slump. Furthermore, due to the lately surging inflation readings, we are seeing traders start to bet on faster Fed hikes. So it seems interesting to get some ideas of how the post-pandemic consumption behaviors of the American households would be, and quantify the potential impact of policy shifting on these behaviors using two classic time series models - the <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Vector_autoregression">Vector Autoregression (VAR)</a> and the <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Error_correction_model#VECM">Vector Error Correction Model (VECM)</a>. Tribute to Christopher Sims who first promoted the use of VAR in empirical macroeconomics. </p>
<p>We will build the models following the steps below, </p>
<img src="http://yumeng-li.github.io/VECM_files/flowchart.png">

<p>And before we get onto that, I will introduce some of the economic theories behind predicting real consumption.</p>
<h3 id="Economic-Intuitions-Behind"><a href="#Economic-Intuitions-Behind" class="headerlink" title="Economic Intuitions Behind"></a>Economic Intuitions Behind</h3><p>Let&#x2019;s first recap some of the main determinants of private consumption,</p>
<ul>
<li>Disposable Income (in the current period) (+)</li>
<li>Expectations (consumer confidence indexes, employment growth) (+)</li>
<li>Wealth (stock market performance, housing prices) (+)</li>
<li>Uncertainty (precautionary saving) (-)</li>
<li>Availability of Credit (+)</li>
<li>Real Interest Rate (?)<br>&#xA0; &#xA0; - Substitution effect (-)<br>&#xA0; &#xA0; - Income effect (+)</li>
</ul>
<p>You might remember that perhaps the simplest theories of consumption, Keynesian theories of consumption, would link private consumption to <strong>disposable income</strong> in the current period, and of course positively. So as disposable income goes up, private consumption is expected to go up as well. More sophisticated, forward looking models would give more prominence or link consumption not to disposable income today but to <strong>expectations</strong> about the evolution of disposable income in the future. A number of indicators that can be used to gauge these expectations, for example, consumer confidence index, employment growth or my personally preferred - yield spreads. These can be used as a proxy for expectations for future income.</p>
<p>Another variable thought to be important by economists in terms of explaining the behavior of private consumption is <strong>wealth</strong>. In that sense, for this wealth channel, the performance of the stock market or even the behavior of housing prices, would have an impact on private consumption.</p>
<p>Another important factor is <strong>uncertainty</strong>, which would actually have a negative association with private consumption because it would create incentives for what the economists called precautionary saving. So agents will try to create a buffer to be able to fall back on in case a bad shock hits.</p>
<p><strong>The availability of credit</strong> is also thought to be an important determinant with, obviously, a positive relationship.</p>
<p>And finally, the <strong>real interest rate</strong> is also thought to be one of the important determinants of private consumption.<br>The effect of real interest on consumption can be a little bit ambiguous. Perhaps the most straightforward or direct effect<br>would be what economists call a substitution effect where, as interest rates go up, the incentives to consume are reduced. So agents will have, on the contrary, incentives to save more. And therefore, an increase in rates will be linked to a decrease in private consumption. But on the other hand, it is also possible that if consumers are net savers the increase in interest rates<br>might generate some additional income&#x2013; what economists call an income effect&#x2013;and this might actually lead to an increase in private consumption.</p>
<h2 id="Getting-Data"><a href="#Getting-Data" class="headerlink" title="Getting Data"></a>Getting Data</h2><p>Based on the economic intuitions, I grabbed the following well-defined and organized economic data from the <a target="_blank" rel="noopener" href="https://fred.stlouisfed.org/">Federal Reserve Economic Data</a> of the St. Louis Fed. The FRED allows modelers to write programs and build applications that retrieve data through an API provided. There is a list of <a target="_blank" rel="noopener" href="https://fred.stlouisfed.org/docs/api/fred/">packages/libraries</a> that one can turn to, depending on one&#x2019;s programming language. Since I have to model everything in Python as it&#x2019;s compatible with the github&#xA0;blog, I use the library <a target="_blank" rel="noopener" href="https://github.com/mortada/fredapi">fredapi</a> to request the data. However, in practice, I would probably go for EViews as most economists prefer and do. </p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd</span><br><span class="line"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np</span><br><span class="line"><span class="hljs-keyword">from</span> fredapi <span class="hljs-keyword">import</span> Fred</span><br><span class="line"><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt</span><br><span class="line"><span class="hljs-keyword">from</span> datetime <span class="hljs-keyword">import</span> datetime</span><br><span class="line">%matplotlib inline</span><br><span class="line"></span><br><span class="line"><span class="hljs-keyword">import</span> statsmodels.api <span class="hljs-keyword">as</span> sm</span><br><span class="line"><span class="hljs-keyword">from</span> statsmodels.tsa.stattools <span class="hljs-keyword">import</span> adfuller</span><br><span class="line"><span class="hljs-keyword">from</span> statsmodels.tsa.stattools <span class="hljs-keyword">import</span> kpss</span><br><span class="line"><span class="hljs-keyword">from</span> statsmodels.tsa.api <span class="hljs-keyword">import</span> VAR</span><br><span class="line"><span class="hljs-keyword">from</span> statsmodels.tsa.vector_ar <span class="hljs-keyword">import</span> vecm</span><br><span class="line"><span class="hljs-keyword">from</span> statsmodels.tsa <span class="hljs-keyword">import</span> filters</span><br><span class="line"></span><br><span class="line">pd.options.display.max_colwidth = <span class="hljs-number">50</span></span><br><span class="line">pd.options.display.max_rows = <span class="hljs-number">400</span></span><br></pre></td></tr></tbody></table></figure>


<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">fred = Fred(api_key=your_key_string)</span><br></pre></td></tr></tbody></table></figure>

<p>The period I study is from 1962Q1 to 2021Q2, which ensures that the economic data that I am interested in all has a value. </p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">startDate = <span class="hljs-string">&apos;1962-01-01&apos;</span></span><br><span class="line">endDate = <span class="hljs-string">&apos;2021-04-01&apos;</span></span><br></pre></td></tr></tbody></table></figure>


<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">get_series_and_print_info</span>(<span class="hljs-params">fred, series_id, startDate, endDate, name</span>):</span></span><br><span class="line">&#xA0; &#xA0; series = fred.get_series(series_id, observation_start=startDate, observation_end=endDate, frequency=<span class="hljs-string">&apos;q&apos;</span>)</span><br><span class="line">&#xA0; &#xA0; print(name+<span class="hljs-string">&apos;: &apos;</span>+fred.get_series_info(series_id).title)</span><br><span class="line">&#xA0; &#xA0; <span class="hljs-keyword">return</span> series</span><br></pre></td></tr></tbody></table></figure>


<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">data = {}</span><br><span class="line">data[<span class="hljs-string">&apos;saving_rate&apos;</span>] = get_series_and_print_info(fred, <span class="hljs-string">&apos;a072rc1q156sbea&apos;</span>,startDate, endDate, <span class="hljs-string">&apos;saving_rate&apos;</span>)</span><br><span class="line">data[<span class="hljs-string">&apos;ns&apos;</span>] = get_series_and_print_info(fred, <span class="hljs-string">&apos;w986rc1q027sbea&apos;</span>,startDate, endDate, <span class="hljs-string">&apos;ns&apos;</span>)</span><br><span class="line">data[<span class="hljs-string">&apos;nnw&apos;</span>] = get_series_and_print_info(fred, <span class="hljs-string">&apos;TNWBSHNO&apos;</span>,startDate, endDate, <span class="hljs-string">&apos;nnw&apos;</span>)</span><br><span class="line">data[<span class="hljs-string">&apos;nl&apos;</span>] = get_series_and_print_info(fred, <span class="hljs-string">&apos;TCMILBSHNO&apos;</span>,startDate, endDate, <span class="hljs-string">&apos;nl&apos;</span>)</span><br><span class="line">data[<span class="hljs-string">&apos;na&apos;</span>] = get_series_and_print_info(fred, <span class="hljs-string">&apos;TABSHNO&apos;</span>,startDate, endDate, <span class="hljs-string">&apos;na&apos;</span>)</span><br><span class="line">data[<span class="hljs-string">&apos;nfa&apos;</span>] = get_series_and_print_info(fred, <span class="hljs-string">&apos;TFAABSHNO&apos;</span>,startDate, endDate, <span class="hljs-string">&apos;nfa&apos;</span>)</span><br><span class="line">data[<span class="hljs-string">&apos;interest_payments&apos;</span>] = get_series_and_print_info(fred, <span class="hljs-string">&apos;b069rc1q027sbea&apos;</span>,startDate, endDate, <span class="hljs-string">&apos;interest_payments&apos;</span>)</span><br><span class="line">data[<span class="hljs-string">&apos;cpi&apos;</span>] = get_series_and_print_info(fred, <span class="hljs-string">&apos;cpiaucsl&apos;</span>,startDate, endDate, <span class="hljs-string">&apos;cpi&apos;</span>)</span><br><span class="line">data[<span class="hljs-string">&apos;c_deflator&apos;</span>] = get_series_and_print_info(fred, <span class="hljs-string">&apos;dpcerd3q086sbea&apos;</span>,startDate, endDate, <span class="hljs-string">&apos;c_deflator&apos;</span>)/<span class="hljs-number">100</span></span><br><span class="line">data[<span class="hljs-string">&apos;dff&apos;</span>] = get_series_and_print_info(fred, <span class="hljs-string">&apos;DFF&apos;</span>,startDate, endDate, <span class="hljs-string">&apos;dff&apos;</span>)</span><br><span class="line">data[<span class="hljs-string">&apos;yield_3m&apos;</span>] = get_series_and_print_info(fred, <span class="hljs-string">&apos;dgs3&apos;</span>,startDate, endDate, <span class="hljs-string">&apos;yield_3m&apos;</span>)</span><br><span class="line">data[<span class="hljs-string">&apos;yield_10y&apos;</span>] = get_series_and_print_info(fred, <span class="hljs-string">&apos;irltlt01usq156n&apos;</span>,startDate, endDate, <span class="hljs-string">&apos;yield_10y&apos;</span>)</span><br><span class="line">data[<span class="hljs-string">&apos;prime_rate&apos;</span>] = get_series_and_print_info(fred, <span class="hljs-string">&apos;dprime&apos;</span>,startDate, endDate, <span class="hljs-string">&apos;prime_rate&apos;</span>)</span><br><span class="line">data[<span class="hljs-string">&apos;m2&apos;</span>] = get_series_and_print_info(fred, <span class="hljs-string">&apos;M2REAL&apos;</span>,startDate, endDate, <span class="hljs-string">&apos;m2&apos;</span>)</span><br><span class="line">data[<span class="hljs-string">&apos;ndy&apos;</span>] = get_series_and_print_info(fred, <span class="hljs-string">&apos;dpi&apos;</span>,startDate, endDate, <span class="hljs-string">&apos;ndy&apos;</span>)</span><br><span class="line">data[<span class="hljs-string">&apos;rdy&apos;</span>] = get_series_and_print_info(fred, <span class="hljs-string">&apos;dpic96&apos;</span>,startDate, endDate, <span class="hljs-string">&apos;rdy&apos;</span>)</span><br><span class="line">data[<span class="hljs-string">&apos;npiy&apos;</span>] = get_series_and_print_info(fred, <span class="hljs-string">&apos;b703rc1q027sbea&apos;</span>,startDate, endDate, <span class="hljs-string">&apos;npiy&apos;</span>)</span><br><span class="line">data[<span class="hljs-string">&apos;ny&apos;</span>] = get_series_and_print_info(fred, <span class="hljs-string">&apos;pincome&apos;</span>,startDate, endDate, <span class="hljs-string">&apos;ny&apos;</span>)</span><br><span class="line">data[<span class="hljs-string">&apos;ry&apos;</span>] = get_series_and_print_info(fred, <span class="hljs-string">&apos;rpi&apos;</span>,startDate, endDate, <span class="hljs-string">&apos;ry&apos;</span>)</span><br><span class="line">data[<span class="hljs-string">&apos;nc&apos;</span>] = get_series_and_print_info(fred, <span class="hljs-string">&apos;pcec&apos;</span>,startDate, endDate, <span class="hljs-string">&apos;nc&apos;</span>)</span><br><span class="line">data[<span class="hljs-string">&apos;rc&apos;</span>] = get_series_and_print_info(fred, <span class="hljs-string">&apos;pcecc96&apos;</span>,startDate, endDate, <span class="hljs-string">&apos;rc&apos;</span>)</span><br><span class="line">data[<span class="hljs-string">&apos;nc_services&apos;</span>] = get_series_and_print_info(fred, <span class="hljs-string">&apos;pcesv&apos;</span>,startDate, endDate, <span class="hljs-string">&apos;nc_services&apos;</span>)</span><br><span class="line">data[<span class="hljs-string">&apos;nc_durable_goods&apos;</span>] = get_series_and_print_info(fred, <span class="hljs-string">&apos;pcdg&apos;</span>,startDate, endDate, <span class="hljs-string">&apos;nc_durable_goods&apos;</span>)</span><br><span class="line">data[<span class="hljs-string">&apos;unemp&apos;</span>] = get_series_and_print_info(fred, <span class="hljs-string">&apos;unrate&apos;</span>,startDate, endDate, <span class="hljs-string">&apos;unemp&apos;</span>)</span><br><span class="line">data[<span class="hljs-string">&apos;house_prices&apos;</span>] = get_series_and_print_info(fred, <span class="hljs-string">&apos;ussthpi&apos;</span>,startDate, endDate, <span class="hljs-string">&apos;house_prices&apos;</span>)</span><br><span class="line">data[<span class="hljs-string">&apos;mortgage_30y&apos;</span>] = get_series_and_print_info(fred, <span class="hljs-string">&apos;MORTGAGE30US&apos;</span>,startDate, endDate, <span class="hljs-string">&apos;mortgage_30y&apos;</span>)</span><br><span class="line">data[<span class="hljs-string">&apos;consumer_confidence&apos;</span>] = get_series_and_print_info(fred, <span class="hljs-string">&apos;CSCICP03USM665S&apos;</span>,startDate, endDate, <span class="hljs-string">&apos;consumer_confidence&apos;</span>)</span><br><span class="line">data[<span class="hljs-string">&apos;gov_debt&apos;</span>] = get_series_and_print_info(fred, <span class="hljs-string">&apos;fygfdpun&apos;</span>,startDate, endDate, <span class="hljs-string">&apos;gov_debt&apos;</span>)</span><br><span class="line">data[<span class="hljs-string">&apos;gov_transfers&apos;</span>] = get_series_and_print_info(fred, <span class="hljs-string">&apos;w211rc1q027sbea&apos;</span>,startDate, endDate, <span class="hljs-string">&apos;gov_transfers&apos;</span>)</span><br></pre></td></tr></tbody></table></figure>

<p>&#xA0; &#xA0; saving_rate: Personal saving as a percentage of disposable personal income<br>&#xA0; &#xA0; ns: Net private saving: Households and institutions<br>&#xA0; &#xA0; nnw: Households and Nonprofit Organizations; Net Worth, Level<br>&#xA0; &#xA0; nl: Households and Nonprofit Organizations; Debt Securities and Loans; Liability, Level<br>&#xA0; &#xA0; na: Households and Nonprofit Organizations; Total Assets, Level<br>&#xA0; &#xA0; nfa: Households and Nonprofit Organizations; Total Financial Assets, Level<br>&#xA0; &#xA0; interest_payments: Personal interest payments<br>&#xA0; &#xA0; cpi: Consumer Price Index for All Urban Consumers: All Items in U.S. City Average<br>&#xA0; &#xA0; c_deflator: Personal consumption expenditures (implicit price deflator)<br>&#xA0; &#xA0; dff: Federal Funds Effective Rate<br>&#xA0; &#xA0; yield_3m: Market Yield on U.S. Treasury Securities at 3-Year Constant Maturity<br>&#xA0; &#xA0; yield_10y: Long-Term Government Bond Yields: 10-year: Main (Including Benchmark) for the United States<br>&#xA0; &#xA0; prime_rate: Bank Prime Loan Rate<br>&#xA0; &#xA0; m2: Real M2 Money Stock<br>&#xA0; &#xA0; ndy: Disposable Personal Income<br>&#xA0; &#xA0; rdy: Real Disposable Personal Income<br>&#xA0; &#xA0; npiy: Personal income receipts on assets: Personal dividend income<br>&#xA0; &#xA0; ny: Personal Income<br>&#xA0; &#xA0; ry: Real Personal Income<br>&#xA0; &#xA0; nc: Personal Consumption Expenditures<br>&#xA0; &#xA0; rc: Real Personal Consumption Expenditures<br>&#xA0; &#xA0; nc_services: Personal Consumption Expenditures: Services<br>&#xA0; &#xA0; nc_durable_goods: Personal Consumption Expenditures: Durable Goods<br>&#xA0; &#xA0; unemp: Unemployment Rate<br>&#xA0; &#xA0; house_prices: All-Transactions House Price Index for the United States<br>&#xA0; &#xA0; mortgage_30y: 30-Year Fixed Rate Mortgage Average in the United States<br>&#xA0; &#xA0; consumer_confidence: Consumer Opinion Surveys: Confidence Indicators: Composite Indicators: OECD Indicator for the United States<br>&#xA0; &#xA0; gov_debt: Federal Debt Held by the Public<br>&#xA0; &#xA0; gov_transfers: Personal current transfer payments</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df = pd.DataFrame(data)</span><br></pre></td></tr></tbody></table></figure>

<p>Since we are interested in predicting the <strong>real consumption</strong>, which means eliminating the effect of prices, nominal variables including consumption, disposable income and net wealth need to be converted into real terms. Here they are deflated by the price deflator for consumption (c_deflator). </p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">df[<span class="hljs-string">&apos;rnw&apos;</span>] = df[<span class="hljs-string">&apos;nnw&apos;</span>]/df[<span class="hljs-string">&apos;c_deflator&apos;</span>]</span><br><span class="line">df[<span class="hljs-string">&apos;rl&apos;</span>] = df[<span class="hljs-string">&apos;nl&apos;</span>]/df[<span class="hljs-string">&apos;c_deflator&apos;</span>]</span><br><span class="line">df[<span class="hljs-string">&apos;ra&apos;</span>] = df[<span class="hljs-string">&apos;na&apos;</span>]/df[<span class="hljs-string">&apos;c_deflator&apos;</span>]</span><br><span class="line">df[<span class="hljs-string">&apos;rfa&apos;</span>] = df[<span class="hljs-string">&apos;nfa&apos;</span>]/df[<span class="hljs-string">&apos;c_deflator&apos;</span>]</span><br></pre></td></tr></tbody></table></figure>

<h2 id="Data-Visualization-and-Preliminary-Analysis-Some-Fun-Ideas"><a href="#Data-Visualization-and-Preliminary-Analysis-Some-Fun-Ideas" class="headerlink" title="Data Visualization and Preliminary Analysis (Some Fun Ideas)"></a>Data Visualization and Preliminary Analysis (Some Fun Ideas)</h2><h3 id="Structural-Breaks"><a href="#Structural-Breaks" class="headerlink" title="Structural Breaks"></a>Structural Breaks</h3><p>To get a feel for the data, let&#x2019;s plot log(rc/rdy) against log(rnw/rdy). Also regress log(rc/rdy) on a constant and log(rnw/rdy) and then examine the fitted residuals, looking for evidence of any structural changes in the relationship between consumption, disposable income and net wealth. </p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">fig, ax1 = plt.subplots()</span><br><span class="line">ax1.set_xlabel(<span class="hljs-string">&apos;Year&apos;</span>)</span><br><span class="line">ax1.set_ylabel(<span class="hljs-string">&apos;rdy rl&apos;</span>)</span><br><span class="line">ax1.plot(df[<span class="hljs-string">&apos;rc&apos;</span>], color=<span class="hljs-string">&apos;tab:grey&apos;</span>,label=<span class="hljs-string">&apos;real consumption&apos;</span>)</span><br><span class="line">ax1.plot(df[<span class="hljs-string">&apos;rdy&apos;</span>], color=<span class="hljs-string">&apos;tab:blue&apos;</span>,label=<span class="hljs-string">&apos;real disposable income&apos;</span>)</span><br><span class="line">ax1.plot(df[<span class="hljs-string">&apos;rl&apos;</span>], color=<span class="hljs-string">&apos;tab:green&apos;</span>,label=<span class="hljs-string">&apos;real liability&apos;</span>)</span><br><span class="line">ax1.legend(loc=<span class="hljs-string">&apos;upper left&apos;</span>)</span><br><span class="line">ax2 = ax1.twinx() &#xA0;</span><br><span class="line">ax2.set_ylabel(<span class="hljs-string">&apos;rnw ra rfa&apos;</span>) </span><br><span class="line">ax2.plot(df[<span class="hljs-string">&apos;rnw&apos;</span>], color=<span class="hljs-string">&apos;tab:red&apos;</span>,label=<span class="hljs-string">&apos;real net worth&apos;</span>)</span><br><span class="line">ax2.plot(df[<span class="hljs-string">&apos;ra&apos;</span>], color=<span class="hljs-string">&apos;tab:orange&apos;</span>,label=<span class="hljs-string">&apos;real asset&apos;</span>)</span><br><span class="line">ax2.plot(df[<span class="hljs-string">&apos;rfa&apos;</span>], color=<span class="hljs-string">&apos;tab:purple&apos;</span>,label=<span class="hljs-string">&apos;real fincl asset&apos;</span>)</span><br><span class="line">ax2.legend(loc=<span class="hljs-string">&apos;lower right&apos;</span>)</span><br><span class="line">fig.tight_layout() </span><br><span class="line">plt.show()</span><br></pre></td></tr></tbody></table></figure>

<img src="http://yumeng-li.github.io/VECM_files/VECM_14_0.png">



<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">data1 = df[<span class="hljs-string">&apos;rc&apos;</span>]/df[<span class="hljs-string">&apos;rdy&apos;</span>]</span><br><span class="line">data2 = df[<span class="hljs-string">&apos;rc&apos;</span>]/df[<span class="hljs-string">&apos;rnw&apos;</span>]</span><br><span class="line">data3 = df[<span class="hljs-string">&apos;rc&apos;</span>]/df[<span class="hljs-string">&apos;rl&apos;</span>]</span><br><span class="line">data4 = df[<span class="hljs-string">&apos;rc&apos;</span>]/df[<span class="hljs-string">&apos;ra&apos;</span>]</span><br><span class="line">data5 = df[<span class="hljs-string">&apos;rc&apos;</span>]/df[<span class="hljs-string">&apos;rfa&apos;</span>]</span><br><span class="line">fig, ax1 = plt.subplots()</span><br><span class="line">ax1.set_xlabel(<span class="hljs-string">&apos;Year&apos;</span>)</span><br><span class="line">ax1.set_ylabel(<span class="hljs-string">&apos;rdy&apos;</span>)</span><br><span class="line">ax1.plot(data1, color=<span class="hljs-string">&apos;tab:blue&apos;</span>,label=<span class="hljs-string">&apos;rc/rdy&apos;</span>)</span><br><span class="line">ax1.legend(loc=<span class="hljs-string">&apos;upper left&apos;</span>)</span><br><span class="line">ax2 = ax1.twinx() &#xA0;</span><br><span class="line">ax2.set_ylabel(<span class="hljs-string">&apos;rnw ra rfa&apos;</span>) </span><br><span class="line">ax2.plot(data2, color=<span class="hljs-string">&apos;tab:red&apos;</span>,label=<span class="hljs-string">&apos;rc/rnw&apos;</span>)</span><br><span class="line">ax2.plot(data4, color=<span class="hljs-string">&apos;tab:orange&apos;</span>,label=<span class="hljs-string">&apos;rc/ra&apos;</span>)</span><br><span class="line">ax2.plot(data5, color=<span class="hljs-string">&apos;tab:purple&apos;</span>,label=<span class="hljs-string">&apos;rc/rfa&apos;</span>)</span><br><span class="line">ax2.legend(loc=<span class="hljs-string">&apos;upper right&apos;</span>)</span><br><span class="line">fig.tight_layout() </span><br><span class="line">plt.show()</span><br></pre></td></tr></tbody></table></figure>
<img src="http://yumeng-li.github.io/VECM_files/VECM_15_0.png">



<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">data1 = np.log(df[<span class="hljs-string">&apos;rc&apos;</span>]/df[<span class="hljs-string">&apos;rdy&apos;</span>])</span><br><span class="line">data2 = np.log(df[<span class="hljs-string">&apos;rnw&apos;</span>]/df[<span class="hljs-string">&apos;rdy&apos;</span>])</span><br><span class="line">fig, ax1 = plt.subplots()</span><br><span class="line">color = <span class="hljs-string">&apos;tab:green&apos;</span></span><br><span class="line">ax1.set_xlabel(<span class="hljs-string">&apos;Year&apos;</span>)</span><br><span class="line">ax1.set_ylabel(<span class="hljs-string">&apos;rc/rdy&apos;</span>, color=color)</span><br><span class="line">ax1.plot(data1, color=color)</span><br><span class="line">ax1.tick_params(axis=<span class="hljs-string">&apos;y&apos;</span>, labelcolor=color)</span><br><span class="line">ax2 = ax1.twinx() &#xA0;</span><br><span class="line">color = <span class="hljs-string">&apos;tab:blue&apos;</span></span><br><span class="line">ax2.set_ylabel(<span class="hljs-string">&apos;rnw/rdy&apos;</span>, color=color) </span><br><span class="line">ax2.plot(data2, color=color)</span><br><span class="line">ax2.tick_params(axis=<span class="hljs-string">&apos;y&apos;</span>, labelcolor=color)</span><br><span class="line">fig.tight_layout()</span><br><span class="line">ax1.axvline(x=datetime(<span class="hljs-number">1975</span>, <span class="hljs-number">10</span>, <span class="hljs-number">1</span>), color=<span class="hljs-string">&apos;k&apos;</span>)</span><br><span class="line">ax1.axvline(x=datetime(<span class="hljs-number">2008</span>, <span class="hljs-number">7</span>, <span class="hljs-number">1</span>), color=<span class="hljs-string">&apos;k&apos;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></tbody></table></figure>

<img src="http://yumeng-li.github.io/VECM_files/VECM_16_0.png">


<p>Are there any structural breaks in the relationship before 1975Q4 and after 2008Q3? If so, we have to create dummies to allow for them. Let&#x2019;s run a regression to further confirm it.</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Y = np.log(df[<span class="hljs-string">&apos;rc&apos;</span>]/df[<span class="hljs-string">&apos;rdy&apos;</span>])</span><br><span class="line">X = np.log(df[<span class="hljs-string">&apos;rnw&apos;</span>]/df[<span class="hljs-string">&apos;rdy&apos;</span>])</span><br><span class="line">X = sm.add_constant(X)</span><br><span class="line">model = sm.OLS(Y,X)</span><br><span class="line">res = model.fit()</span><br></pre></td></tr></tbody></table></figure>


<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">ax = res.resid[:<span class="hljs-number">-8</span>].plot(style=<span class="hljs-string">&apos;.&apos;</span>,grid=<span class="hljs-literal">True</span>)</span><br><span class="line">ax.xaxis.grid(<span class="hljs-literal">True</span>, linestyle=<span class="hljs-string">&apos;--&apos;</span>, linewidth=<span class="hljs-number">0.25</span>)</span><br><span class="line">ax.axvline(x=datetime(<span class="hljs-number">1975</span>, <span class="hljs-number">10</span>, <span class="hljs-number">1</span>), color=<span class="hljs-string">&apos;k&apos;</span>)</span><br><span class="line">ax.axvline(x=datetime(<span class="hljs-number">2008</span>, <span class="hljs-number">7</span>, <span class="hljs-number">1</span>), color=<span class="hljs-string">&apos;k&apos;</span>)</span><br></pre></td></tr></tbody></table></figure>




<img src="http://yumeng-li.github.io/VECM_files/VECM_19_1.png">



<p>Both the time plot and the regression of log(rc/rdy) on a constant and log(rnw/rdy) suggests that there were structural breaks around 1975Q4 and 2008Q3. Notice the consistently negative residuals till 1975Q4 and the wider gap between log(rc/rdy) and log(rnw/rdy) before 1976 compared to the gap after 1976. Another one is that household net worth and assets have increased at an accelerating rate after the 2008 global financial crisis (increasing rnw/rdy ratio). While at the same time, we don&#x2019;t see a rising trend of consumption compared to income even though households have accumulated higher real net wealth benefited from the booming asset prices post-crisis, which is a quite different behavior compared to pre-crisis.</p>
<p>These are strong suggestive evidence that structural breaks occurred around 1975Q4 and 2008Q3. <strong>But why? What are the drivers?</strong></p>
<h3 id="Inequality-and-the-Kondratiev-theory"><a href="#Inequality-and-the-Kondratiev-theory" class="headerlink" title="Inequality and the Kondratiev theory"></a>Inequality and the Kondratiev theory</h3><p>Let&#x2019;s now take a look at the inequality data, which will give us better clues to our questions. </p>
<p>Data is downloaded from the World Inequality Database <a target="_blank" rel="noopener" href="https://wid.world/data/">WID</a>. Buckets I selected are top 10%, middle 40% and the bottom 50% of the US population.</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">income_inequality = pd.read_csv(<span class="hljs-string">&apos;income_inequality.csv&apos;</span>,index_col=<span class="hljs-number">0</span>)</span><br><span class="line">wealth_inequality = pd.read_csv(<span class="hljs-string">&apos;wealth_inequality.csv&apos;</span>,index_col=<span class="hljs-number">0</span>)</span><br><span class="line">pretax_income = pd.read_csv(<span class="hljs-string">&apos;pretax_income.csv&apos;</span>,index_col=<span class="hljs-number">0</span>)</span><br><span class="line">net_wealth = pd.read_csv(<span class="hljs-string">&apos;net_wealth.csv&apos;</span>,index_col=<span class="hljs-number">0</span>)</span><br></pre></td></tr></tbody></table></figure>

<p>As we can tell from the following graphs, income inequality in the US has risen dramatically since the mid 1970s, see how the income of the top 10% grows whereas that of the middle and bottom classes remains nearly flat. This issue has drawn heightened attention in recent years (including me :D). </p>
<p>In the past decade, economic observers have also become increasingly worried about &#x201C;secular stagnation&#x201D; - or a chronic shortfall of aggregate demand, fearing that this shortfall will constrain American economic growth in coming years. These two phenomena&#x2014;<strong>rising inequality</strong> and <strong>chronic weakness of demand</strong> - are related. Specifically, since the <strong>marginal propensity to consume (MPC)</strong> is much lower at the higher wealth quintiles (for low-wealth households, the MPC is <strong>10 times larger</strong> than it is for wealthy households, see this <a target="_blank" rel="noopener" href="https://www.bostonfed.org/publications/research-department-working-paper/2019/estimating-the-marginal-propensity-to-consume-using-the-distributions-income-consumption-wealth.aspx">paper</a> elaborating it), rising inequality transfers income from high MPC households in the bottom and middle of the income distribution to lower MPC households at the top. All else equal, this redistribution away from low- to high-saving households reduces consumption spending, which drags on demand growth.</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pretax_income.iloc[:,<span class="hljs-number">1</span>:].plot(title=<span class="hljs-string">&apos;Pre-tax Income&apos;</span>)</span><br></pre></td></tr></tbody></table></figure>




<img src="http://yumeng-li.github.io/VECM_files/VECM_26_1.png">




<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">net_wealth.iloc[:,<span class="hljs-number">1</span>:].plot(title=<span class="hljs-string">&apos;Net Wealth&apos;</span>)</span><br></pre></td></tr></tbody></table></figure>




<p>&#xA0; &#xA0; &lt;matplotlib.axes._subplots.AxesSubplot at 0x23c76c39e48&gt;</p>
<img src="http://yumeng-li.github.io/VECM_files/VECM_27_1.png">



<p>Due to the differences in income growth, we see a larger and larger wealth gap between groups.</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">data1 = wealth_inequality[<span class="hljs-string">&apos;sw_p99p100&apos;</span>]</span><br><span class="line">data2 = wealth_inequality[<span class="hljs-string">&apos;sw_p90p100&apos;</span>]</span><br><span class="line">data3 = wealth_inequality[<span class="hljs-string">&apos;sw_p50p90&apos;</span>]</span><br><span class="line">data4 = wealth_inequality[<span class="hljs-string">&apos;sw_p0p50&apos;</span>]</span><br><span class="line"></span><br><span class="line">fig, ax1 = plt.subplots()</span><br><span class="line">color = <span class="hljs-string">&apos;tab:blue&apos;</span></span><br><span class="line">ax1.set_xlabel(<span class="hljs-string">&apos;Year&apos;</span>)</span><br><span class="line">ax1.set_ylabel(<span class="hljs-string">&apos;Rich&apos;</span>, color=color)</span><br><span class="line">ax1.plot(data1, color=<span class="hljs-string">&apos;royalblue&apos;</span>,label=<span class="hljs-string">&apos;Share of p99p100&apos;</span>)</span><br><span class="line">ax1.plot(data2, color=<span class="hljs-string">&apos;dodgerblue&apos;</span>,label=<span class="hljs-string">&apos;Share of p90p100&apos;</span>)</span><br><span class="line">ax1.tick_params(axis=<span class="hljs-string">&apos;y&apos;</span>, labelcolor=color)</span><br><span class="line">ax1.legend(loc =<span class="hljs-string">&apos;center left&apos;</span>)</span><br><span class="line">ax1.set_title(<span class="hljs-string">&apos;Wealth Inequality&apos;</span>)</span><br><span class="line">ax2 = ax1.twinx() &#xA0;</span><br><span class="line">color = <span class="hljs-string">&apos;tab:green&apos;</span></span><br><span class="line">ax2.set_ylabel(<span class="hljs-string">&apos;Poor&apos;</span>, color=color) </span><br><span class="line">ax2.plot(data3, color=<span class="hljs-string">&apos;limegreen&apos;</span>,label=<span class="hljs-string">&apos;Share of p50p90&apos;</span>)</span><br><span class="line">ax2.plot(data4, color=<span class="hljs-string">&apos;lightgreen&apos;</span>,label=<span class="hljs-string">&apos;Share of p0p50&apos;</span>)</span><br><span class="line">ax2.tick_params(axis=<span class="hljs-string">&apos;y&apos;</span>, labelcolor=color)</span><br><span class="line">fig.tight_layout() </span><br><span class="line">ax2.legend(loc =<span class="hljs-string">&apos;center right&apos;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></tbody></table></figure>

<img src="http://yumeng-li.github.io/VECM_files/VECM_29_0.png">


<p>Based on the theory, we can assume that there is a negative correlation between income inequality and consumption. Here I created an inequality indicator using the difference in shares of wealth of the top 10% and the bottom 50%, so that we can further look into the change of inequality over the last half century. The long-term trend is estimated using the <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Hodrick%E2%80%93Prescott_filter"><strong>Hodrick&#x2013;Prescott filter</strong></a>, which can smooth the data by getting rid of the short-term fluctuations (shorter cycles).</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">wealth_inequality.index = pd.to_datetime(wealth_inequality.index, format=<span class="hljs-string">&apos;%Y&apos;</span>)</span><br><span class="line"><span class="hljs-comment"># create a variable nw_inequlity for the wealth gap between top 10% and bottom 50%</span></span><br><span class="line">wealth_inequality[<span class="hljs-string">&apos;nw_inequality&apos;</span>] = wealth_inequality[<span class="hljs-string">&apos;sw_p90p100&apos;</span>]-wealth_inequality[<span class="hljs-string">&apos;sw_p50p90&apos;</span>]</span><br><span class="line"><span class="hljs-comment"># apply the Hodrick-Prescott filter to smooth the inequality data and estimate the long-term trend</span></span><br><span class="line">wealth_inequality[<span class="hljs-string">&apos;f_nw_inequality&apos;</span>] = pd.DataFrame(filters.hp_filter.hpfilter(wealth_inequality[<span class="hljs-string">&apos;nw_inequality&apos;</span>],lamb=<span class="hljs-number">6.25</span>)[<span class="hljs-number">1</span>])</span><br></pre></td></tr></tbody></table></figure>


<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">f, (ax1, ax2) = plt.subplots(<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,figsize=(<span class="hljs-number">15</span>,<span class="hljs-number">4</span>))</span><br><span class="line">ax1.plot(wealth_inequality[<span class="hljs-string">&apos;nw_inequality&apos;</span>])</span><br><span class="line">ax2.plot(wealth_inequality[<span class="hljs-string">&apos;f_nw_inequality&apos;</span>])</span><br><span class="line">ax1.set_title(<span class="hljs-string">&apos;nw inequality&apos;</span>)</span><br><span class="line">ax2.set_title(<span class="hljs-string">&apos;filtered nw inequality (trend)&apos;</span>)</span><br></pre></td></tr></tbody></table></figure>


<img src="http://yumeng-li.github.io/VECM_files/VECM_32_1.png">



<p>Seeing the long-term inequity trend, it&#x2019;s no wonder why we have identified two structural breaks. </p>
<p>Actually up to now, if you are familiar with the theory of Kondratieff cycles, this probably starts to look quite interesting to you. <strong>Kondratiev waves</strong> (also called super-cycles, great surges, long waves, K-waves or the long economic cycle) are hypothesized cycle-like phenomena in the modern world economy. The phenomenon is closely connected with the technology life cycle. It is stated that the period of a wave ranges from forty to sixty years, the cycles consist of alternating intervals of high sectoral growth and intervals of relatively slow growth. </p>
<p>Though there is a lack of agreement about both the cause of the waves and the start and end years of particular waves, <strong>inequity appears to be the most obvious driver</strong>, and yet some researchers have presented a technological and credit cycle explanation as well. However, among several modern timing versions of the cycles based on any of the causes, the consensus is that the start of our current cycle - Wave of the Information and Telecommunications Revolution falls into the range of early 1970s to mid 1980s, which happens to be the first break point we identified.</p>
<p>Every wave of innovations lasts approximately until the profits from the new innovation or sector fall to the level of other, older, more traditional sectors. It is a situation when the new technology, which originally increased a capacity to utilize new sources from nature, reached its limits and it is not possible to overcome this limit without an application of another new technology. For the end of an application phase of any wave there is typically an economic crisis and economic stagnation. The financial crisis of 2007&#x2013;2008 is viewed by some as a result of the coming end of the Wave of the Information and Telecommunications Revolution, our second break point.</p>
<p>So after all, I create two dummy variables below to represent the turning points of the cycle.</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">df[<span class="hljs-string">&apos;sb_1975_4&apos;</span>] = np.where(df.index &lt; <span class="hljs-string">&apos;1976-10-1&apos;</span>,<span class="hljs-number">0</span>,<span class="hljs-number">1</span>)</span><br><span class="line">df[<span class="hljs-string">&apos;sb_2008_3&apos;</span>] = np.where(df.index &lt; <span class="hljs-string">&apos;2008-07-1&apos;</span>,<span class="hljs-number">0</span>,<span class="hljs-number">1</span>)</span><br></pre></td></tr></tbody></table></figure>

<h2 id="Long-run-Model-Specification"><a href="#Long-run-Model-Specification" class="headerlink" title="Long-run Model Specification"></a>Long-run Model Specification</h2><p>Now let&#x2019;s put aside the fun ideas and start model development with specifying the long-run predictive equation for real consumption.</p>
<p>Consider the following long-run model for U.S. real consumption:</p>
<p>\begin{align}<br>\log rc_t &amp;= \beta_0 + \beta_1 \log rdy_t + \beta_1 \log rnw_t + \epsilon_t<br>\end{align}</p>
<p>It&#x2019;s helpful to express our model in logarithmic form, so that the parameter estimates are elasticities.</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">df[<span class="hljs-string">&apos;ln_rc&apos;</span>] = np.log(df[<span class="hljs-string">&apos;rc&apos;</span>])</span><br><span class="line">df[<span class="hljs-string">&apos;ln_rdy&apos;</span>] = np.log(df[<span class="hljs-string">&apos;rdy&apos;</span>])</span><br><span class="line">df[<span class="hljs-string">&apos;ln_rnw&apos;</span>] = np.log(df[<span class="hljs-string">&apos;rnw&apos;</span>])</span><br><span class="line">df[<span class="hljs-string">&apos;const&apos;</span>] = <span class="hljs-number">1</span></span><br></pre></td></tr></tbody></table></figure>

<h3 id="Sample-and-Forecast-Period"><a href="#Sample-and-Forecast-Period" class="headerlink" title="Sample and Forecast Period"></a>Sample and Forecast Period</h3><p>The sample period for model specification i.e., specifying the preferred model for forecasting purposes can be 1962:1 to 2016:4 (and no later than 2007:4). Remember we have to stick with the train data from now on for the model development. The test set will be preserved for testing the predictive power of the model.</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cut = <span class="hljs-number">220</span></span><br><span class="line">train,test = df[:cut], df[cut:]</span><br><span class="line">train.shape,test.shape</span><br></pre></td></tr></tbody></table></figure>




<p>&#xA0; &#xA0; ((220, 39), (18, 39))</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train.tail()</span><br></pre></td></tr></tbody></table></figure>




<div>
<style scoped>
  .dataframe tbody tr th:only-of-type {
    vertical-align: middle;
  }

<p>  .dataframe tbody tr th {<br>    vertical-align: top;<br>  }</p>
<p>  .dataframe thead th {<br>    text-align: right;<br>  }<br></style><p></p>

&#xA0; 
&#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; 
&#xA0; 
&#xA0; 
&#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; 
&#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; 
&#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; 
&#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; 
&#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; 
&#xA0; <table border="1" class="dataframe"><thead><tr style="text-align: right;"><th></th><th>saving_rate</th><th>ns</th><th>nnw</th><th>nl</th><th>na</th><th>nfa</th><th>interest_payments</th><th>cpi</th><th>c_deflator</th><th>dff</th><th>...</th><th>rnw</th><th>rl</th><th>ra</th><th>rfa</th><th>sb_1975_4</th><th>sb_2008_3</th><th>ln_rc</th><th>ln_rdy</th><th>ln_rnw</th><th>const</th></tr></thead><tbody><tr><th>2015-10-01</th><td>7.4</td><td>1026.972</td><td>89638.832</td><td>14191.141</td><td>104202.900</td><td>72619.083</td><td>271.856</td><td>237.837</td><td>1.03286</td><td>0.16</td><td>...</td><td>86787.010824</td><td>13739.655907</td><td>100887.729218</td><td>70308.737873</td><td>1</td><td>1</td><td>9.392924</td><td>9.505978</td><td>11.371212</td><td>1</td></tr><tr><th>2016-01-01</th><td>7.5</td><td>1046.172</td><td>90792.562</td><td>14181.444</td><td>105350.157</td><td>73464.821</td><td>266.871</td><td>237.689</td><td>1.03340</td><td>0.36</td><td>...</td><td>87858.101413</td><td>13723.092704</td><td>101945.187730</td><td>71090.401587</td><td>1</td><td>1</td><td>9.400231</td><td>9.513525</td><td>11.383478</td><td>1</td></tr><tr><th>2016-04-01</th><td>6.9</td><td>969.635</td><td>92170.184</td><td>14313.771</td><td>106862.978</td><td>74353.858</td><td>271.273</td><td>239.590</td><td>1.03989</td><td>0.37</td><td>...</td><td>88634.551731</td><td>13764.697228</td><td>102763.732702</td><td>71501.656906</td><td>1</td><td>1</td><td>9.405301</td><td>9.511885</td><td>11.392277</td><td>1</td></tr><tr><th>2016-07-01</th><td>6.8</td><td>961.218</td><td>93991.425</td><td>14497.803</td><td>108869.331</td><td>75700.352</td><td>274.959</td><td>240.607</td><td>1.04379</td><td>0.39</td><td>...</td><td>90048.213721</td><td>13889.578363</td><td>104301.948668</td><td>72524.503971</td><td>1</td><td>1</td><td>9.411142</td><td>9.516705</td><td>11.408101</td><td>1</td></tr><tr><th>2016-10-01</th><td>6.8</td><td>974.320</td><td>94743.726</td><td>14601.767</td><td>109726.272</td><td>76056.719</td><td>277.958</td><td>242.135</td><td>1.04872</td><td>0.45</td><td>...</td><td>90342.251507</td><td>13923.418072</td><td>104628.758868</td><td>72523.379930</td><td>1</td><td>1</td><td>9.415978</td><td>9.522002</td><td>11.411361</td><td>1</td></tr></tbody>
</table>
<p>5 rows &#xD7; 39 columns</p>
</div>



<h2 id="Check-for-Stationary"><a href="#Check-for-Stationary" class="headerlink" title="Check for Stationary"></a>Check for Stationary</h2><p>I&#x2019;m going to pre-test the data to determine whether if it has a unit root, that is, whether it&#x2019;s stationary or non-stationary.</p>
<h3 id="1-What-is-stationarity"><a href="#1-What-is-stationarity" class="headerlink" title="1. What is stationarity?"></a>1. What is stationarity?</h3><p>A stationary process is a stochastic process whose probability distribution does not change over time. For our work in time series analysis, we mostly care about stationarity in its weak form - covariance stationary.</p>
<h4 id="Covariance-stationary"><a href="#Covariance-stationary" class="headerlink" title="Covariance stationary:"></a>Covariance stationary:</h4><ul>
<li>Constant mean:<br>\begin{align}<br>&amp;E(y_t) = E(y_{t+1}) = \mu<br>\end{align}</li>
<li>Constant variance:<br>\begin{align}<br>&amp;Var(y_t) = Var(y_{t+1}) = \sigma^2<br>\end{align}</li>
<li>Covariance depends on time that has elapsed between observations, not on reference period:<br>\begin{align}<br>&amp;Cov(y_t,y_{t+j}) = Cov(y_s,y_{s+j}) = \gamma_j<br>\end{align}</li>
</ul>
<p>To get an intuition of stationarity, one can imagine a frictionless pendulum. It swings back and forth in an oscillatory motion, yet the amplitude and frequency remain constant. Although the pendulum is moving, the process is stationary as its &#x201C;statistics&#x201D; are constant (frequency and amplitude). However, if a force were to be applied to the pendulum, either the frequency or amplitude would change, thus making the process non-stationary. </p>
<h3 id="2-Why-is-stationarity-important"><a href="#2-Why-is-stationarity-important" class="headerlink" title="2. Why is stationarity important?"></a>2. Why is stationarity important?</h3><p>Stationarity is one important assumption underlying in time series analysis as if the series is non-stationary, the shocks do not die out, the impact of disturbances become more influential over time and so gives a poor forecast accuracy.</p>
<h4 id="Consequences-of-non-stationarity"><a href="#Consequences-of-non-stationarity" class="headerlink" title="Consequences of non-stationarity"></a>Consequences of non-stationarity</h4><ul>
<li>The effect of a shock will diminish as time elapses</li>
<li>Statistical consequences<br>&#xA0; &#xA0; &#xA0;- Non-normal distribution of test statistics<br>&#xA0; &#xA0; &#xA0;- Bias in autoregressive coefficients; we might mistakenly estimate an AR(1), deficient forecast<br>&#xA0; &#xA0; &#xA0;- Usual confidence intervals for coefficients not valid, poor forecast ability</li>
</ul>
<h3 id="3-How-do-we-determine-whether-a-time-series-is-nonstationary"><a href="#3-How-do-we-determine-whether-a-time-series-is-nonstationary" class="headerlink" title="3. How do we determine whether a time series is nonstationary?"></a>3. How do we determine whether a time series is nonstationary?</h3><p>As always, visual inspection is one of the most useful ways to identify non stationary series. It looks that all of the curves obviously have a trend, which is definitely against the definition of stationarity.</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train[[<span class="hljs-string">&apos;ln_rc&apos;</span>,<span class="hljs-string">&apos;ln_rdy&apos;</span>,<span class="hljs-string">&apos;ln_rnw&apos;</span>]].plot()</span><br></pre></td></tr></tbody></table></figure>




<p>&#xA0; &#xA0; &lt;matplotlib.axes._subplots.AxesSubplot at 0x23c0007c248&gt;</p>
<img src="http://yumeng-li.github.io/VECM_files/VECM_44_1.png">



<p>Then we can turn to statistical tests. The first would be <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Augmented_Dickey%E2%80%93Fuller_test"><strong>Augmented Dickey-Fuller (ADF) Test</strong></a>, which tests for non-stationarity. </p>
<p>Recall AR(1) model:<br>\begin{align}<br>y_t = &#xA0;by{t_1} + \epsilon_t<br>\end{align}</p>
<p>A special case is the random walk when $b = 1$. However, for stationarity, it requires $b&lt;1$. </p>
<p>If generalizing to AR(p), it means that roots of the polynomial below must all be &gt;1 in abs value,<br>\begin{align}<br>1 - b_1z -b_2z^2-b_3z^3-&#x2026;-b_pz^p<br>\end{align}</p>
<p>If one of the roots = 1, then it is said to have a <strong>unit root</strong> i.e., non-stationary.</p>
<p>The ADF Test can test for the coefficient on $y_{t-1}$. Basically, it regresses y on its lag testing for significance of coefficient. I won&#x2019;t go into great mathematical details, if anyone&#x2019;s interested, check out this <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Augmented_Dickey%E2%80%93Fuller_test">wiki page</a>.</p>
<p>However, ADF has been found to have low power in certain circumstances:</p>
<ul>
<li>stationary processes with near-unit roots. For example, it has difficulty distinguishing between b = 1 and b = 0.95, especially with small samples.</li>
<li>Trend stationary processes.</li>
</ul>
<p>So alternative tests have been designed, which are <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Phillips%E2%80%93Perron_test"><strong>Phillips&#x2013;Perron (PP) Test</strong></a> and <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/KPSS_test"><strong>Kwiatkowski&#x2013;Phillips&#x2013;Schmidt&#x2013;Shin (KPSS)Test</strong></a>. I usually prefer to use all three of them together, but here I would focus on ADF and KPSS as they have been implemented in statsmodels.</p>
<p>A few notes:<br>&#xA0;- ADF and PP are called <strong>unit root tests</strong>; the null hypothesis is that $y_t$ has a unit root; is I(1) or higher.<br>&#xA0;- KPSS, on the other hand, is a <strong>stationarity test</strong>; the null hypothesis is that $y_t$ is I(0).<br>&#xA0;- Correct specification is key. Intercept and trend should be included when appropriate as critical values for the t-statistics will vary depending on whether they are included.<br>&#xA0;- Structural breaks can complicate matters further. That&#x2019;s why we start this blog with detecting structural breaks.</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">augmented_dickey_fuller_test</span>(<span class="hljs-params">time_series</span>):</span></span><br><span class="line">&#xA0; &#xA0; result = adfuller(time_series.values)</span><br><span class="line">&#xA0; &#xA0; print(<span class="hljs-string">&apos;ADF Statistic: %f&apos;</span> % result[<span class="hljs-number">0</span>])</span><br><span class="line">&#xA0; &#xA0; print(<span class="hljs-string">&apos;ADF p-value: %f&apos;</span> % result[<span class="hljs-number">1</span>])</span><br><span class="line">&#xA0; &#xA0; <span class="hljs-keyword">if</span> result[<span class="hljs-number">1</span>]&lt;<span class="hljs-number">0.05</span>:</span><br><span class="line">&#xA0; &#xA0; &#xA0; &#xA0; print(<span class="hljs-string">&apos;stationary - null hypothesis of a unit root can be rejected at a 5% significance level&apos;</span>)</span><br><span class="line">&#xA0; &#xA0; <span class="hljs-keyword">else</span>:</span><br><span class="line">&#xA0; &#xA0; &#xA0; &#xA0; print(<span class="hljs-string">&apos;non-stationary - null hypothesis of a unit root cannot be rejected at a 5% significance level&apos;</span>)</span><br><span class="line"></span><br><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">kpss_test</span>(<span class="hljs-params">timeseries</span>):</span></span><br><span class="line">&#xA0; &#xA0; result = kpss(timeseries, regression=<span class="hljs-string">&quot;c&quot;</span>)</span><br><span class="line">&#xA0; &#xA0; print(<span class="hljs-string">&apos;KPSS Statistic: %f&apos;</span> % result[<span class="hljs-number">0</span>])</span><br><span class="line">&#xA0; &#xA0; print(<span class="hljs-string">&apos;KPSS p-value: %f&apos;</span> % result[<span class="hljs-number">1</span>])</span><br><span class="line">&#xA0; &#xA0; <span class="hljs-keyword">if</span> result[<span class="hljs-number">1</span>]&gt;=<span class="hljs-number">0.05</span>:</span><br><span class="line">&#xA0; &#xA0; &#xA0; &#xA0; print(<span class="hljs-string">&apos;stationary - null hypothesis of stationary cannot be rejected at a 5% significance level&apos;</span>)</span><br><span class="line">&#xA0; &#xA0; <span class="hljs-keyword">else</span>:</span><br><span class="line">&#xA0; &#xA0; &#xA0; &#xA0; print(<span class="hljs-string">&apos;non-stationary - null hypothesis of stationary can be rejected at a 5% significance level&apos;</span>)</span><br></pre></td></tr></tbody></table></figure>


<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">augmented_dickey_fuller_test(train[<span class="hljs-string">&apos;ln_rc&apos;</span>])</span><br><span class="line">kpss_test(train[<span class="hljs-string">&apos;ln_rc&apos;</span>])</span><br></pre></td></tr></tbody></table></figure>

<p>&#xA0; &#xA0; ADF Statistic: -1.949803<br>&#xA0; &#xA0; ADF p-value: 0.309007<br>&#xA0; &#xA0; non-stationary - null hypothesis of a unit root cannot be rejected at a 5% significance level<br>&#xA0; &#xA0; KPSS Statistic: 2.297233<br>&#xA0; &#xA0; KPSS p-value: 0.010000<br>&#xA0; &#xA0; non-stationary - null hypothesis of stationary can be rejected at a 5% significance level</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">augmented_dickey_fuller_test(train[<span class="hljs-string">&apos;ln_rdy&apos;</span>])</span><br><span class="line">kpss_test(train[<span class="hljs-string">&apos;ln_rdy&apos;</span>])</span><br></pre></td></tr></tbody></table></figure>

<p>&#xA0; &#xA0; ADF Statistic: -2.970069<br>&#xA0; &#xA0; ADF p-value: 0.037786<br>&#xA0; &#xA0; stationary - null hypothesis of a unit root can be rejected at a 5% significance level<br>&#xA0; &#xA0; KPSS Statistic: 2.292161<br>&#xA0; &#xA0; KPSS p-value: 0.010000<br>&#xA0; &#xA0; non-stationary - null hypothesis of stationary can be rejected at a 5% significance level</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">augmented_dickey_fuller_test(train[<span class="hljs-string">&apos;ln_rnw&apos;</span>])</span><br><span class="line">kpss_test(train[<span class="hljs-string">&apos;ln_rnw&apos;</span>])</span><br></pre></td></tr></tbody></table></figure>

<p>&#xA0; &#xA0; ADF Statistic: -0.266563<br>&#xA0; &#xA0; ADF p-value: 0.930108<br>&#xA0; &#xA0; non-stationary - null hypothesis of a unit root cannot be rejected at a 5% significance level<br>&#xA0; &#xA0; KPSS Statistic: 2.300497<br>&#xA0; &#xA0; KPSS p-value: 0.010000<br>&#xA0; &#xA0; non-stationary - null hypothesis of stationary can be rejected at a 5% significance level</p>
<p>None of the variables can be considered stationary with the two tests. We can therefore ask whether the variables form a cointegrated system with a given number of &#x201C;common trends&#x201D;. <strong>Intuitively, I would argue that there exists one cointegration equation among the three variables as in the long run, one&#x2019;s amount consumed and wealth accumulated should be equal to the income that one earns.</strong> </p>
<p>We will discuss how to test for cointegration relationships and how to estimate them using a vector error correction model in the next blog.</p>
</body></html>
    
    </div>
    
    
</article>




    
        <article class="article content gallery" itemscope itemprop="blogPost">
    <h1 class="article-title is-size-3 is-size-4-mobile" itemprop="name">
        
            <a href="/2021/10/02/projectcovidmixed/" itemprop="url">Project Covid Admissions Based on Vaccine Progress, Part 3 -- Mixed-effect models and Bayesian Inference</a>
        
    </h1>
    <div class="article-meta columns is-variable is-1 is-multiline is-mobile is-size-7-mobile">
        <span class="column is-narrow">
            <time datetime="2021-10-02T16:00:00.000Z" itemprop="datePublished">Oct 2 2021</time>
        </span>
        
        <span class="column is-narrow article-category">
            <i class="far fa-folder"></i>
            <a class="article-category-link" href="/categories/Applied-Statistics/">Applied Statistics</a>
        </span>
        
        
        <span class="column is-narrow">
            
            
            25 minutes read (About 3775 words)
        </span>
        
    </div>
    <div class="article-entry is-size-6-mobile" itemprop="articleBody">
    
        <html><head></head><body><p>In the last two blogs, we went through linear regression and generalized linear regression in search of the best solution to the covid data by relaxing the assumptions of linear regression and OLS. Based on what have done so far, our conclusion is negative binomial regression fit the data best. However, still, we have this problem that for some states with high vaccination rates, the increasing cases are over-estimated, and a few state such as FL and KY, are more like outliers to the model. Today, we are going to fixed these issues with the <strong>mixed-effects model</strong>, specifically, the generalized linear mixed-effects model(GLMM), which is very popular in biostatistics and econometrics, especially useful for economists when modeling cross-country macro data.</p>
<p>A <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Mixed_model">mixed-effects model</a> contains both fixed effects and random effects. They are particularly useful in settings where measurements are made on clusters of related statistical units. There are several ways people usually choose to deal with this type of inherently &#x201C;lumpy&#x201D; data.</p>
<h4 id="1-Pooled-Data-Regression"><a href="#1-Pooled-Data-Regression" class="headerlink" title="1. Pooled Data Regression"></a>1. Pooled Data Regression</h4><p>One can run a regular regression on $y$ versus $x$ ignoring the group information, like what we have done in the last two blogs.</p>
<p>It works but:</p>
<ul>
<li>It throws away useful information.</li>
<li>Linear Regression assumes observations are <strong>independent</strong> here they are not.</li>
<li>The $\alpha$ and $\beta$ coefficients are <em>unbiased estimates</em> (not bad).</li>
<li>The confidence intervals will be <strong>too optimistic</strong>.</li>
</ul>
<h4 id="2-Unpooled-Data-Regression"><a href="#2-Unpooled-Data-Regression" class="headerlink" title="2. Unpooled Data Regression"></a>2. Unpooled Data Regression</h4><p>Instead one can run a regression for each of the groups.</p>
<ul>
<li>One needs to estimate too many coefficients (51 $\alpha$s and 51 $\beta$s in our case).</li>
<li>This results in <strong>unbiased</strong>, but <strong>very noisy</strong> estimates if some groups have few observations.</li>
</ul>
<h4 id="3-Pooled-Slope-Unpooled-Intercept"><a href="#3-Pooled-Slope-Unpooled-Intercept" class="headerlink" title="3. Pooled Slope, Unpooled Intercept"></a>3. Pooled Slope, Unpooled Intercept</h4><p>We can instead fit a regression with a dummy variable for each group.</p>
<p>$$<br>    y_i = \alpha + \beta x + \rho^T Z_i<br>$$<br>where $\rho$ is now a vector with as many coefficients as groups, and $Z$ is a vector of dummies, one column per group (we must leave one group out to avoid co-linearity of Z).</p>
<p>This method:</p>
<ul>
<li>Pools information for the slope $\beta$.</li>
<li>Treats each group intercept independently. <strong>If some group $g$ has few examples $\rho_g$ will be noisy</strong>.</li>
</ul>
<h4 id="4-Penalized-Linear-Regression"><a href="#4-Penalized-Linear-Regression" class="headerlink" title="4. Penalized Linear Regression"></a>4. Penalized Linear Regression</h4><p>To pool over the different groups, we can introduce a <strong>regression penalty</strong> to penalize model coefficients to reduce the model degrees of freedom.</p>
<ul>
<li>Theoretically, the correct penalty is proportional to the ration of variances for the observations $\sigma_Y^2$ and the random effects $\sigma_\rho^2$.</li>
<li>Because there is no penalty for $\alpha$, the mean over all the population will be matched exactly.</li>
</ul>
<h4 id="5-Linear-Mixed-effect-Model"><a href="#5-Linear-Mixed-effect-Model" class="headerlink" title="5. Linear Mixed-effect Model"></a>5. Linear Mixed-effect Model</h4><p>For people who are familiar with penalized regressions and regularizations like lasso and rigid, the estimation of mixed models would be very straightforward as essentially, they are the same. Same result can be obtained with less work by fitting to a <strong>Linear Mixed Effect Model</strong> with <a target="_blank" rel="noopener" href="https://www.statsmodels.org/0.8.0/mixed_linear.html">statsmodels.mixed_linear</a> module. </p>
<h4 id="6-Generalized-Linear-Mixed-effects-Model-GLMM"><a href="#6-Generalized-Linear-Mixed-effects-Model-GLMM" class="headerlink" title="6. Generalized Linear Mixed-effects Model (GLMM)"></a>6. Generalized Linear Mixed-effects Model (GLMM)</h4><p>Today, as we already know that negative binomial distribution fits the covid data better, we will skip the linear mixed model and start with the non-linear GLMM. Since the model is relatively complex, we will be using a much cooler statistical inference method &#x2013; <strong>Bayesian</strong>.</p>
<h2 id="Preliminaries"><a href="#Preliminaries" class="headerlink" title="Preliminaries"></a>Preliminaries</h2><figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np</span><br><span class="line"><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd</span><br><span class="line"><span class="hljs-keyword">import</span> matplotlib</span><br><span class="line"><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt</span><br><span class="line"><span class="hljs-keyword">import</span> matplotlib.cm <span class="hljs-keyword">as</span> cm</span><br><span class="line"><span class="hljs-keyword">import</span> seaborn <span class="hljs-keyword">as</span> sns</span><br><span class="line"><span class="hljs-keyword">import</span> pymc3 <span class="hljs-keyword">as</span> pm</span><br><span class="line"><span class="hljs-keyword">import</span> arviz <span class="hljs-keyword">as</span> az</span><br><span class="line"><span class="hljs-keyword">import</span> sys</span><br><span class="line"><span class="hljs-keyword">import</span> covid_analysis <span class="hljs-keyword">as</span> covid</span><br></pre></td></tr></tbody></table></figure>

<p>As usual, let&#x2019;s prepare the data. Hospital admissions and vaccinations are from the CDC website, and the most up-to-date populations by state are from the US Census Bureau.</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data_dir=<span class="hljs-string">&quot;../data&quot;</span></span><br></pre></td></tr></tbody></table></figure>


<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">hospitalizations=pd.read_csv(<span class="hljs-string">f&quot;<span class="hljs-subst">{data_dir}</span>/covid_hospitalizations.csv&quot;</span>,parse_dates=[<span class="hljs-string">&quot;date&quot;</span>])</span><br><span class="line">vaccinations=pd.read_csv(<span class="hljs-string">f&quot;<span class="hljs-subst">{data_dir}</span>/covid_vaccinations.csv&quot;</span>,parse_dates=[<span class="hljs-string">&quot;date&quot;</span>])</span><br><span class="line">population=pd.read_csv(<span class="hljs-string">f&quot;<span class="hljs-subst">{data_dir}</span>/population.csv&quot;</span>)</span><br><span class="line"></span><br><span class="line">event=<span class="hljs-string">&quot;admissions&quot;</span></span><br><span class="line">data=hospitalizations</span><br><span class="line"></span><br><span class="line">data=data.merge(vaccinations,on=[<span class="hljs-string">&quot;date&quot;</span>,<span class="hljs-string">&quot;state&quot;</span>])</span><br><span class="line">data=data.merge(population,on=<span class="hljs-string">&quot;state&quot;</span>)</span><br><span class="line">data[<span class="hljs-string">&quot;vaccinated&quot;</span>]=data[<span class="hljs-string">&quot;vaccinated&quot;</span>]/data[<span class="hljs-string">&quot;population&quot;</span>]</span><br><span class="line">data.head()</span><br></pre></td></tr></tbody></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;</code></pre>
<p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>date</th>
      <th>state</th>
      <th>used_beds</th>
      <th>admissions</th>
      <th>vaccinated</th>
      <th>population</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>2021-06-10</td>
      <td>MT</td>
      <td>66.0</td>
      <td>16.0</td>
      <td>0.397597</td>
      <td>1080577</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2021-06-21</td>
      <td>MT</td>
      <td>57.0</td>
      <td>13.0</td>
      <td>0.411614</td>
      <td>1080577</td>
    </tr>
    <tr>
      <th>2</th>
      <td>2021-08-05</td>
      <td>MT</td>
      <td>149.0</td>
      <td>23.0</td>
      <td>0.440153</td>
      <td>1080577</td>
    </tr>
    <tr>
      <th>3</th>
      <td>2021-07-15</td>
      <td>MT</td>
      <td>62.0</td>
      <td>9.0</td>
      <td>0.431722</td>
      <td>1080577</td>
    </tr>
    <tr>
      <th>4</th>
      <td>2021-08-21</td>
      <td>MT</td>
      <td>224.0</td>
      <td>59.0</td>
      <td>0.449196</td>
      <td>1080577</td>
    </tr>
  </tbody>
</table>
</div>



<p>Split the train and test data,</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-comment"># train dates</span></span><br><span class="line">start_train=<span class="hljs-string">&quot;2021-07-15&quot;</span></span><br><span class="line">end_train=<span class="hljs-string">&quot;2021-08-15&quot;</span></span><br><span class="line">train_data=data[(data[<span class="hljs-string">&apos;date&apos;</span>] &gt;= start_train) &amp; (data[<span class="hljs-string">&apos;date&apos;</span>] &lt;= end_train)].copy()</span><br><span class="line">date0=train_data[<span class="hljs-string">&quot;date&quot;</span>].min()</span><br><span class="line"><span class="hljs-comment"># test dates</span></span><br><span class="line">test_period=<span class="hljs-number">7</span> <span class="hljs-comment"># days</span></span><br><span class="line">test_end=pd.Timestamp(end_train)+pd.DateOffset(days=test_period)</span><br><span class="line">test_data=data[(data[<span class="hljs-string">&quot;date&quot;</span>]&gt;end_train) &amp; (data[<span class="hljs-string">&quot;date&quot;</span>]&lt;=test_end)].copy()</span><br></pre></td></tr></tbody></table></figure>

<p>Here we write a function to generate our desired matrix. This time, we will need an extra step to label data by state.</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">define_variables</span>(<span class="hljs-params">data,date0</span>):</span></span><br><span class="line">        N=len(data)</span><br><span class="line">        T=(data[<span class="hljs-string">&quot;date&quot;</span>]-date0).dt.days/<span class="hljs-number">30</span></span><br><span class="line">        X=pd.DataFrame({<span class="hljs-string">&quot;const&quot;</span>:np.ones(N)})</span><br><span class="line">        X[<span class="hljs-string">&quot;vaccinated&quot;</span>]=data[<span class="hljs-string">&quot;vaccinated&quot;</span>].values</span><br><span class="line">        X[<span class="hljs-string">&quot;T&quot;</span>]=T.values</span><br><span class="line">        X[<span class="hljs-string">&quot;T2&quot;</span>]=T.values**<span class="hljs-number">2</span></span><br><span class="line">        P=data[<span class="hljs-string">&quot;population&quot;</span>].values </span><br><span class="line">        Z=pd.get_dummies(data[<span class="hljs-string">&quot;state&quot;</span>])        </span><br><span class="line">        <span class="hljs-keyword">return</span> X,Z,P</span><br></pre></td></tr></tbody></table></figure>


<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">X_train,Z_train,P_train=define_variables(train_data,date0)</span><br><span class="line">Y_train=train_data[event]</span><br><span class="line">G_train=Z_train.values.argmax(axis=<span class="hljs-number">1</span>)</span><br></pre></td></tr></tbody></table></figure>

<p>We have 51 regions in total,</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-comment"># number of states/regions</span></span><br><span class="line">K = G_train.max()+<span class="hljs-number">1</span></span><br><span class="line">K</span><br></pre></td></tr></tbody></table></figure>




<pre><code>51</code></pre>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-comment"># all states/regions</span></span><br><span class="line">states=Z_train.columns</span><br><span class="line">states</span><br></pre></td></tr></tbody></table></figure>




<pre><code>Index([&apos;AK&apos;, &apos;AL&apos;, &apos;AR&apos;, &apos;AZ&apos;, &apos;CA&apos;, &apos;CO&apos;, &apos;CT&apos;, &apos;DC&apos;, &apos;DE&apos;, &apos;FL&apos;, &apos;GA&apos;, &apos;HI&apos;,
       &apos;IA&apos;, &apos;ID&apos;, &apos;IL&apos;, &apos;IN&apos;, &apos;KS&apos;, &apos;KY&apos;, &apos;LA&apos;, &apos;MA&apos;, &apos;MD&apos;, &apos;ME&apos;, &apos;MI&apos;, &apos;MN&apos;,
       &apos;MO&apos;, &apos;MS&apos;, &apos;MT&apos;, &apos;NC&apos;, &apos;ND&apos;, &apos;NE&apos;, &apos;NH&apos;, &apos;NJ&apos;, &apos;NM&apos;, &apos;NV&apos;, &apos;NY&apos;, &apos;OH&apos;,
       &apos;OK&apos;, &apos;OR&apos;, &apos;PA&apos;, &apos;RI&apos;, &apos;SC&apos;, &apos;SD&apos;, &apos;TN&apos;, &apos;TX&apos;, &apos;UT&apos;, &apos;VA&apos;, &apos;VT&apos;, &apos;WA&apos;,
       &apos;WI&apos;, &apos;WV&apos;, &apos;WY&apos;],
      dtype=&apos;object&apos;)</code></pre>
<h2 id="Mixed-Negative-Binomial-Regression-Model"><a href="#Mixed-Negative-Binomial-Regression-Model" class="headerlink" title="Mixed Negative Binomial Regression Model"></a>Mixed Negative Binomial Regression Model</h2><h3 id="Model-Definition"><a href="#Model-Definition" class="headerlink" title="Model Definition"></a>Model Definition</h3><p>We have data from multiple dates, so it makes sense to incorporate Time as a extra variable.</p>
<p>\begin{eqnarray}<br>    \eta_{i,j}                  =&amp; \beta_0 + \beta_1 x_{i,j}  + \beta_2 t_{j} + \beta_3 t_{j}^2  + \rho_i\newline<br>    \rho_i \sim &amp; N(0,\sigma_{\rho}^2) \newline<br>    \lambda_{i,j} =&amp; e^{\eta_{i,j}} \newline<br>   y_{i,j}          \sim &amp; \text{NB}(y;\lambda_{i,j} P_i,\alpha)<br>\end{eqnarray}<br>with priors<br>\begin{eqnarray}<br>    \beta_0 \sim &amp; N( \bar{y},1) \newline<br>    \beta_i \sim &amp; N(0,1) \newline<br>     \log \sigma_\rho \sim &amp;  N(0,1) \newline<br>     \log \alpha \sim &amp; N(0,1)\newline<br>\end{eqnarray}<br>where </p>
<ul>
<li>Each index $i$ represents a  state in the US.</li>
<li>Each index $j$ represents a particular date.</li>
<li>$y_{i,j}$ is the number of hospital admissions on  state $i$ at date $j$.</li>
<li>the  <em>random effects (random intercept)</em>  $\rho_i$ for different states $i$ are independent and have a normal distribution. It will be fit to the data.</li>
<li>$x_{i,j}$ is the percentage of the state population fully vaccinated on state $i$ at date $j$.</li>
<li>$t_j$ is the number of days elapsed since the beginning of the training period.</li>
<li>$P_i$ is the population of  state $i$.</li>
<li>$\bar{y}$ is log of the average per person incidence on the event in the US Population.</li>
<li>the parameters and $\beta_1,\beta_2,\beta_3$ will be fitted to the observed data to maximize agreement with the model.</li>
<li>the parameter $\sigma_\beta$ is our prior uncertainty about the value of $\beta_i$, it is set to 1.</li>
</ul>
<p>To estimate the model coefficients, we will go for <strong>full Bayesian</strong>. </p>
<h3 id="Bayesian-Approach"><a href="#Bayesian-Approach" class="headerlink" title="Bayesian Approach"></a>Bayesian Approach</h3><p>Unlike the classical frequentist methods, in Bayesian analysis, a parameter is summarized by an entire distribution of values instead of one fixed value, and it provides a natural and principled way of combining prior information or expert knowledge with the data observed.</p>
<p>Both Bayesian methods and classical methods have advantages and disadvantages, and there are some similarities. When the sample size is large, Bayesian inference often provides results for parametric models that are very similar to the results produced by frequentist methods. Some advantages to using Bayesian analysis include the following:</p>
<ol>
<li><p>It provides a natural and principled way of combining prior information with data, within a solid decision theoretical framework. You can incorporate past information about a parameter and form a prior distribution for future analysis. </p>
</li>
<li><p>It provides interpretable answers based on <strong>parameter distributions</strong>, such as &#x201C;the true parameter has a probability of 0.95 of falling in a 95% credible interval.&#x201D;</p>
</li>
<li><p>It provides a convenient setting for a wide range of <strong>complex models</strong>, such as the GLMM model we are trying to build but not easy to get an analytical solution. MCMC, along with other numerical methods, makes computations tractable for virtually all parametric models.</p>
</li>
</ol>
<p>The disadvantages of Bayesian are quite obvious as well:</p>
<ol>
<li><p>It does not tell you how to select a prior. There is no correct way to choose a prior. Bayesian inferences require skills to translate subjective prior beliefs into a mathematically formulated prior.</p>
</li>
<li><p>It often comes with a <strong>high computational cost</strong>, especially in models with a large number of parameters. </p>
</li>
</ol>
<p>To solve our model with Bayesian approach, we have to specify the distributions of the parameters/variables. The good thing is that there are convenient functions provided by <code>pymc3</code>. </p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Y_bar=np.log(Y_train.sum()/P_train.sum())</span><br><span class="line">beta0=np.array([Y_bar,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,])</span><br><span class="line">sigma_beta=np.array([<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>])</span><br></pre></td></tr></tbody></table></figure>

<p>I choose $N(0,1)$ for $\beta$s , and $lognormal(0,1)$ for $\sigma_\rho$s just to speed up the convergence.</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-keyword">with</span> pm.Model() <span class="hljs-keyword">as</span> mod:</span><br><span class="line">    beta=pm.Normal(<span class="hljs-string">&quot;beta&quot;</span>,mu=beta0,</span><br><span class="line">                   sigma=sigma_beta,</span><br><span class="line">                   shape=(len(beta0))</span><br><span class="line">                  )</span><br><span class="line">    lsigma_group=pm.Normal(<span class="hljs-string">&quot;lsigma_group&quot;</span>,mu=<span class="hljs-number">0</span>,sigma=<span class="hljs-number">1</span>)</span><br><span class="line">    sigma_group=pm.Deterministic(<span class="hljs-string">&quot;sigma_group&quot;</span>,np.exp(lsigma_group))</span><br><span class="line">    rho=pm.Normal(<span class="hljs-string">&quot;rho&quot;</span>,mu=<span class="hljs-number">0.0</span>,sigma=sigma_group,shape=(K)) <span class="hljs-comment"># a vector of K values</span></span><br><span class="line">    eta=pm.math.dot(X_train.values,beta)+rho[G_train]</span><br><span class="line">    y_hat=pm.Deterministic(<span class="hljs-string">&quot;eta&quot;</span>,pm.math.exp(eta))</span><br><span class="line">    a=pm.Lognormal(<span class="hljs-string">&quot;alpha&quot;</span>,mu=<span class="hljs-number">0</span>,sigma=<span class="hljs-number">1</span>)</span><br><span class="line">    y=pm.NegativeBinomial(<span class="hljs-string">&quot;y&quot;</span>,mu=y_hat*P_train,alpha=a,observed=Y_train)</span><br></pre></td></tr></tbody></table></figure>

<p>Then we can start the Monte Carlo sampling process. 4 chains, of which 1000 samples are generated. That means there will be 4000 samples for each of the parameters. Be patient, the computation can take a while. (Honestly, sometimes HOURS if using personal laptops)</p>
<h3 id="Monte-Carlo-Sampling"><a href="#Monte-Carlo-Sampling" class="headerlink" title="Monte Carlo Sampling"></a>Monte Carlo Sampling</h3><figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-keyword">with</span> mod:</span><br><span class="line">    trace = pm.sample(<span class="hljs-number">1000</span>,chains=<span class="hljs-number">4</span>,cores=<span class="hljs-number">4</span>, tune=<span class="hljs-number">500</span>)</span><br></pre></td></tr></tbody></table></figure>

<pre><code>Auto-assigning NUTS sampler...
Initializing NUTS using jitter+adapt_diag...
Multiprocess sampling (4 chains in 4 jobs)
NUTS: [alpha, rho, lsigma_group, beta]
Sampling 4 chains, 0 divergences: 100%|&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;| 6000/6000 [04:36&lt;00:00, 21.72draws/s]
The acceptance probability does not match the target. It is 0.8915155058428972, but should be close to 0.8. Try to increase the number of tuning steps.
The rhat statistic is larger than 1.05 for some parameters. This indicates slight problems during sampling.
The estimated number of effective samples is smaller than 200 for some parameters.</code></pre>
<h4 id="Regression-Coefficients"><a href="#Regression-Coefficients" class="headerlink" title="Regression Coefficients"></a>Regression Coefficients</h4><p>One of the advantages of Bayesian is that a parameter is summarized by <strong>a distribution of values</strong> instead of a fixed value, which definately helps us get a better understanding of the things going on, like how high the uncertainty is, how much confidence the model has in the estimations.</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-keyword">with</span> mod:</span><br><span class="line">    summary=az.summary(trace, var_names=[<span class="hljs-string">&quot;beta&quot;</span>,<span class="hljs-string">&quot;sigma_group&quot;</span>], fmt=<span class="hljs-string">&quot;wide&quot;</span>,round_to=<span class="hljs-number">2</span>)</span><br><span class="line">summary.index=[<span class="hljs-string">&quot;intercept&quot;</span>,<span class="hljs-string">&quot;vaccinated&quot;</span>,<span class="hljs-string">&quot;T&quot;</span>,<span class="hljs-string">&quot;T2&quot;</span>,<span class="hljs-string">&quot;sigma_group&quot;</span>]</span><br><span class="line">summary</span><br></pre></td></tr></tbody></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;</code></pre>
<p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>mean</th>
      <th>sd</th>
      <th>hdi_3%</th>
      <th>hdi_97%</th>
      <th>mcse_mean</th>
      <th>mcse_sd</th>
      <th>ess_mean</th>
      <th>ess_sd</th>
      <th>ess_bulk</th>
      <th>ess_tail</th>
      <th>r_hat</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>intercept</th>
      <td>-9.42</td>
      <td>0.31</td>
      <td>-10.06</td>
      <td>-8.84</td>
      <td>0.02</td>
      <td>0.02</td>
      <td>160.54</td>
      <td>160.54</td>
      <td>142.03</td>
      <td>351.53</td>
      <td>1.04</td>
    </tr>
    <tr>
      <th>vaccinated</th>
      <td>-5.36</td>
      <td>0.66</td>
      <td>-6.57</td>
      <td>-4.02</td>
      <td>0.05</td>
      <td>0.04</td>
      <td>149.80</td>
      <td>119.62</td>
      <td>150.75</td>
      <td>192.52</td>
      <td>1.04</td>
    </tr>
    <tr>
      <th>T</th>
      <td>1.79</td>
      <td>0.07</td>
      <td>1.66</td>
      <td>1.91</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>749.07</td>
      <td>749.07</td>
      <td>751.34</td>
      <td>1212.85</td>
      <td>1.01</td>
    </tr>
    <tr>
      <th>T2</th>
      <td>-0.37</td>
      <td>0.06</td>
      <td>-0.47</td>
      <td>-0.25</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>916.41</td>
      <td>916.41</td>
      <td>917.97</td>
      <td>1287.44</td>
      <td>1.01</td>
    </tr>
    <tr>
      <th>sigma_group</th>
      <td>0.55</td>
      <td>0.06</td>
      <td>0.43</td>
      <td>0.66</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>1010.83</td>
      <td>1010.83</td>
      <td>1001.36</td>
      <td>1747.82</td>
      <td>1.00</td>
    </tr>
  </tbody>
</table>
</div>



<p>We can see the distribution of parameters very clearly,</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-keyword">with</span> mod:</span><br><span class="line">    az.plot_trace(trace,var_names=[<span class="hljs-string">&quot;beta&quot;</span>,<span class="hljs-string">&quot;sigma_group&quot;</span>,<span class="hljs-string">&quot;alpha&quot;</span>])</span><br></pre></td></tr></tbody></table></figure>

<img src="http://yumeng-li.github.io/projectcovidmixed_files/projectcovidmixed_27_0.png">


<h4 id="Random-Effects"><a href="#Random-Effects" class="headerlink" title="Random Effects"></a>Random Effects</h4><p>We also get a distribution for the  random, state specific, effects.</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-keyword">with</span> mod:</span><br><span class="line">    random_effects=az.summary(trace, var_names=[<span class="hljs-string">&quot;rho&quot;</span>], fmt=<span class="hljs-string">&quot;wide&quot;</span>,round_to=<span class="hljs-number">2</span>)</span><br><span class="line">random_effects.index=states</span><br></pre></td></tr></tbody></table></figure>


<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">random_effects.head(<span class="hljs-number">9</span>)</span><br></pre></td></tr></tbody></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;</code></pre>
<p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>mean</th>
      <th>sd</th>
      <th>hdi_3%</th>
      <th>hdi_97%</th>
      <th>mcse_mean</th>
      <th>mcse_sd</th>
      <th>ess_mean</th>
      <th>ess_sd</th>
      <th>ess_bulk</th>
      <th>ess_tail</th>
      <th>r_hat</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>AK</th>
      <td>0.03</td>
      <td>0.09</td>
      <td>-0.15</td>
      <td>0.20</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>63.60</td>
      <td>63.60</td>
      <td>65.35</td>
      <td>339.69</td>
      <td>1.05</td>
    </tr>
    <tr>
      <th>AL</th>
      <td>0.55</td>
      <td>0.12</td>
      <td>0.33</td>
      <td>0.77</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>88.48</td>
      <td>88.48</td>
      <td>86.71</td>
      <td>428.54</td>
      <td>1.04</td>
    </tr>
    <tr>
      <th>AR</th>
      <td>0.71</td>
      <td>0.11</td>
      <td>0.51</td>
      <td>0.92</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>74.26</td>
      <td>74.26</td>
      <td>74.61</td>
      <td>435.70</td>
      <td>1.05</td>
    </tr>
    <tr>
      <th>AZ</th>
      <td>0.12</td>
      <td>0.08</td>
      <td>-0.04</td>
      <td>0.27</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>49.42</td>
      <td>49.42</td>
      <td>51.50</td>
      <td>317.49</td>
      <td>1.06</td>
    </tr>
    <tr>
      <th>CA</th>
      <td>0.34</td>
      <td>0.09</td>
      <td>0.19</td>
      <td>0.51</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>47.83</td>
      <td>43.42</td>
      <td>46.91</td>
      <td>344.41</td>
      <td>1.07</td>
    </tr>
    <tr>
      <th>CO</th>
      <td>0.16</td>
      <td>0.09</td>
      <td>-0.01</td>
      <td>0.34</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>45.42</td>
      <td>42.62</td>
      <td>44.54</td>
      <td>297.58</td>
      <td>1.07</td>
    </tr>
    <tr>
      <th>CT</th>
      <td>0.10</td>
      <td>0.13</td>
      <td>-0.18</td>
      <td>0.33</td>
      <td>0.02</td>
      <td>0.01</td>
      <td>76.68</td>
      <td>76.68</td>
      <td>72.85</td>
      <td>279.41</td>
      <td>1.05</td>
    </tr>
    <tr>
      <th>DC</th>
      <td>-0.31</td>
      <td>0.12</td>
      <td>-0.52</td>
      <td>-0.09</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>75.16</td>
      <td>75.16</td>
      <td>74.36</td>
      <td>724.17</td>
      <td>1.04</td>
    </tr>
    <tr>
      <th>DE</th>
      <td>-0.48</td>
      <td>0.10</td>
      <td>-0.67</td>
      <td>-0.28</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>74.12</td>
      <td>74.12</td>
      <td>72.69</td>
      <td>614.42</td>
      <td>1.05</td>
    </tr>
  </tbody>
</table>
</div>



<p>Comparing to the other states, states such as FL, KY which are seen as outliers in our previous models have much larger random effects.</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">random_effects.loc[[<span class="hljs-string">&apos;FL&apos;</span>,<span class="hljs-string">&apos;KY&apos;</span>]]</span><br></pre></td></tr></tbody></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;</code></pre>
<p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>mean</th>
      <th>sd</th>
      <th>hdi_3%</th>
      <th>hdi_97%</th>
      <th>mcse_mean</th>
      <th>mcse_sd</th>
      <th>ess_mean</th>
      <th>ess_sd</th>
      <th>ess_bulk</th>
      <th>ess_tail</th>
      <th>r_hat</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>FL</th>
      <td>1.64</td>
      <td>0.08</td>
      <td>1.48</td>
      <td>1.78</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>44.39</td>
      <td>43.59</td>
      <td>45.16</td>
      <td>292.35</td>
      <td>1.07</td>
    </tr>
    <tr>
      <th>KY</th>
      <td>0.99</td>
      <td>0.08</td>
      <td>0.85</td>
      <td>1.16</td>
      <td>0.01</td>
      <td>0.01</td>
      <td>45.57</td>
      <td>44.32</td>
      <td>47.46</td>
      <td>324.94</td>
      <td>1.06</td>
    </tr>
  </tbody>
</table>
</div>



<h2 id="In-Sample-Predictions"><a href="#In-Sample-Predictions" class="headerlink" title="In Sample Predictions"></a>In Sample Predictions</h2><p>Now, let&#x2019;s make some predictions and see how the mixed model works.</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">select_data</span>(<span class="hljs-params">data,periods</span>):</span></span><br><span class="line">    dates=data[<span class="hljs-string">&quot;date&quot;</span>].unique()</span><br><span class="line">    selected_dates=covid.select_dates(dates,periods)</span><br><span class="line">    used_data=data.merge(selected_dates,on=<span class="hljs-string">&quot;date&quot;</span>)</span><br><span class="line">    <span class="hljs-keyword">return</span> used_data.groupby(<span class="hljs-string">&quot;date&quot;</span>)</span><br></pre></td></tr></tbody></table></figure>


<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">plot_result</span>(<span class="hljs-params">data,norm,ax</span>):</span></span><br><span class="line">    P=data[<span class="hljs-string">&quot;population&quot;</span>]</span><br><span class="line">    m=ax.scatter(data[<span class="hljs-string">&quot;vaccinated&quot;</span>],data[event]/data[<span class="hljs-string">&quot;population&quot;</span>]*<span class="hljs-number">100</span>_000,</span><br><span class="line">                c=P,norm=norm,cmap=<span class="hljs-string">&quot;Greys&quot;</span>,label=event)</span><br><span class="line">    </span><br><span class="line">    ax.errorbar(data[<span class="hljs-string">&quot;vaccinated&quot;</span>],data[<span class="hljs-string">&quot;y_pred&quot;</span>],yerr=data[<span class="hljs-string">&quot;y_std&quot;</span>],fmt=<span class="hljs-string">&quot;D&quot;</span>,alpha=<span class="hljs-number">0.25</span>,</span><br><span class="line">               label=<span class="hljs-string">&quot;predicted&quot;</span>)<span class="hljs-comment">#,c=P,norm=norm,cmap=&quot;Blues&quot;,label=&quot;predicted&quot;)</span></span><br><span class="line">    ax.set_xlabel(<span class="hljs-string">&quot;vaccinated&quot;</span>)</span><br><span class="line">    ax.set_ylabel(<span class="hljs-string">f&quot;<span class="hljs-subst">{event}</span> per 100k&quot;</span>)</span><br><span class="line">    ax.legend()</span><br><span class="line">    <span class="hljs-keyword">return</span> m</span><br><span class="line"></span><br><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">plot_time_facets</span>(<span class="hljs-params">data,event</span>):</span></span><br><span class="line">    <span class="hljs-comment"># Create two subplots and unpack the output array immediately</span></span><br><span class="line">    fig, axes = plt.subplots(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>, figsize=(<span class="hljs-number">14</span>,<span class="hljs-number">12</span>),sharey=<span class="hljs-literal">True</span>,sharex=<span class="hljs-literal">True</span>)</span><br><span class="line">    P=data[<span class="hljs-string">&quot;population&quot;</span>]</span><br><span class="line">    norm=matplotlib.colors.LogNorm(vmin=<span class="hljs-number">100</span>_000, vmax=P.max(), clip=<span class="hljs-literal">False</span>)</span><br><span class="line">    count=<span class="hljs-number">0</span></span><br><span class="line">    <span class="hljs-keyword">for</span> date,group <span class="hljs-keyword">in</span> select_data(data,<span class="hljs-number">4</span>):</span><br><span class="line">        col=count %<span class="hljs-number">2</span> </span><br><span class="line">        row=count //<span class="hljs-number">2</span></span><br><span class="line">        ax=axes[row][col]</span><br><span class="line">        ax.set_title(date.strftime(<span class="hljs-string">&quot;%Y-%m-%d&quot;</span>))</span><br><span class="line">        im=plot_result(group,norm,ax)</span><br><span class="line">        count+=<span class="hljs-number">1</span></span><br><span class="line">    cbar = fig.colorbar(im, ax=axes.ravel().tolist(), shrink=<span class="hljs-number">0.5</span>,label=<span class="hljs-string">&quot;population&quot;</span>)</span><br></pre></td></tr></tbody></table></figure>


<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-keyword">with</span> pm.Model() <span class="hljs-keyword">as</span> mod:</span><br><span class="line">    y_pred=pm.NegativeBinomial(<span class="hljs-string">&quot;y_pred&quot;</span>,mu=y_hat*<span class="hljs-number">100</span>_000,alpha=a,shape=(len(X_train)))</span><br><span class="line">    posterior_predictive = pm.sample_posterior_predictive(trace, var_names=[<span class="hljs-string">&quot;y_pred&quot;</span>])</span><br></pre></td></tr></tbody></table></figure>

<pre><code>100%|&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;| 4000/4000 [00:05&lt;00:00, 798.51it/s]</code></pre>
<p>Remember, there are 4000 sample predictions for each of the records, I won&#x2019;t plot out all of them, but will do their means with one standard deviation of uncertainty. The error bars would make prefect graphical representations.</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">predicted=posterior_predictive[<span class="hljs-string">&quot;y_pred&quot;</span>]</span><br><span class="line">predicted.shape,train_data.shape</span><br></pre></td></tr></tbody></table></figure>




<pre><code>((4000, 1632), (1632, 6))</code></pre>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">y_pred_mean=predicted.mean(axis=<span class="hljs-number">0</span>)</span><br><span class="line">y_pred_std=predicted.std(axis=<span class="hljs-number">0</span>)</span><br><span class="line"></span><br><span class="line">data=train_data.copy()</span><br><span class="line">data[<span class="hljs-string">&quot;y_pred&quot;</span>]=y_pred_mean</span><br><span class="line">data[<span class="hljs-string">&quot;y_std&quot;</span>]=y_pred_std</span><br></pre></td></tr></tbody></table></figure>

<p>Looks like the predictions are closer to the actuals since we incorporate the random effects. Hooray!</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">plot_time_facets(data,event)</span><br></pre></td></tr></tbody></table></figure>

<img src="http://yumeng-li.github.io/projectcovidmixed_files/projectcovidmixed_43_0.png">


<h2 id="Differences-comparing-to-linear-regression"><a href="#Differences-comparing-to-linear-regression" class="headerlink" title="Differences comparing to linear regression"></a>Differences comparing to linear regression</h2><h3 id="Random-Intercepts"><a href="#Random-Intercepts" class="headerlink" title="Random Intercepts"></a>Random Intercepts</h3><p>Let&#x2019;s take a further look by state to better understand what I meant by random effects/intercepts.</p>
<p>Below is all the observed data points marked by state. We can see that roughly the slopes of different groups are close, while the intercepts can vary.</p>
<p>In case you wonder why the slopes are upward, that&#x2019;s due to the passage of time, remember time is our explanatory variable as well, but we can only plot two dimensions.</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">fig, ax = plt.subplots()</span><br><span class="line">P=data[<span class="hljs-string">&quot;population&quot;</span>]</span><br><span class="line">norm=matplotlib.colors.LogNorm(vmin=<span class="hljs-number">100</span>_000, vmax=P.max(), clip=<span class="hljs-literal">False</span>)</span><br><span class="line">m=ax.scatter(data[<span class="hljs-string">&quot;vaccinated&quot;</span>],data[event]/data[<span class="hljs-string">&quot;population&quot;</span>]*<span class="hljs-number">100</span>_000,</span><br><span class="line">            c=P,norm=norm,cmap=<span class="hljs-string">&quot;Greys&quot;</span>,label=event, s = <span class="hljs-number">5</span>)</span><br><span class="line">ax.set_xlabel(<span class="hljs-string">&quot;vaccinated&quot;</span>)</span><br><span class="line">ax.set_ylabel(<span class="hljs-string">f&quot;<span class="hljs-subst">{event}</span> per 100k by state&quot;</span>)</span><br><span class="line">ax.legend()</span><br></pre></td></tr></tbody></table></figure>



<img src="http://yumeng-li.github.io/projectcovidmixed_files/projectcovidmixed_46_1.png">



<p>Let&#x2019;s select three states in the middle to plot their predictions against actuals.</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">plot_states</span>(<span class="hljs-params">data, states, var</span>):</span></span><br><span class="line">    data = data[data[<span class="hljs-string">&apos;state&apos;</span>].isin(states)]</span><br><span class="line">    data = data.sort_values(<span class="hljs-string">&apos;vaccinated&apos;</span>)</span><br><span class="line">    x = data[var]</span><br><span class="line">    y = data[<span class="hljs-string">&quot;y_pred&quot;</span>]</span><br><span class="line">    std = data[<span class="hljs-string">&quot;y_std&quot;</span>]</span><br><span class="line">    cmp=cm.get_cmap(<span class="hljs-string">&quot;tab10&quot;</span>)</span><br><span class="line">    plt.xlim([<span class="hljs-number">0.4475</span>,<span class="hljs-number">0.466</span>])</span><br><span class="line">    plt.scatter(x,y, c=<span class="hljs-string">&apos;black&apos;</span>,s=<span class="hljs-number">15</span>)</span><br><span class="line">    colors = np.array([<span class="hljs-string">&apos;tab:blue&apos;</span>,<span class="hljs-string">&apos;tab:green&apos;</span>,<span class="hljs-string">&apos;tab:orange&apos;</span>,<span class="hljs-string">&apos;tab:red&apos;</span>])</span><br><span class="line">    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(len(states)):</span><br><span class="line">        s = states[i]</span><br><span class="line">        ds = data[data[<span class="hljs-string">&apos;state&apos;</span>]==s]</span><br><span class="line">        x = ds[var]</span><br><span class="line">        y = ds[<span class="hljs-string">&quot;y_pred&quot;</span>]</span><br><span class="line">        std = ds[<span class="hljs-string">&quot;y_std&quot;</span>]</span><br><span class="line">        color = colors[i]</span><br><span class="line">        plt.plot(x,y,<span class="hljs-string">&apos;k-&apos;</span>,color=color,linewidth=<span class="hljs-number">2.0</span>, label=<span class="hljs-string">f&quot;predicted <span class="hljs-subst">{s}</span>&quot;</span>)</span><br><span class="line">        plt.fill_between(x,y+std,y-std,alpha=<span class="hljs-number">0.15</span>,color=color)</span><br><span class="line">        plt.xlabel(<span class="hljs-string">&quot;vaccinated&quot;</span>)</span><br><span class="line">        plt.ylabel(<span class="hljs-string">f&quot;<span class="hljs-subst">{event}</span> per 100k by state&quot;</span>)</span><br><span class="line">        plt.legend(loc=<span class="hljs-string">&quot;upper left&quot;</span>)</span><br></pre></td></tr></tbody></table></figure>

<p>This shows clearly how the mixed model leverages all the data points to estimate the correlation (slope) between admissions and vaccinations, but at the same time allows states to have different starts (intercept) due to fundamental differences which we consider are random.</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">plot_states(data,[<span class="hljs-string">&apos;KY&apos;</span>,<span class="hljs-string">&apos;AK&apos;</span>,<span class="hljs-string">&apos;KS&apos;</span>],<span class="hljs-string">&quot;vaccinated&quot;</span>)</span><br></pre></td></tr></tbody></table></figure>

<img src="http://yumeng-li.github.io/projectcovidmixed_files/projectcovidmixed_50_0.png">


<h3 id="Variance"><a href="#Variance" class="headerlink" title="Variance"></a>Variance</h3><p>The Bayesian approach gives the distributions of the predictions, so we don&#x2019;t need to rack our brains for the analytical solution of the prediction band, instead, we can simply plot it out with the samples generated by <code>pymc3</code>. Here I used one unit of the sample standard deviation. </p>
<p>As we can see, the variance of prediction is larger with greater admissions, meaning they are positively correlated, which is not what we&#x2019;ve seen with the linear regression that the variance of dependent variable is always constant. So using GLMM <strong>based on a negative binomial distribution</strong> allows the magnitude of the variance to change as a function of the predicted value.</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">plot_state</span>(<span class="hljs-params">data, state, color</span>):</span></span><br><span class="line">    data = data[data[<span class="hljs-string">&apos;state&apos;</span>]==state]</span><br><span class="line">    fig, ax = plt.subplots()</span><br><span class="line">    ax.scatter(data[<span class="hljs-string">&quot;vaccinated&quot;</span>],data[event]/data[<span class="hljs-string">&quot;population&quot;</span>]*<span class="hljs-number">100</span>_000,c=<span class="hljs-string">&apos;black&apos;</span>,s=<span class="hljs-number">15</span>,label=event)    </span><br><span class="line">    ax.errorbar(data[<span class="hljs-string">&quot;vaccinated&quot;</span>],data[<span class="hljs-string">&quot;y_pred&quot;</span>],yerr=data[<span class="hljs-string">&quot;y_std&quot;</span>],fmt=<span class="hljs-string">&quot;o&quot;</span>,alpha=<span class="hljs-number">0.25</span>,c=color,</span><br><span class="line">               label=<span class="hljs-string">&quot;predicted&quot;</span>)</span><br><span class="line">    ax.set_xlabel(<span class="hljs-string">&quot;vaccinated&quot;</span>)</span><br><span class="line">    ax.set_ylabel(<span class="hljs-string">f&quot;<span class="hljs-subst">{event}</span> per 100k&quot;</span>)</span><br><span class="line">    ax.legend()</span><br></pre></td></tr></tbody></table></figure>


<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">plot_state(data,<span class="hljs-string">&apos;KY&apos;</span>,<span class="hljs-string">&apos;tab:blue&apos;</span>)</span><br></pre></td></tr></tbody></table></figure>

<img src="http://yumeng-li.github.io/projectcovidmixed_files/projectcovidmixed_53_0.png">



<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">plot_state(data,<span class="hljs-string">&apos;FL&apos;</span>,<span class="hljs-string">&apos;tab:green&apos;</span>)</span><br></pre></td></tr></tbody></table></figure>

<img src="http://yumeng-li.github.io/projectcovidmixed_files/projectcovidmixed_54_0.png">


<h2 id="Predicted-Dependence-on-vaccination-Rate"><a href="#Predicted-Dependence-on-vaccination-Rate" class="headerlink" title="Predicted Dependence on vaccination Rate"></a>Predicted Dependence on vaccination Rate</h2><p>One of the things that we are interested is that how does the vaccination rate impact the admissions?</p>
<p>To see this better, I fixed the time at Aug.15, and let the vaccination rate to vary from 20% to 100%.</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-comment"># given current time point, generate test set for vaccination rate from 20% to 100%</span></span><br><span class="line">V_N=<span class="hljs-number">201</span></span><br><span class="line">V=np.linspace(<span class="hljs-number">0.2</span>,<span class="hljs-number">1</span>,V_N)</span><br><span class="line">T=np.ones(V_N)</span><br><span class="line">T2=T**<span class="hljs-number">2</span></span><br><span class="line">X2_test=np.c_[np.ones(T_N),V,T,T2]</span><br></pre></td></tr></tbody></table></figure>


<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-keyword">with</span> pm.Model() <span class="hljs-keyword">as</span> mod:</span><br><span class="line">    eta2_pred=pm.math.dot(X2_test,beta)</span><br><span class="line">    y_pred2=pm.Deterministic(<span class="hljs-string">&quot;y_pred2&quot;</span>,pm.math.exp(eta2_pred)*<span class="hljs-number">100</span>_000)</span><br><span class="line">    posterior_predictive = pm.sample_posterior_predictive(trace, var_names=[<span class="hljs-string">&quot;y_pred2&quot;</span>])</span><br></pre></td></tr></tbody></table></figure>

<pre><code>100%|&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;| 4000/4000 [00:01&lt;00:00, 3891.10it/s]</code></pre>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-comment"># generate predicted mean and standard deviation</span></span><br><span class="line">predicted=posterior_predictive[<span class="hljs-string">&quot;y_pred2&quot;</span>]</span><br><span class="line">y_pred_mean=predicted.mean(axis=<span class="hljs-number">0</span>)</span><br><span class="line">y_pred_std=predicted.std(axis=<span class="hljs-number">0</span>)</span><br></pre></td></tr></tbody></table></figure>

<p>We can see that vaccinations and admissions are negatively correlated. And when vaccination rate is low, the prediction can be quite uncertain.</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">plt.plot(V,y_pred_mean,<span class="hljs-string">&quot;k--&quot;</span>,linewidth=<span class="hljs-number">1</span>)</span><br><span class="line">plt.fill_between(V,y_pred_mean+y_pred_std,y_pred_mean-y_pred_std,alpha=<span class="hljs-number">0.07</span>,color=<span class="hljs-string">&quot;k&quot;</span>)</span><br><span class="line">plt.fill_between(V,y_pred_mean+<span class="hljs-number">2</span>*y_pred_std,y_pred_mean<span class="hljs-number">-2</span>*y_pred_std,alpha=<span class="hljs-number">0.07</span>,color=<span class="hljs-string">&quot;k&quot;</span>)</span><br><span class="line">plt.xlabel(<span class="hljs-string">&quot;Vaccinated&quot;</span>)</span><br><span class="line">plt.ylabel(<span class="hljs-string">f&quot;<span class="hljs-subst">{event}</span> per 100k&quot;</span>)</span><br></pre></td></tr></tbody></table></figure>



<img src="http://yumeng-li.github.io/projectcovidmixed_files/projectcovidmixed_61_1.png">



<h2 id="Predicted-Time-Evolution"><a href="#Predicted-Time-Evolution" class="headerlink" title="Predicted Time Evolution"></a>Predicted Time Evolution</h2><p>The other thing that we are most interested in is that how the third wave of covid19 is going to evolve. How many admissions are there going to be given the current vaccination rate? So I use the GLMM model to project the next two months from Aug.15th on. </p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-comment"># given the average vaccination rates</span></span><br><span class="line">mean_vac=(train_data[<span class="hljs-string">&quot;vaccinated&quot;</span>]*train_data[<span class="hljs-string">&quot;population&quot;</span>]).sum()/train_data[<span class="hljs-string">&quot;population&quot;</span>].sum()</span><br><span class="line">mean_vac</span><br></pre></td></tr></tbody></table></figure>




<pre><code>0.48852180987716365</code></pre>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-comment"># generate test set for the past month and the next 2 months</span></span><br><span class="line">T_N=<span class="hljs-number">201</span></span><br><span class="line">T=np.linspace(<span class="hljs-number">0</span>,<span class="hljs-number">3</span>,T_N)</span><br><span class="line">T2=T**<span class="hljs-number">2</span></span><br><span class="line">X1_test=np.c_[np.ones(T_N),mean_vac*np.ones(T_N),T,T2]</span><br></pre></td></tr></tbody></table></figure>


<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-keyword">with</span> pm.Model() <span class="hljs-keyword">as</span> mod:</span><br><span class="line">    eta1_pred=pm.math.dot(X1_test,beta)</span><br><span class="line">    y_pred1=pm.Deterministic(<span class="hljs-string">&quot;y_pred1&quot;</span>,pm.math.exp(eta1_pred)*<span class="hljs-number">100</span>_000)</span><br><span class="line">    posterior_predictive = pm.sample_posterior_predictive(trace, var_names=[<span class="hljs-string">&quot;y_pred1&quot;</span>])</span><br></pre></td></tr></tbody></table></figure>

<pre><code>100%|&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;&#x2588;| 4000/4000 [00:01&lt;00:00, 3831.47it/s]</code></pre>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-comment"># generate predicted mean and standard deviation</span></span><br><span class="line">predicted=posterior_predictive[<span class="hljs-string">&quot;y_pred1&quot;</span>]</span><br><span class="line">y_pred_mean=predicted.mean(axis=<span class="hljs-number">0</span>)</span><br><span class="line">y_pred_std=predicted.std(axis=<span class="hljs-number">0</span>)</span><br></pre></td></tr></tbody></table></figure>

<p><strong>The prediction tells us that the admission will peak in one and half month from Aug.15th, which is by the end of September.</strong> However, I would be careful as the model have low confidence when projecting beyond one month. Look at that wide confidential interval band after T=2. Basically, the reality can fall any where between the curve flattening out as early as the start of September to that we won&#x2019;t be able to see a peak until mid October. But overall I am very optimistic as there&#x2019;s at least 80% of chance that we&#x2019;ll see it peaking before mid October. Eventually, we will get there. So hang in there, everyone!!</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">T0=T[T&lt;<span class="hljs-number">1</span>]</span><br><span class="line">plt.plot(T0,y_pred_mean[T&lt;<span class="hljs-number">1</span>],<span class="hljs-string">&quot;k&quot;</span>,linewidth=<span class="hljs-number">4</span>,label=<span class="hljs-string">&quot;fitted&quot;</span>)</span><br><span class="line">plt.plot(T,y_pred_mean,<span class="hljs-string">&quot;k--&quot;</span>,linewidth=<span class="hljs-number">1</span>,label=<span class="hljs-string">&quot;extrapolated&quot;</span>)</span><br><span class="line">plt.fill_between(T,y_pred_mean+y_pred_std,y_pred_mean-y_pred_std,alpha=<span class="hljs-number">0.07</span>,color=<span class="hljs-string">&quot;k&quot;</span>)</span><br><span class="line">plt.fill_between(T,y_pred_mean+<span class="hljs-number">2</span>*y_pred_std,y_pred_mean<span class="hljs-number">-2</span>*y_pred_std,alpha=<span class="hljs-number">0.07</span>,color=<span class="hljs-string">&quot;k&quot;</span>,)</span><br><span class="line">plt.xlabel(<span class="hljs-string">&quot;T (months)&quot;</span>)</span><br><span class="line">plt.ylabel(<span class="hljs-string">f&quot;<span class="hljs-subst">{event}</span> per 100k&quot;</span>)</span><br><span class="line">plt.legend(loc=<span class="hljs-string">&quot;lower right&quot;</span>)</span><br></pre></td></tr></tbody></table></figure>




<img src="http://yumeng-li.github.io/projectcovidmixed_files/projectcovidmixed_68_1.png">



<p>Officially we are done, cheers! I hope you find this project inspiring. And if you read it all the way through - congrats, you are an incredibly patient person, have a cookie!</p>
<img src="http://yumeng-li.github.io/projectcovidmixed_files/projectcovidmixed_cookie.png" width="400">
</body></html>
    
    </div>
    
    
</article>




    
        <article class="article content gallery" itemscope itemprop="blogPost">
    <h1 class="article-title is-size-3 is-size-4-mobile" itemprop="name">
        
            <a href="/2021/09/26/projectcovidGLM/" itemprop="url">Project Covid Admissions Based on Vaccine Progress, Part 2 -- Generalized Linear Models, From Poisson Regression to Negative Binomial Regression</a>
        
    </h1>
    <div class="article-meta columns is-variable is-1 is-multiline is-mobile is-size-7-mobile">
        <span class="column is-narrow">
            <time datetime="2021-09-26T16:00:00.000Z" itemprop="datePublished">Sep 26 2021</time>
        </span>
        
        <span class="column is-narrow article-category">
            <i class="far fa-folder"></i>
            <a class="article-category-link" href="/categories/Applied-Statistics/">Applied Statistics</a>
        </span>
        
        
        <span class="column is-narrow">
            
            
            18 minutes read (About 2705 words)
        </span>
        
    </div>
    <div class="article-entry is-size-6-mobile" itemprop="articleBody">
    
        <html><head></head><body><p>In the last blog, we try to use linear regressions to model hospital admissions and vaccinations. The findings were the linear model tend to over-estimating how fast the admission rate is increasing on the higher range of vaccination rates, and residual variance is negatively correlated with vaccinations rates, suggesting heteroskedasticity. But it&#x2019;s easy-peasy for <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Generalized_linear_model">Generalized linear models</a> to fix those issues. GLM generalizes linear regression, 1. by allowing the linear model to be related to the response variable via a link function, 2. by allowing for the response variable to have an error distribution other than the normal distribution so that the magnitude of the variance of each measurement can change as a function of its predicted value.</p>
<h2 id="Preliminaries"><a href="#Preliminaries" class="headerlink" title="Preliminaries"></a>Preliminaries</h2><figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np</span><br><span class="line"><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd</span><br><span class="line"><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt</span><br><span class="line"><span class="hljs-keyword">import</span> matplotlib.colors</span><br><span class="line"><span class="hljs-keyword">import</span> seaborn <span class="hljs-keyword">as</span> sns</span><br><span class="line"><span class="hljs-keyword">import</span> statsmodels.api <span class="hljs-keyword">as</span> sm</span><br><span class="line"><span class="hljs-keyword">from</span> statsmodels.discrete.discrete_model <span class="hljs-keyword">import</span> Poisson</span><br><span class="line"><span class="hljs-keyword">from</span> statsmodels.discrete.discrete_model <span class="hljs-keyword">import</span> NegativeBinomial</span><br><span class="line"><span class="hljs-keyword">import</span> sys</span><br><span class="line"><span class="hljs-keyword">import</span> covid_analysis <span class="hljs-keyword">as</span> covid</span><br></pre></td></tr></tbody></table></figure>

<p>The data I use includes hospital admissions and vaccinations from the CDC website, and the most up-to-date populations by state from the US Census Bureau.</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data_dir=<span class="hljs-string">&quot;../data&quot;</span></span><br></pre></td></tr></tbody></table></figure>


<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">hospitalizations=pd.read_csv(<span class="hljs-string">f&quot;<span class="hljs-subst">{data_dir}</span>/covid_hospitalizations.csv&quot;</span>,parse_dates=[<span class="hljs-string">&quot;date&quot;</span>])</span><br><span class="line">vaccinations=pd.read_csv(<span class="hljs-string">f&quot;<span class="hljs-subst">{data_dir}</span>/covid_vaccinations.csv&quot;</span>,parse_dates=[<span class="hljs-string">&quot;date&quot;</span>])</span><br><span class="line">population=pd.read_csv(<span class="hljs-string">f&quot;<span class="hljs-subst">{data_dir}</span>/population.csv&quot;</span>)</span><br><span class="line"></span><br><span class="line">event=<span class="hljs-string">&quot;admissions&quot;</span></span><br><span class="line">data=hospitalizations</span><br><span class="line"></span><br><span class="line">data=data.merge(vaccinations,on=[<span class="hljs-string">&quot;date&quot;</span>,<span class="hljs-string">&quot;state&quot;</span>])</span><br><span class="line">data=data.merge(population,on=<span class="hljs-string">&quot;state&quot;</span>)</span><br><span class="line">data[<span class="hljs-string">&quot;vaccinated&quot;</span>]=data[<span class="hljs-string">&quot;vaccinated&quot;</span>]/data[<span class="hljs-string">&quot;population&quot;</span>]</span><br><span class="line">data.head()</span><br></pre></td></tr></tbody></table></figure>


<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;</code></pre>
<p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>date</th>
      <th>state</th>
      <th>used_beds</th>
      <th>admissions</th>
      <th>vaccinated</th>
      <th>population</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>2021-06-10</td>
      <td>MT</td>
      <td>66.0</td>
      <td>16.0</td>
      <td>0.397597</td>
      <td>1080577</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2021-06-21</td>
      <td>MT</td>
      <td>57.0</td>
      <td>13.0</td>
      <td>0.411614</td>
      <td>1080577</td>
    </tr>
    <tr>
      <th>2</th>
      <td>2021-08-05</td>
      <td>MT</td>
      <td>149.0</td>
      <td>23.0</td>
      <td>0.440153</td>
      <td>1080577</td>
    </tr>
    <tr>
      <th>3</th>
      <td>2021-07-15</td>
      <td>MT</td>
      <td>62.0</td>
      <td>9.0</td>
      <td>0.431722</td>
      <td>1080577</td>
    </tr>
    <tr>
      <th>4</th>
      <td>2021-08-21</td>
      <td>MT</td>
      <td>224.0</td>
      <td>59.0</td>
      <td>0.449196</td>
      <td>1080577</td>
    </tr>
  </tbody>
</table>
</div>



<h2 id="Poisson-Regression"><a href="#Poisson-Regression" class="headerlink" title="Poisson Regression"></a>Poisson Regression</h2><p>Like what we did with the linear regression, we can choose either to use the  <strong>Poisson Regression</strong> model from <a target="_blank" rel="noopener" href="https://www.statsmodels.org/stable/generated/statsmodels.discrete.discrete_model.Poisson.html">statsmodels</a>, or the GLM from the same lib. The two will give us the exact same estimations and statistics.</p>
<h3 id="Model-Definition"><a href="#Model-Definition" class="headerlink" title="Model Definition"></a>Model Definition</h3><p>We have data from multiple dates, so it makes sense to incorporate Time as an extra variable.</p>
<p>\begin{eqnarray}<br>    &amp;\eta_i &amp;=c + \beta_1 x_i  + \beta_2 t_i + \beta_3 t_i^2 \newline<br>    \mathbb{E}(y_i|x_i) =&amp; \lambda_iP_i &amp;= e^{\eta_i }P_i \newline<br>    &amp;p(y_i|x_i) &amp; = \text{Poisson}(y;\lambda_i P_i)<br>\end{eqnarray}<br>where </p>
<ul>
<li>Each observation $i$ represents a single state at one particular date.</li>
<li>$y_i$ is the number of hospital admissions on that state.</li>
<li>$x_i$ is the percentage of the state population fully vaccinated that state.</li>
<li>$t_i$ is the number of days elapsed since the beginning of the training period.</li>
<li>$P_i$ is the population of the  state.</li>
<li>the parameters $c$ and $\beta_1,\beta_2,\beta_3$ will be fitted to the observed data to maximize agreement with the model</li>
</ul>
<p>We including coefficients for $t$ and $t^2$ the dependence of the admission rate on time can have some curvature and does not need to be linear.</p>
<p>This can be fitter as a <strong>linear model</strong> where the inputs are $x_i$, $t_i$ and $t_i^2$</p>
<p>First we select the training period:</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">start_train=<span class="hljs-string">&quot;2021-07-15&quot;</span></span><br><span class="line">end_train=<span class="hljs-string">&quot;2021-08-15&quot;</span></span><br><span class="line">train_data=data[(data[<span class="hljs-string">&apos;date&apos;</span>] &gt;= start_train) &amp; (data[<span class="hljs-string">&apos;date&apos;</span>] &lt;= end_train)].copy()</span><br><span class="line">date0=train_data[<span class="hljs-string">&quot;date&quot;</span>].min()</span><br></pre></td></tr></tbody></table></figure>

<p>Then a 7-day testing period:</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">test_period=<span class="hljs-number">7</span> <span class="hljs-comment"># days</span></span><br><span class="line">test_end=pd.Timestamp(end_train)+pd.DateOffset(days=test_period)</span><br><span class="line">test_data=data[(data[<span class="hljs-string">&quot;date&quot;</span>]&gt;end_train) &amp; (data[<span class="hljs-string">&quot;date&quot;</span>]&lt;=test_end)].copy()</span><br></pre></td></tr></tbody></table></figure>


<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">X_train,P_train=covid.define_variables(train_data,date0)</span><br><span class="line">Y_train=train_data[event]</span><br><span class="line">X_test,P_test=covid.define_variables(test_data,date0)</span><br><span class="line">Y_test=test_data[event]</span><br></pre></td></tr></tbody></table></figure>

<p>Fit the model to the train data,</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">mod=Poisson(Y_train,X_train,exposure=P_train.values)</span><br><span class="line">res=mod.fit()</span><br><span class="line">res.summary()</span><br></pre></td></tr></tbody></table></figure>

<pre><code>Optimization terminated successfully.
         Current function value: 32.115992
         Iterations 6</code></pre>
<table class="simpletable">
<caption>Poisson Regression Results</caption>
<tbody><tr>
  <th>Dep. Variable:</th>      <td>admissions</td>    <th>  No. Observations:  </th>  <td>  1632</td> 
</tr>
<tr>
  <th>Model:</th>                <td>Poisson</td>     <th>  Df Residuals:      </th>  <td>  1628</td> 
</tr>
<tr>
  <th>Method:</th>                 <td>MLE</td>       <th>  Df Model:          </th>  <td>     3</td> 
</tr>
<tr>
  <th>Date:</th>            <td>Sun, 26 Sep 2021</td> <th>  Pseudo R-squ.:     </th>  <td>0.4493</td> 
</tr>
<tr>
  <th>Time:</th>                <td>19:44:49</td>     <th>  Log-Likelihood:    </th> <td> -52413.</td>
</tr>
<tr>
  <th>converged:</th>             <td>True</td>       <th>  LL-Null:           </th> <td> -95175.</td>
</tr>
<tr>
  <th>Covariance Type:</th>     <td>nonrobust</td>    <th>  LLR p-value:       </th>  <td> 0.000</td> 
</tr>
</tbody></table>
<table class="simpletable">
<tbody><tr>
       <td></td>         <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P&gt;|z|</th>  <th>[0.025</th>    <th>0.975]</th>  
</tr>
<tr>
  <th>vaccinated</th> <td>   -6.7589</td> <td>    0.030</td> <td> -225.710</td> <td> 0.000</td> <td>   -6.818</td> <td>   -6.700</td>
</tr>
<tr>
  <th>T</th>          <td>    0.0750</td> <td>    0.001</td> <td>   75.492</td> <td> 0.000</td> <td>    0.073</td> <td>    0.077</td>
</tr>
<tr>
  <th>T2</th>         <td>   -0.0009</td> <td> 2.84e-05</td> <td>  -31.349</td> <td> 0.000</td> <td>   -0.001</td> <td>   -0.001</td>
</tr>
<tr>
  <th>const</th>      <td>   -8.4423</td> <td>    0.015</td> <td> -547.566</td> <td> 0.000</td> <td>   -8.473</td> <td>   -8.412</td>
</tr>
</tbody></table>




<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">mod=sm.GLM(Y_train,X_train,exposure=P_train,family=sm.families.Poisson(sm.families.links.log()))</span><br><span class="line">glm_res=mod.fit()</span><br><span class="line">glm_res.summary()</span><br></pre></td></tr></tbody></table></figure>




<table class="simpletable">
<caption>Generalized Linear Model Regression Results</caption>
<tbody><tr>
  <th>Dep. Variable:</th>      <td>admissions</td>    <th>  No. Observations:  </th>  <td>  1632</td> 
</tr>
<tr>
  <th>Model:</th>                  <td>GLM</td>       <th>  Df Residuals:      </th>  <td>  1628</td> 
</tr>
<tr>
  <th>Model Family:</th>         <td>Poisson</td>     <th>  Df Model:          </th>  <td>     3</td> 
</tr>
<tr>
  <th>Link Function:</th>          <td>log</td>       <th>  Scale:             </th> <td>  1.0000</td>
</tr>
<tr>
  <th>Method:</th>                <td>IRLS</td>       <th>  Log-Likelihood:    </th> <td> -52413.</td>
</tr>
<tr>
  <th>Date:</th>            <td>Sun, 26 Sep 2021</td> <th>  Deviance:          </th> <td>  95443.</td>
</tr>
<tr>
  <th>Time:</th>                <td>19:44:50</td>     <th>  Pearson chi2:      </th> <td>1.26e+05</td>
</tr>
<tr>
  <th>No. Iterations:</th>          <td>6</td>        <th>                     </th>     <td> </td>   
</tr>
<tr>
  <th>Covariance Type:</th>     <td>nonrobust</td>    <th>                     </th>     <td> </td>   
</tr>
</tbody></table>
<table class="simpletable">
<tbody><tr>
       <td></td>         <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P&gt;|z|</th>  <th>[0.025</th>    <th>0.975]</th>  
</tr>
<tr>
  <th>vaccinated</th> <td>   -6.7589</td> <td>    0.030</td> <td> -225.710</td> <td> 0.000</td> <td>   -6.818</td> <td>   -6.700</td>
</tr>
<tr>
  <th>T</th>          <td>    0.0750</td> <td>    0.001</td> <td>   75.492</td> <td> 0.000</td> <td>    0.073</td> <td>    0.077</td>
</tr>
<tr>
  <th>T2</th>         <td>   -0.0009</td> <td> 2.84e-05</td> <td>  -31.349</td> <td> 0.000</td> <td>   -0.001</td> <td>   -0.001</td>
</tr>
<tr>
  <th>const</th>      <td>   -8.4423</td> <td>    0.015</td> <td> -547.566</td> <td> 0.000</td> <td>   -8.473</td> <td>   -8.412</td>
</tr>
</tbody></table>



<p>Poisson regression is fit by maximum likelihood, there are several <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Logistic_regression#Pseudo-R-squared">choices of <strong>Pseudo R-square</strong></a><br>Here, what statsmodels implements for poisson regression is McFadden $R_{McF}^2$ that is defined as ratio of log likelihood for the fitted model and log likelihood of a model fitted to just a constant.</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-comment"># Pseudo R-squ McF</span></span><br><span class="line">res.prsquared</span><br></pre></td></tr></tbody></table></figure>




<pre><code>0.4492953669633617</code></pre>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">res.llf, res.llnull</span><br></pre></td></tr></tbody></table></figure>




<pre><code>(-52413.29834953645, -95174.97258108124)</code></pre>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-number">1</span>-res.llf/res.llnull</span><br></pre></td></tr></tbody></table></figure>




<pre><code>0.4492953669633617</code></pre>
<p>Certainly, we can implement other measures of the Pseudo R-square ourselves. Here I did $R_{L}^2$, which is deviance-based. It&#x2019;s the most analogous index to the squared multiple correlations in linear regression. We will stick to it in our analysis from now on.</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">poisson_deviance</span>(<span class="hljs-params">y,y_pred</span>):</span></span><br><span class="line">    <span class="hljs-keyword">return</span> <span class="hljs-number">2</span>*np.sum(y*np.log(np.maximum(y,<span class="hljs-number">1e-12</span>)/y_pred)-(y-y_pred))</span><br></pre></td></tr></tbody></table></figure>


<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">mu_train=Y_train.sum()/P_train.sum()</span><br><span class="line">Y_pred=glm_res.predict(X_train,exposure=P_train)</span><br><span class="line"></span><br><span class="line">deviance=poisson_deviance(Y_train,Y_pred)</span><br><span class="line">null_deviance=poisson_deviance(Y_train,mu_train*P_train)</span><br><span class="line">R2=<span class="hljs-number">1</span>-deviance/null_deviance</span><br><span class="line"><span class="hljs-comment"># Pseudo R-squ</span></span><br><span class="line">R2</span><br></pre></td></tr></tbody></table></figure>




<pre><code>0.4725915412837366</code></pre>
<p>If you don&#x2019;t want bother yourself with the mathematical formulas, here&#x2019;s a simpler way (my favorite as the laziest person ever). Just fit the model with a constant, and take the log-likelihood/deviance as your null case.</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">mod=sm.GLM(Y_train,X_train[<span class="hljs-string">&apos;const&apos;</span>],exposure=P_train,family=sm.families.Poisson(sm.families.links.log()))</span><br><span class="line">res_null=mod.fit()</span><br><span class="line"><span class="hljs-comment"># deviance for a const fit model</span></span><br><span class="line">res_null.deviance</span><br></pre></td></tr></tbody></table></figure>




<pre><code>180966.73552551522</code></pre>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-comment"># null deviance from our math formula</span></span><br><span class="line">null_deviance</span><br></pre></td></tr></tbody></table></figure>




<pre><code>180966.7355255152</code></pre>
<p>After all, the R-squared we have for now is 0.473.</p>
<p>Next let&#x2019;s check on how the model predicts. </p>
<h3 id="Extrapolation-of-Time-trends"><a href="#Extrapolation-of-Time-trends" class="headerlink" title="Extrapolation of Time trends"></a>Extrapolation of Time trends</h3><p>Remember what we saw last time with the linear model? Its projection was a straight line. But the poisson model is telling us a different story, that the admissions will flatten out assuming the current vaccination rate. Well, it is onto something, isn&#x2019;t it?</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">covid.plot_time_extrapolation(train_data,glm_res,event,date0)</span><br></pre></td></tr></tbody></table></figure>

<img src="http://yumeng-li.github.io/projectcovidGLM_files/projectcovidGLM_30_0.png">



<h3 id="In-sample-model-Fit"><a href="#In-sample-model-Fit" class="headerlink" title="In-sample model Fit"></a>In-sample model Fit</h3><p>To better evaluate the model fit, we visualize the real observations and model predictions. Here&#x2019;s some snapshots of the fit for a few selected dates on the training period.</p>
<p>As we can see, curves from the poisson regression model fits the data much better than the linear model, indicating the correlation between admissions and vaccinations are not linear.</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">covid.plot_time_facets(train_data,glm_res,event,date0)</span><br></pre></td></tr></tbody></table></figure>


<img src="http://yumeng-li.github.io/projectcovidGLM_files/projectcovidGLM_32_0.png">


<p>But is this enough? Certainly not, let&#x2019;s verify the deviance residuals by visualizing them,</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">plot_deviance_residuals</span>(<span class="hljs-params">data,event,res</span>):</span></span><br><span class="line">    P=data[<span class="hljs-string">&quot;population&quot;</span>]</span><br><span class="line">    <span class="hljs-comment"># Create two subplots and unpack the output array immediately</span></span><br><span class="line">    f, (ax1, ax2) = plt.subplots(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>,sharex=<span class="hljs-literal">True</span>,figsize=(<span class="hljs-number">16</span>,<span class="hljs-number">4</span>))</span><br><span class="line">    norm=matplotlib.colors.LogNorm(vmin=<span class="hljs-number">100</span>_000, vmax=P.max(), clip=<span class="hljs-literal">False</span>)</span><br><span class="line">    m=ax1.scatter(data[<span class="hljs-string">&quot;vaccinated&quot;</span>],data[event]/data[<span class="hljs-string">&quot;population&quot;</span>]*<span class="hljs-number">100</span>_000,</span><br><span class="line">                c=P,norm=norm,cmap=<span class="hljs-string">&quot;Blues&quot;</span>,label=event)</span><br><span class="line">    plt.colorbar(m,label=<span class="hljs-string">&quot;State Population&quot;</span>)</span><br><span class="line">    x=np.linspace(<span class="hljs-number">0.3</span>,<span class="hljs-number">0.75</span>,<span class="hljs-number">201</span>)</span><br><span class="line">    x= pd.DataFrame({<span class="hljs-string">&apos;vaccinated&apos;</span>:x, <span class="hljs-string">&apos;T&apos;</span>:<span class="hljs-number">31</span>, <span class="hljs-string">&apos;T2&apos;</span>:<span class="hljs-number">31</span>*<span class="hljs-number">31</span>})</span><br><span class="line">    x[<span class="hljs-string">&quot;const&quot;</span>]=np.ones(<span class="hljs-number">201</span>)</span><br><span class="line">    y_pred=res.predict(x)</span><br><span class="line">    ax1.plot(x.vaccinated, y_pred*<span class="hljs-number">100</span>_000,<span class="hljs-string">&quot;k--&quot;</span>,label=<span class="hljs-string">&quot;predicted&quot;</span>) </span><br><span class="line">    </span><br><span class="line">    idx = data.index</span><br><span class="line">    ax2.scatter(data[<span class="hljs-string">&quot;vaccinated&quot;</span>], res.resid_deviance[idx],</span><br><span class="line">                c=P,norm=norm,cmap=<span class="hljs-string">&quot;Blues&quot;</span>,label=event)</span><br><span class="line">    ax2.set_title(<span class="hljs-string">&quot;Residuals&quot;</span>)</span><br><span class="line">    ax2.set_xlabel(<span class="hljs-string">&quot;vaccination&quot;</span>)</span><br><span class="line">    ax2.set_ylabel(<span class="hljs-string">&quot;residual&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="hljs-comment"># select outlier state</span></span><br><span class="line">    outlier_idx=np.argpartition(np.array(res.resid_deviance[idx]), <span class="hljs-number">-2</span>)[<span class="hljs-number">-2</span>:]</span><br><span class="line">    </span><br><span class="line">    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> outlier_idx:</span><br><span class="line">        outlier=data.iloc[i]</span><br><span class="line">        ax1.annotate(outlier[<span class="hljs-string">&quot;state&quot;</span>],(outlier[<span class="hljs-string">&quot;vaccinated&quot;</span>]+<span class="hljs-number">0.01</span>,outlier[event]/outlier[<span class="hljs-string">&quot;population&quot;</span>]*<span class="hljs-number">100</span>_000))</span><br><span class="line">        ax2.annotate(outlier[<span class="hljs-string">&quot;state&quot;</span>],(outlier[<span class="hljs-string">&quot;vaccinated&quot;</span>]+<span class="hljs-number">0.01</span>,res.resid_deviance[idx].iloc[i]))</span><br></pre></td></tr></tbody></table></figure>

<p>The date I choose is Aug.15,</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">offset=<span class="hljs-number">9</span> </span><br><span class="line">interested_date=data[<span class="hljs-string">&quot;date&quot;</span>].max()-pd.offsets.Day(offset)</span><br><span class="line">interested_data=data[data[<span class="hljs-string">&quot;date&quot;</span>]==interested_date]</span><br></pre></td></tr></tbody></table></figure>


<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">plot_deviance_residuals(interested_data,event,glm_res)</span><br></pre></td></tr></tbody></table></figure>

<img src="http://yumeng-li.github.io/projectcovidGLM_files/projectcovidGLM_37_0.png">



<p>The deviance residuals are defined as the signed square roots of the unit deviances, representing the contributions of individual samples to the deviance. The deviance indicates the extent to which the likelihood of the saturated model exceeds the likelihood of the proposed model. If the proposed model has a good fit, the deviance will be small. If the proposed model has a bad fit, the deviance will be high. </p>
<p>Thus, the deviance residuals are analogous to the conventional residuals: when they are squared, we obtain the sum of squares that we use for assessing the fit of the model. However, while the sum of squares is the residual sum of squares for linear models, for GLMs, this is the deviance.</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">deviance=poisson_deviance(Y_train,Y_pred)</span><br><span class="line">sum_of_squared_deviance_residuals = sum(glm_res.resid_deviance**<span class="hljs-number">2</span>)</span><br><span class="line">deviance, sum_of_squared_deviance_residuals</span><br></pre></td></tr></tbody></table></figure>




<pre><code>(95443.38706242564, 95443.38706242583)</code></pre>
<p>In a <em>properly specified model</em>, the deviance is approximately chi-square distributed with n-k-1 degrees of freedom, and the deviance residuals would be independent, standard normal random variables, i.e., ~ norm(0,1). So we would expect the residuals to be mostly in range of  -1 to 1, and rarely fall outside the &#xB1; 3 limits.</p>
<p>However, the model residuals are 10 times larger than expected and we say there is <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Overdispersion"><strong>overdispersion</strong></a>. If overdispersion is present in a dataset, the estimated standard errors and test statistics, the overall goodness-of-fit will be distorted and adjustments must be made. </p>
<p>A more appropriate model will be a Negative Binomial regression.</p>
<h2 id="Model-Prediction"><a href="#Model-Prediction" class="headerlink" title="Model Prediction"></a>Model Prediction</h2><p>We will now use the model <strong>without recalibrating</strong> to make predictions about admission rates on new data.</p>
<p>This is <strong>out of sample</strong> evaluation. It is the only way to make sure the model really works.</p>
<h3 id="Out-of-Sample-R-2"><a href="#Out-of-Sample-R-2" class="headerlink" title="Out of Sample $R^2$"></a>Out of Sample $R^2$</h3><figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">test_admission_mean=Y_test.sum()/P_test.sum()</span><br><span class="line">Y_pred=glm_res.predict(X_test,exposure=P_test)</span><br><span class="line"></span><br><span class="line">null_deviance_test=poisson_deviance(Y_test,P_test*test_admission_mean)</span><br><span class="line">deviance_test=poisson_deviance(Y_test,Y_pred)</span><br><span class="line">R2 = <span class="hljs-number">1</span>-deviance_test/null_deviance_test</span><br><span class="line">R2</span><br></pre></td></tr></tbody></table></figure>




<pre><code>0.371917369051069</code></pre>
<p>Out of sample $R^2$ is worse than for the in sample (approx. 0.473). I think we&#x2019;ll all agree, predicting the future is hard.</p>
<h2 id="Negative-Binomial-Regression"><a href="#Negative-Binomial-Regression" class="headerlink" title="Negative Binomial Regression"></a>Negative Binomial Regression</h2><h3 id="Model-Definition-1"><a href="#Model-Definition-1" class="headerlink" title="Model Definition"></a>Model Definition</h3><p>We have data from multiple dates, so it makes sense to incorporate Time as a extra variable.</p>
<p>\begin{eqnarray}<br>    &amp;\eta_i &amp;=c + \beta_1 x_i  + \beta_2 t_i + \beta_3 t_i^2 \newline<br>      \mathbb{E}(y_i|x_i) =&amp; \lambda_i P_i &amp;= e^{\eta_i } P_i \newline<br>    &amp;p(y_i|x_i) &amp; = \text{NB}(y_i; \lambda_i P_i, \alpha)<br>\end{eqnarray}<br>where </p>
<ul>
<li>Each observation $i$ represents a single state at one particular date.</li>
<li>$y_i$ is the number of hospital admissions on that state.</li>
<li>$x_i$ is the percentage of the state population fully vaccinated that state.</li>
<li>$t_i$ is the number of days elapsed since the beginning of the training period.</li>
<li>$P_i$ is the population of the  state.</li>
<li>the parameters $c$ and $\beta_1,\beta_2,\beta_3$ will be fitted to the observed data to maximize agreement with the model</li>
</ul>
<p>We including coefficients for $t$ and $t^2$ the dependence of the admission rate on time can have some curvature and does not need to be linear.</p>
<p>This can be fitter as a <strong>linear model</strong> where the inputs are $x_i$, $t_i$ and $t_i^2$</p>
<h4 id="Variance-Comparison-to-Poisson-Model"><a href="#Variance-Comparison-to-Poisson-Model" class="headerlink" title="Variance Comparison to Poisson Model"></a>Variance Comparison to Poisson Model</h4><p>Introducing a free additional parameter $\alpha$ give more accurate models than simple parametric models like the Poisson distribution by allowing the mean and variance to be different, unlike the Poisson. The negative binomial distribution has a variance $\hat{y}+\hat{y}^2\alpha$ . This can make the distribution a useful overdispersed alternative to the Poisson distribution.</p>
<p>As we can see, with an extra parameter $\alpha$ to control the variance, the expected variance of observations is larger with the Negative Binomial Model than with the Poisson model.</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">alpha = <span class="hljs-number">0.5</span></span><br><span class="line">y_hat=np.linspace(<span class="hljs-number">0</span>,<span class="hljs-number">10</span>,<span class="hljs-number">201</span>)</span><br><span class="line">plt.plot(y_hat,y_hat,<span class="hljs-string">&quot;k--&quot;</span>,label=<span class="hljs-string">&quot;Poisson&quot;</span>)</span><br><span class="line">plt.plot(y_hat,y_hat+alpha*y_hat**<span class="hljs-number">2</span>,<span class="hljs-string">&quot;k-&quot;</span>,linewidth=<span class="hljs-number">3</span>,label=<span class="hljs-string">&quot;Negative Binomial&quot;</span>)</span><br><span class="line">plt.xlabel(<span class="hljs-string">r&quot;$\hat{y}$&quot;</span>)</span><br><span class="line">plt.ylabel(<span class="hljs-string">r&quot;Var($y|\hat{y}$)&quot;</span>)</span><br><span class="line">plt.legend()</span><br></pre></td></tr></tbody></table></figure>


<img src="http://yumeng-li.github.io/projectcovidGLM_files/projectcovidGLM_49_1.png">


<h4 id="Choosing-alpha"><a href="#Choosing-alpha" class="headerlink" title="Choosing $\alpha$"></a>Choosing $\alpha$</h4><p>Negative Binomial is a GLM model with an extra parameter $\alpha$ that we must choose by maximizing the log likelihood. </p>
<p>The train/test data we fit to the Negative Binomial Regression is the same as what we create at start.</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">lls=[]</span><br><span class="line">alphas=np.linspace(<span class="hljs-number">0.1</span>,<span class="hljs-number">0.5</span>,<span class="hljs-number">500</span>)</span><br><span class="line"><span class="hljs-keyword">for</span> alpha <span class="hljs-keyword">in</span> alphas:</span><br><span class="line">    glm_model=sm.GLM(Y_train,X_train,exposure=P_train,family=sm.families.NegativeBinomial(sm.families.links.log(),alpha=alpha))</span><br><span class="line">    glm_res=glm_model.fit()</span><br><span class="line">    ll=glm_res.llf</span><br><span class="line">    lls.append(ll)</span><br><span class="line">plt.semilogx(alphas,lls) </span><br><span class="line">plt.xlabel(<span class="hljs-string">r&quot;$\alpha$&quot;</span>)</span><br><span class="line">plt.ylabel(<span class="hljs-string">&quot;Log Likelihood&quot;</span>)</span><br><span class="line">alpha=alphas[np.argmax(lls)]</span><br><span class="line">print(<span class="hljs-string">&quot;alpha&quot;</span>,alpha)</span><br></pre></td></tr></tbody></table></figure>

<pre><code>alpha 0.3004008016032064</code></pre>
<img src="http://yumeng-li.github.io/projectcovidGLM_files/projectcovidGLM_52_1.png">


<p>The best alpha is 0.3004 given a run of 500 times. Or easier, we can go for the  <strong>Negative Regression</strong> model from <a target="_blank" rel="noopener" href="https://www.statsmodels.org/stable/generated/statsmodels.discrete.discrete_model.Poisson.html">statsmodels</a>, which can do the dirty work, optimizing alpha for us.</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-comment">#mod=sm.GLM(Y_train,X_train,exposure=P_train,family=nb)</span></span><br><span class="line">mod=NegativeBinomial(Y_train,X_train,exposure=P_train)</span><br><span class="line">res=mod.fit(method=<span class="hljs-string">&quot;lbfgs&quot;</span>)</span><br><span class="line">res.summary()</span><br></pre></td></tr></tbody></table></figure>

<pre><code>/opt/anaconda3/lib/python3.7/site-packages/statsmodels/discrete/discrete_model.py:2642: RuntimeWarning: divide by zero encountered in log
  llf = coeff + size*np.log(prob) + endog*np.log(1-prob)
/opt/anaconda3/lib/python3.7/site-packages/statsmodels/discrete/discrete_model.py:2642: RuntimeWarning: invalid value encountered in multiply
  llf = coeff + size*np.log(prob) + endog*np.log(1-prob)
/opt/anaconda3/lib/python3.7/site-packages/statsmodels/base/model.py:568: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals
  ConvergenceWarning)</code></pre>
<table class="simpletable">
<caption>NegativeBinomial Regression Results</caption>
<tbody><tr>
  <th>Dep. Variable:</th>      <td>admissions</td>    <th>  No. Observations:  </th>  <td>  1632</td> 
</tr>
<tr>
  <th>Model:</th>           <td>NegativeBinomial</td> <th>  Df Residuals:      </th>  <td>  1628</td> 
</tr>
<tr>
  <th>Method:</th>                 <td>MLE</td>       <th>  Df Model:          </th>  <td>     3</td> 
</tr>
<tr>
  <th>Date:</th>            <td>Sun, 26 Sep 2021</td> <th>  Pseudo R-squ.:     </th>  <td>0.08843</td>
</tr>
<tr>
  <th>Time:</th>                <td>20:24:21</td>     <th>  Log-Likelihood:    </th> <td> -7946.2</td>
</tr>
<tr>
  <th>converged:</th>             <td>False</td>      <th>  LL-Null:           </th> <td> -8717.1</td>
</tr>
<tr>
  <th>Covariance Type:</th>     <td>nonrobust</td>    <th>  LLR p-value:       </th>  <td> 0.000</td> 
</tr>
</tbody></table>
<table class="simpletable">
<tbody><tr>
       <td></td>         <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P&gt;|z|</th>  <th>[0.025</th>    <th>0.975]</th>  
</tr>
<tr>
  <th>vaccinated</th> <td>   -6.7968</td> <td>    0.179</td> <td>  -38.019</td> <td> 0.000</td> <td>   -7.147</td> <td>   -6.446</td>
</tr>
<tr>
  <th>T</th>          <td>    0.0525</td> <td>    0.006</td> <td>    8.645</td> <td> 0.000</td> <td>    0.041</td> <td>    0.064</td>
</tr>
<tr>
  <th>T2</th>         <td>   -0.0002</td> <td>    0.000</td> <td>   -1.262</td> <td> 0.207</td> <td>   -0.001</td> <td>    0.000</td>
</tr>
<tr>
  <th>const</th>      <td>   -8.4877</td> <td>    0.092</td> <td>  -92.109</td> <td> 0.000</td> <td>   -8.668</td> <td>   -8.307</td>
</tr>
<tr>
  <th>alpha</th>      <td>    0.3025</td> <td>    0.011</td> <td>   26.621</td> <td> 0.000</td> <td>    0.280</td> <td>    0.325</td>
</tr>
</tbody></table>



<p>The alpha values are close,</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">p=res.params</span><br><span class="line">nb=sm.families.NegativeBinomial(alpha=p[<span class="hljs-string">&quot;alpha&quot;</span>])</span><br><span class="line">nb.alpha</span><br></pre></td></tr></tbody></table></figure>




<pre><code>0.3024936677385029</code></pre>
<p><strong>Compared to Poisson:</strong><br><strong>- The regression coefficients are very similar.</strong><br><strong>- The confidence intervals are much wider.</strong><br><strong>- The t statistics are smaller because we assume a larger variance of residuals.</strong></p>
<p><strong>In summary, switching from Poisson to Negative Binominal yields stable coefficient estimates, but a higher chance for the null hepothesis to be rejected and more reliable confidence intervals.</strong></p>
<p>The <code>statsmodel.family.NegativeBinomial</code> object knows how to compute deviance, and we use it to calculate the R squared,</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">mu_train=Y_train.sum()/P_train.sum()</span><br><span class="line">Y_pred=res.predict(X_train,exposure=P_train)</span><br><span class="line">deviance=nb.deviance(Y_train,Y_pred)</span><br><span class="line">null_deviance=nb.deviance(Y_train,mu_train*P_train)</span><br><span class="line">R2=<span class="hljs-number">1</span>-deviance/null_deviance</span><br><span class="line">R2</span><br></pre></td></tr></tbody></table></figure>




<pre><code>0.5937123540474427</code></pre>
<h3 id="In-sample-model-Fit-1"><a href="#In-sample-model-Fit-1" class="headerlink" title="In-sample model Fit"></a>In-sample model Fit</h3><figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">covid.plot_time_facets(train_data,res,event,date0)</span><br></pre></td></tr></tbody></table></figure>

<img src="http://yumeng-li.github.io/projectcovidGLM_files/projectcovidGLM_61_0.png">


<p>Then most excitingly, let&#x2019;s check out the deviance residuals again. The residuals are now well behaved, randomly fall into the range of -1 to 1. Though FL, KY and a few other states seem to be outliers. But no worries, we will address them in our next blog, using a more advanced method - mixed models.</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">plot_deviance_residuals(interested_data,event,glm_res)</span><br></pre></td></tr></tbody></table></figure>

<img src="http://yumeng-li.github.io/projectcovidGLM_files/projectcovidGLM_63_0.png">


<h2 id="Model-Prediction-1"><a href="#Model-Prediction-1" class="headerlink" title="Model Prediction"></a>Model Prediction</h2><p>Finally, always take a look at the out-of-sample fit. </p>
<p>Though the R squared is not as good as the in-sample, but there&#x2019;s still an improvement comparing to Poisson regression and the linear regressions.</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">test_admission_mean=Y_test.sum()/P_test.sum()</span><br><span class="line">Y_pred=res.predict(X_test,exposure=P_test)</span><br><span class="line"></span><br><span class="line">null_deviance_test=nb.deviance(Y_test,P_test*test_admission_mean)</span><br><span class="line">deviance_test=nb.deviance(Y_test,Y_pred)</span><br><span class="line">R2 = <span class="hljs-number">1</span>-deviance_test/null_deviance_test</span><br><span class="line">R2</span><br></pre></td></tr></tbody></table></figure>




<pre><code>0.4595499732848517</code></pre>
</body></html>
    
    </div>
    
    
</article>




    
        <article class="article content gallery" itemscope itemprop="blogPost">
    <h1 class="article-title is-size-3 is-size-4-mobile" itemprop="name">
        
            <a href="/2021/09/21/projectcovid/" itemprop="url">Project Covid Admissions Based on Vaccine Progress, Part 1 -- From Simple Linear Regression to Weighted Time Series Regression</a>
        
    </h1>
    <div class="article-meta columns is-variable is-1 is-multiline is-mobile is-size-7-mobile">
        <span class="column is-narrow">
            <time datetime="2021-09-21T16:00:00.000Z" itemprop="datePublished">Sep 21 2021</time>
        </span>
        
        <span class="column is-narrow article-category">
            <i class="far fa-folder"></i>
            <a class="article-category-link" href="/categories/Applied-Statistics/">Applied Statistics</a>
        </span>
        
        
        <span class="column is-narrow">
            
            
            16 minutes read (About 2473 words)
        </span>
        
    </div>
    <div class="article-entry is-size-6-mobile" itemprop="articleBody">
    
        <html><head></head><body><p>Lately I&#x2019;ve been working on a project predicting credit losses for mortgages with a bunch of loan-level attributes. To be honest, the data wasn&#x2019;t rich. In pursuit of good predictive power, I had to try from the simple multi-period regression, to generalized linear model with distributions beyond the Gaussian family, then mixed effect models, and even used some curve fitting techniques like splines at last to fix the oversimple (or wrong) assumptions I made at the beginning. Overall, I think the journey was inspiring as it basically shows that one can almost crack any problem with regression analysis, more importantly, in an elegant way.</p>
<p>When it comes to predictive modeling, people always turn to linear regressions, which isn&#x2019;t wrong, just so you know that linear regression models and OLS make a number of assumptions about the predictor variables, the response variable, and their relationship. Violating these assumptions can result in biased predictions or imprecise coefficients, in short cause your model to be less predictive. Well, the good news is that there are numerous extensions have been developed that allow each of these assumptions to be relaxed, and in some cases eliminated entirely, but only if you know what they are and when to use them. More commonly, I see people get stuck and choose to live with a just-fine-fit model without knowing that they can easily fix it by employing a more generalized regression models. Now let&#x2019;s see how powerful regressions can get through a trendy modeling case - project covid admissions based on the vaccinate progress.</p>
<h2 id="Preliminaries"><a href="#Preliminaries" class="headerlink" title="Preliminaries"></a>Preliminaries</h2><figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np</span><br><span class="line"><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd</span><br><span class="line"><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt</span><br><span class="line"><span class="hljs-keyword">import</span> matplotlib.colors</span><br><span class="line"><span class="hljs-keyword">import</span> seaborn <span class="hljs-keyword">as</span> sns</span><br><span class="line"><span class="hljs-keyword">import</span> statsmodels.api <span class="hljs-keyword">as</span> sm</span><br><span class="line"><span class="hljs-keyword">import</span> sys</span><br><span class="line"><span class="hljs-keyword">import</span> covid_analysis <span class="hljs-keyword">as</span> covid <span class="hljs-comment"># some functions developed for this blog</span></span><br></pre></td></tr></tbody></table></figure>

<p>The data I use includes hospital admissions and vaccinations from the CDC website, and the most up-to-date populations by state from the US Census Bureau.</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data_dir=<span class="hljs-string">&quot;../data&quot;</span></span><br></pre></td></tr></tbody></table></figure>


<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">hospitalizations=pd.read_csv(<span class="hljs-string">f&quot;<span class="hljs-subst">{data_dir}</span>/covid_hospitalizations.csv&quot;</span>,parse_dates=[<span class="hljs-string">&quot;date&quot;</span>])</span><br><span class="line">vaccinations=pd.read_csv(<span class="hljs-string">f&quot;<span class="hljs-subst">{data_dir}</span>/covid_vaccinations.csv&quot;</span>,parse_dates=[<span class="hljs-string">&quot;date&quot;</span>])</span><br><span class="line">population=pd.read_csv(<span class="hljs-string">f&quot;<span class="hljs-subst">{data_dir}</span>/population.csv&quot;</span>)</span><br><span class="line"></span><br><span class="line">event=<span class="hljs-string">&quot;admissions&quot;</span></span><br><span class="line">data=hospitalizations</span><br><span class="line"></span><br><span class="line">data=data.merge(vaccinations,on=[<span class="hljs-string">&quot;date&quot;</span>,<span class="hljs-string">&quot;state&quot;</span>])</span><br><span class="line">data=data.merge(population,on=<span class="hljs-string">&quot;state&quot;</span>)</span><br><span class="line">data[<span class="hljs-string">&quot;vaccinated&quot;</span>]=data[<span class="hljs-string">&quot;vaccinated&quot;</span>]/data[<span class="hljs-string">&quot;population&quot;</span>]</span><br><span class="line">data.head()</span><br></pre></td></tr></tbody></table></figure>


<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;</code></pre>
<p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>date</th>
      <th>state</th>
      <th>used_beds</th>
      <th>admissions</th>
      <th>vaccinated</th>
      <th>population</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>2021-06-10</td>
      <td>MT</td>
      <td>66.0</td>
      <td>16.0</td>
      <td>0.397597</td>
      <td>1080577</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2021-06-21</td>
      <td>MT</td>
      <td>57.0</td>
      <td>13.0</td>
      <td>0.411614</td>
      <td>1080577</td>
    </tr>
    <tr>
      <th>2</th>
      <td>2021-08-05</td>
      <td>MT</td>
      <td>149.0</td>
      <td>23.0</td>
      <td>0.440153</td>
      <td>1080577</td>
    </tr>
    <tr>
      <th>3</th>
      <td>2021-07-15</td>
      <td>MT</td>
      <td>62.0</td>
      <td>9.0</td>
      <td>0.431722</td>
      <td>1080577</td>
    </tr>
    <tr>
      <th>4</th>
      <td>2021-08-21</td>
      <td>MT</td>
      <td>224.0</td>
      <td>59.0</td>
      <td>0.449196</td>
      <td>1080577</td>
    </tr>
  </tbody>
</table>
</div>



<br>
Now we are good to go. As always, let&#x2019;s first check on our old friend, simple linear regression with one single explanatory variable, as a warm-up. 

<p>Here specifically, I am using weighted least squares rather than ordinary least squares to incorporate the knowledge of state populations. WLS is a generalization of OLS and a specialization of generalized least squares, which can relax the assumptions of constant variance, allowing the variances of the observations to be unequal i.e., heteroscedasticity (btw one of my favorite words in statistics, the pronunciation&#x2019;s fun).</p>
<h2 id="Single-Period-Regression"><a href="#Single-Period-Regression" class="headerlink" title="Single Period Regression"></a>Single Period Regression</h2><h3 id="Model-Definition"><a href="#Model-Definition" class="headerlink" title="Model Definition"></a>Model Definition</h3><p>The weighted linear regression model is defined as:<br>\begin{eqnarray}<br>    \hat{y}_i &amp;= c + \beta x_i \newline<br>    y_i &amp;=\hat{y}_i+ \epsilon_i \newline<br>\end{eqnarray}<br>and we minimize the <strong>weighted least squares error</strong> :<br>\begin{equation}<br>    E_w = \sum_i P_i(y - \hat{y}_i)^2<br>\end{equation}<br>where </p>
<ul>
<li>Each observation $i$ represents a single state.</li>
<li>$\hat{y}_i$ is the predicted probability of hospital admissions <em>per  persons</em> on that state.</li>
<li>$y_i$ is the actual number of events per person observed.</li>
<li>$x_i$ is the percentage of the state population fully vaccinated.</li>
<li>$P_i$ is the population of the  state.</li>
<li>$\epsilon_i$ is a Gaussian uncorrelated noise with constant variance $\sigma$ </li>
<li>the parameters $c$ and $\beta$ will be fitted to the observed data to maximize agreement with the model</li>
</ul>
<p>We first fit the regression model to <strong>single  day</strong> of state hospitalization data.</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">offset=<span class="hljs-number">9</span> <span class="hljs-comment"># if we do not want to look at the last day, subtract here.</span></span><br></pre></td></tr></tbody></table></figure>

<p>Let&#x2019;s take Aug.15,</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">last_date=data[<span class="hljs-string">&quot;date&quot;</span>].max()-pd.offsets.Day(offset)</span><br><span class="line">last_data=data[data[<span class="hljs-string">&quot;date&quot;</span>]==last_date]</span><br></pre></td></tr></tbody></table></figure>


<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">X=sm.add_constant(last_data[<span class="hljs-string">&quot;vaccinated&quot;</span>])</span><br><span class="line">P=last_data[<span class="hljs-string">&quot;population&quot;</span>]</span><br><span class="line">Y=last_data[event]/last_data[<span class="hljs-string">&quot;population&quot;</span>]</span><br></pre></td></tr></tbody></table></figure>

<h3 id="Use-statmodels-for-Weighted-Linear-Regression"><a href="#Use-statmodels-for-Weighted-Linear-Regression" class="headerlink" title="Use statmodels for Weighted Linear Regression"></a>Use statmodels for Weighted Linear Regression</h3><p>Let&#x2019;s first use the  <strong>Weighted Linear Regression</strong> model from <a target="_blank" rel="noopener" href="https://www.statsmodels.org/stable/generated/statsmodels.discrete.discrete_model.Poisson.html">statmodels</a>.<br>The exact same estimation can be done by a different way, which I will show later.</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">mod=sm.WLS(Y,X,P)</span><br><span class="line">res=mod.fit()</span><br><span class="line">res.params</span><br></pre></td></tr></tbody></table></figure>




<pre><code>const         0.000127
vaccinated   -0.000190
dtype: float64</code></pre>
<p>So here&#x2019;s our estimation of the coefficients and some statistics that are helpful for diagnosis. It&#x2019;s easy to read that the R-squared is 0.327. But we can also calculate it ourselves.</p>
<p>The R-square is defined as ratio of square loss to target variance,</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">square_error</span>(<span class="hljs-params">y,y_hat,P</span>):</span></span><br><span class="line">    err=y-y_hat</span><br><span class="line">    <span class="hljs-keyword">return</span> np.sum(P*err**<span class="hljs-number">2</span>)</span><br></pre></td></tr></tbody></table></figure>


<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Y_pred=res.predict(X)</span><br><span class="line">Y_bar=(Y*P).sum()/P.sum()</span><br><span class="line">R2=<span class="hljs-number">1</span> - square_error(Y,Y_pred,P)/square_error(Y,Y_bar,P)</span><br><span class="line">R2</span><br></pre></td></tr></tbody></table></figure>




<pre><code>0.3266337795171833</code></pre>
<h3 id="Use-statmodels-GLM-for-Weighted-Linear-Regression"><a href="#Use-statmodels-GLM-for-Weighted-Linear-Regression" class="headerlink" title="Use statmodels GLM for Weighted Linear Regression"></a>Use statmodels GLM for Weighted Linear Regression</h3><p>Exactly the same model can be written in the more general language of <strong>Generalized Linear models</strong> as</p>
<h3 id="Model-Definition-1"><a href="#Model-Definition-1" class="headerlink" title="Model Definition"></a>Model Definition</h3><p>The linear regression  model is defined as:<br>\begin{eqnarray}<br>    &amp;\eta_i &amp;=c + \beta*x_i \newline<br>    \mathbb{E}(y_i|x_i) =&amp; \hat{y}_i &amp;= \eta_i  \newline<br>    &amp;p(y_i|x_i) &amp; = N(\hat{y}_i,\sigma)<br>\end{eqnarray}</p>
<p>The fact that $\eta_i=\hat{y}_i$ means we are using the <strong>identity</strong> link between the Gaussian family and the linear model $\eta$. One should be aware here that linear model is just a specical case of GLM.</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">glm_model=sm.GLM(Y,X,family=sm.families.Gaussian(sm.families.links.identity()),var_weights=P)</span><br><span class="line">glm_res=glm_model.fit()</span><br><span class="line">glm_res.params</span><br></pre></td></tr></tbody></table></figure>




<pre><code>const         0.000127
vaccinated   -0.000190
dtype: float64</code></pre>
<p>Results <strong>are the same</strong>. But with the <code>GLM</code> language we will be able to perform regression many more kinds of variables. </p>
<p>The generalization of square error for a GLM model is called the  <strong>deviance</strong>. This is  the <em>square error</em> the Gaussian Family used  in linear regression. Let&#x2019;s calculate the in-sample R-squared in the GLM way and compare it with the previous.</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">glm_res.deviance, square_error(Y,Y_pred,P)</span><br></pre></td></tr></tbody></table></figure>




<pre><code>(0.12246143037040369, 0.12246143037040369)</code></pre>
<p>The resuls also include the deviance for the <strong>null model</strong> fitted to just a constant</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">glm_res.null_deviance,square_error(Y,Y_bar,P)</span><br></pre></td></tr></tbody></table></figure>




<pre><code>(0.1818645287591, 0.1818645287591)</code></pre>
<p>The ratio defines an $R^2$ value. Not surprisingly, the results are exactly the same. Overall, I admit a 32.7% R-squared model can barely be said to be a good model. But no worries, all can be fixed.</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-number">1</span>-glm_res.deviance/glm_res.null_deviance</span><br></pre></td></tr></tbody></table></figure>




<pre><code>0.3266337795171833</code></pre>
<p>Now, as we have a better understanding of the data and the modeling methodologies, let&#x2019;s move on to model the time series. </p>
<p>The data we have here is time series data, indeed panel data, as time series and cross-sectional data are both special cases of panel data that are in one dimension only. But we will deal with the multi-subjects matter in the next blog(probably should have a spoiler tag oops). Anyways, let&#x2019;s first start with the multi-period regression, here I have trend variables added to the model to represent time.</p>
<h2 id="Multiple-Period-Regression"><a href="#Multiple-Period-Regression" class="headerlink" title="Multiple Period Regression"></a>Multiple Period Regression</h2><h3 id="Model-Definition-2"><a href="#Model-Definition-2" class="headerlink" title="Model Definition"></a>Model Definition</h3><p>\begin{eqnarray}<br>    &amp;\eta_i &amp;=c + \beta_1 x_i  + \beta_2 t_i + \beta_3 t_i^2 \newline<br>    \mathbb{E}(y_i|x_i) =&amp; \hat{y}_i &amp;= \eta_i  \newline<br>    &amp;p(y_i|x_i) &amp; = N(\hat{y_i},\sigma^2)<br>\end{eqnarray}<br>where </p>
<ul>
<li>Each observation $i$ represents a single state at one particular date.</li>
<li>$y_i$ is the number of events on that state.</li>
<li>$\hat{y}_i$ is the <em>predicted</em> number of events on that state.</li>
<li>$x_i$ is the percentage of the state population fully vaccinated on that state.</li>
<li>$t_i$ is the number of days elapsed since the beginning of the training period.</li>
<li>$\sigma^2$ is the level of <em>noise</em> of the observations (the variance of the residuals).</li>
<li>the parameters $c$ and $\beta_1,\beta_2,\beta_3$ will be fitted to the observed data to maximize agreement with the model</li>
</ul>
<p>This can be fitter as a <strong>linear model</strong> where the inputs are $x_i$, $t_i$ and $t_i^2$</p>
<p>First we select the training period:</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">start_train=<span class="hljs-string">&quot;2021-07-15&quot;</span></span><br><span class="line">end_train=<span class="hljs-string">&quot;2021-08-15&quot;</span></span><br><span class="line">train_data=data[(data[<span class="hljs-string">&apos;date&apos;</span>] &gt;= start_train) &amp; (data[<span class="hljs-string">&apos;date&apos;</span>] &lt;= end_train)].copy()</span><br><span class="line">date0=train_data[<span class="hljs-string">&quot;date&quot;</span>].min()</span><br></pre></td></tr></tbody></table></figure>

<p>Then a 7-day testing period:</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">test_period=<span class="hljs-number">7</span> <span class="hljs-comment"># days</span></span><br><span class="line">test_end=pd.Timestamp(end_train)+pd.DateOffset(days=test_period)</span><br><span class="line">test_data=data[(data[<span class="hljs-string">&quot;date&quot;</span>]&gt;end_train) &amp; (data[<span class="hljs-string">&quot;date&quot;</span>]&lt;=test_end)].copy()</span><br></pre></td></tr></tbody></table></figure>

<p>I write a function to generate the <strong>desired matrix</strong>,</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">define_variables</span>(<span class="hljs-params">data,date0</span>):</span></span><br><span class="line">        N=len(data)</span><br><span class="line">        T=(data[<span class="hljs-string">&quot;date&quot;</span>]-date0).dt.days</span><br><span class="line">        X=data[<span class="hljs-string">&quot;vaccinated&quot;</span>].copy().to_frame()</span><br><span class="line">        X[<span class="hljs-string">&quot;T&quot;</span>]=T</span><br><span class="line">        X[<span class="hljs-string">&quot;T2&quot;</span>]=T**<span class="hljs-number">2</span></span><br><span class="line">        P=data[<span class="hljs-string">&quot;population&quot;</span>]</span><br><span class="line">        X[<span class="hljs-string">&quot;const&quot;</span>]=np.ones(N)</span><br><span class="line">        <span class="hljs-keyword">return</span> X,P</span><br></pre></td></tr></tbody></table></figure>


<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">X_train,P_train=covid.define_variables(train_data,date0)</span><br><span class="line">Y_train=train_data[event]/train_data[<span class="hljs-string">&quot;population&quot;</span>]</span><br><span class="line">X_test,P_test=covid.define_variables(test_data,date0)</span><br><span class="line">Y_test=test_data[event]/test_data[<span class="hljs-string">&quot;population&quot;</span>]</span><br></pre></td></tr></tbody></table></figure>

<h3 id="Weighted-Multi-period-Linear-Regression"><a href="#Weighted-Multi-period-Linear-Regression" class="headerlink" title="Weighted Multi-period Linear Regression"></a>Weighted Multi-period Linear Regression</h3><figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">mod=sm.WLS(Y_train,X_train,P_train)</span><br><span class="line">res=mod.fit()</span><br><span class="line">res.summary()</span><br></pre></td></tr></tbody></table></figure>




<table class="simpletable">
<caption>WLS Regression Results</caption>
<tbody><tr>
  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared:         </th>  <td>   0.373</td> 
</tr>
<tr>
  <th>Model:</th>                   <td>WLS</td>       <th>  Adj. R-squared:    </th>  <td>   0.371</td> 
</tr>
<tr>
  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th>  <td>   322.3</td> 
</tr>
<tr>
  <th>Date:</th>             <td>Tue, 21 Sep 2021</td> <th>  Prob (F-statistic):</th>  <td>3.12e-164</td>
</tr>
<tr>
  <th>Time:</th>                 <td>15:16:26</td>     <th>  Log-Likelihood:    </th>  <td>  15151.</td> 
</tr>
<tr>
  <th>No. Observations:</th>      <td>  1632</td>      <th>  AIC:               </th> <td>-3.029e+04</td>
</tr>
<tr>
  <th>Df Residuals:</th>          <td>  1628</td>      <th>  BIC:               </th> <td>-3.027e+04</td>
</tr>
<tr>
  <th>Df Model:</th>              <td>     3</td>      <th>                     </th>      <td> </td>    
</tr>
<tr>
  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>      <td> </td>    
</tr>
</tbody></table>
<table class="simpletable">
<tbody><tr>
       <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P&gt;|t|</th>  <th>[0.025</th>    <th>0.975]</th>  
</tr>
<tr>
  <th>vaccinated</th> <td>   -0.0002</td> <td> 6.13e-06</td> <td>  -24.647</td> <td> 0.000</td> <td>   -0.000</td> <td>   -0.000</td>
</tr>
<tr>
  <th>T</th>          <td> 1.038e-06</td> <td> 1.82e-07</td> <td>    5.686</td> <td> 0.000</td> <td>  6.8e-07</td> <td>  1.4e-06</td>
</tr>
<tr>
  <th>T2</th>         <td> -1.18e-09</td> <td> 5.69e-09</td> <td>   -0.207</td> <td> 0.836</td> <td>-1.23e-08</td> <td> 9.98e-09</td>
</tr>
<tr>
  <th>const</th>      <td>   8.1e-05</td> <td> 3.17e-06</td> <td>   25.544</td> <td> 0.000</td> <td> 7.48e-05</td> <td> 8.72e-05</td>
</tr>
</tbody></table>
<table class="simpletable">
<tbody><tr>
  <th>Omnibus:</th>       <td>1418.486</td> <th>  Durbin-Watson:     </th> <td>   0.372</td> 
</tr>
<tr>
  <th>Prob(Omnibus):</th>  <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>43359.192</td>
</tr>
<tr>
  <th>Skew:</th>           <td> 4.022</td>  <th>  Prob(JB):          </th> <td>    0.00</td> 
</tr>
<tr>
  <th>Kurtosis:</th>       <td>26.936</td>  <th>  Cond. No.          </th> <td>6.93e+03</td> 
</tr>
</tbody></table><br><br>Notes:<br>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br>[2] The condition number is large, 6.93e+03. This might indicate that there are<br>strong multicollinearity or other numerical problems.

<br>
Calculate the R-squared again, still, not good enough.So let&apos;s take a deep dive and see where can be improved.


<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">Y_bar=np.sum(Y_train*P_train)/np.sum(P_train) <span class="hljs-comment"># weighted average</span></span><br><span class="line">Y_pred=res.predict(X_train)</span><br><span class="line"></span><br><span class="line">deviance=square_error(Y_train,Y_pred,P_train)</span><br><span class="line">null_deviance=square_error(Y_train,Y_bar,P_train)</span><br><span class="line">R2=<span class="hljs-number">1</span>-deviance/null_deviance</span><br><span class="line">R2</span><br></pre></td></tr></tbody></table></figure>




<pre><code>0.3726019173009557</code></pre>
<h3 id="Extrapolation-of-Time-trends"><a href="#Extrapolation-of-Time-trends" class="headerlink" title="Extrapolation of Time trends"></a>Extrapolation of Time trends</h3><p>The linear model projects the admission to be increasing steadily if assuming current vaccination rate.</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">covid.plot_time_extrapolation(train_data,res,event,date0)</span><br></pre></td></tr></tbody></table></figure>
<img src="http://yumeng-li.github.io/projectcovid_files/projectcovid_43_0.png">


<h3 id="In-sample-model-Fit"><a href="#In-sample-model-Fit" class="headerlink" title="In-sample model Fit"></a>In-sample model Fit</h3><p>To better evaluate the model fit, we visualize the real observations and model prodictions. Here&#x2019;s some <strong>snapshots</strong> of the fit for a few selected dates on the training period. </p>
<p>Observations are arranged by state and date.</p>
<p>Don&#x2019;t have to say much, you can tell the weighted linear model doesn&#x2019;t fit well as the corelation between vaccinations and admissions looks more like a curve than a straight line.</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">covid.plot_time_facets(train_data,res,event,date0)</span><br></pre></td></tr></tbody></table></figure>

<img src="http://yumeng-li.github.io/projectcovid_files/projectcovid_45_0.png">


<p>We can further plot out the residuals. I choose one date from the dates above, Aug.15 and apply the following function to generate its residual plot.</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">plot_residuals</span>(<span class="hljs-params">data,event,res</span>):</span></span><br><span class="line">    P=data[<span class="hljs-string">&quot;population&quot;</span>]</span><br><span class="line">    <span class="hljs-comment"># Create two subplots and unpack the output array immediately</span></span><br><span class="line">    f, (ax1, ax2) = plt.subplots(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>,sharex=<span class="hljs-literal">True</span>,figsize=(<span class="hljs-number">16</span>,<span class="hljs-number">4</span>))</span><br><span class="line">    norm=matplotlib.colors.LogNorm(vmin=<span class="hljs-number">100</span>_000, vmax=P.max(), clip=<span class="hljs-literal">False</span>)</span><br><span class="line">    m=ax1.scatter(data[<span class="hljs-string">&quot;vaccinated&quot;</span>],data[event]/data[<span class="hljs-string">&quot;population&quot;</span>]*<span class="hljs-number">100</span>_000,</span><br><span class="line">                c=P,norm=norm,cmap=<span class="hljs-string">&quot;Blues&quot;</span>,label=event)</span><br><span class="line">    plt.colorbar(m,label=<span class="hljs-string">&quot;State Population&quot;</span>)</span><br><span class="line">    x=np.linspace(<span class="hljs-number">0.3</span>,<span class="hljs-number">0.75</span>,<span class="hljs-number">201</span>)</span><br><span class="line">    x= pd.DataFrame({<span class="hljs-string">&apos;vaccinated&apos;</span>:x, <span class="hljs-string">&apos;T&apos;</span>:<span class="hljs-number">31</span>, <span class="hljs-string">&apos;T2&apos;</span>:<span class="hljs-number">31</span>*<span class="hljs-number">31</span>})</span><br><span class="line">    x[<span class="hljs-string">&quot;const&quot;</span>]=np.ones(<span class="hljs-number">201</span>)</span><br><span class="line">    y_pred=res.predict(x)</span><br><span class="line">    ax1.plot(x.vaccinated, y_pred*<span class="hljs-number">100</span>_000,<span class="hljs-string">&quot;k--&quot;</span>,label=<span class="hljs-string">&quot;predicted&quot;</span>) </span><br><span class="line">    </span><br><span class="line">    idx = data.index</span><br><span class="line">    ax2.scatter(data[<span class="hljs-string">&quot;vaccinated&quot;</span>], res.resid[idx],</span><br><span class="line">                c=P,norm=norm,cmap=<span class="hljs-string">&quot;Blues&quot;</span>,label=event)</span><br><span class="line">    ax2.set_ybound(lower=<span class="hljs-number">-0.00008</span>,upper=<span class="hljs-number">0.00008</span>)</span><br><span class="line">    ax2.set_title(<span class="hljs-string">&quot;Residuals&quot;</span>)</span><br><span class="line">    ax2.set_xlabel(<span class="hljs-string">&quot;vaccination&quot;</span>)</span><br><span class="line">    ax2.set_ylabel(<span class="hljs-string">&quot;residual&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="hljs-comment"># select outlier state</span></span><br><span class="line">    outlier_idx=np.argpartition(np.array(res.resid[idx]), <span class="hljs-number">-2</span>)[<span class="hljs-number">-2</span>:]</span><br><span class="line">    </span><br><span class="line">    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> outlier_idx:</span><br><span class="line">        outlier=data.iloc[i]</span><br><span class="line">        ax1.annotate(outlier[<span class="hljs-string">&quot;state&quot;</span>],(outlier[<span class="hljs-string">&quot;vaccinated&quot;</span>]+<span class="hljs-number">0.01</span>,outlier[event]/outlier[<span class="hljs-string">&quot;population&quot;</span>]*<span class="hljs-number">100</span>_000))</span><br><span class="line">        ax2.annotate(outlier[<span class="hljs-string">&quot;state&quot;</span>],(outlier[<span class="hljs-string">&quot;vaccinated&quot;</span>]+<span class="hljs-number">0.01</span>,res.resid[idx].iloc[i]))</span><br></pre></td></tr></tbody></table></figure>

<p>We can see that </p>
<ul>
<li>Predicted events can be negative for large vaccination rates, saying we are <strong>over estimating</strong> how fast the admission rate is increasing.</li>
<li>residual variance is negatively correlated with vaccinations rate i.e., smaller vaccination rates have larger magnitude residuals. A <strong>Poisson model</strong> can fix those issues, which will be the topic of our next blog.</li>
<li>Some states like FL and KY look like outliers, indicating that they may have different intercepts than the others, which is not surprising as states are different subjects and each may have unique fundamentals. A <strong>mixed model</strong> would be a good solution for this. </li>
</ul>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">plot_residuals(last_data,event,res)</span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure>

<img src="http://yumeng-li.github.io/projectcovid_files/projectcovid_49_0.png">


<h2 id="Model-Prediction"><a href="#Model-Prediction" class="headerlink" title="Model Prediction"></a>Model Prediction</h2><p>We will now use the model <strong>without recalibrating</strong> to make predictions about admission rates on new data.</p>
<p>This is <strong>out of sample</strong> evaluation. It is the only way to make sure the model really works.</p>
<h3 id="Out-of-Sample-Model-Fit"><a href="#Out-of-Sample-Model-Fit" class="headerlink" title="Out of Sample Model Fit"></a>Out of Sample Model Fit</h3><p>Besides greater residuals, the out of sample evaluation is basically telling us a same story. </p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">covid.plot_time_facets(test_data,res,event,date0)</span><br></pre></td></tr></tbody></table></figure>

<img src="http://yumeng-li.github.io/projectcovid_files/projectcovid_53_0.png">


<h2 id="Day-of-the-Week-Effects"><a href="#Day-of-the-Week-Effects" class="headerlink" title="Day of the Week Effects"></a>Day of the Week Effects</h2><p>Before we move on to GLM, there&#x2019;s another trick to play with. The data reporting obviously has day of the week effects,</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">aggregate=data.groupby([<span class="hljs-string">&quot;date&quot;</span>])[event].sum()</span><br><span class="line">aggregate=aggregate.to_frame().reset_index()</span><br><span class="line">aggregate.plot(x=<span class="hljs-string">&quot;date&quot;</span>,y=event)</span><br></pre></td></tr></tbody></table></figure>

<img src="http://yumeng-li.github.io/projectcovid_files/projectcovid_55_1.png">


<p>We use a new function to generate the train data, more dummy variables are added to represent days of a week,</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">define_variables2</span>(<span class="hljs-params">data,date0</span>):</span></span><br><span class="line">        N=len(data)</span><br><span class="line">        T=(data[<span class="hljs-string">&quot;date&quot;</span>]-date0).dt.days</span><br><span class="line">        X=data[<span class="hljs-string">&quot;vaccinated&quot;</span>].copy().to_frame()</span><br><span class="line">        X[<span class="hljs-string">&quot;T&quot;</span>]=T</span><br><span class="line">        X[<span class="hljs-string">&quot;T2&quot;</span>]=T**<span class="hljs-number">2</span></span><br><span class="line">        X[<span class="hljs-string">&quot;const&quot;</span>]=np.ones(N)</span><br><span class="line">        Z=pd.get_dummies(data[<span class="hljs-string">&quot;date&quot;</span>].dt.day_name(),drop_first=<span class="hljs-literal">True</span>)</span><br><span class="line">        X=pd.concat([X,Z],axis=<span class="hljs-number">1</span>)</span><br><span class="line">        P=data[<span class="hljs-string">&quot;population&quot;</span>]</span><br><span class="line">        <span class="hljs-keyword">return</span> X,P</span><br></pre></td></tr></tbody></table></figure>


<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">X_train,P_train=define_variables2(train_data,date0)</span><br><span class="line">X_train.head()</span><br></pre></td></tr></tbody></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;</code></pre>
<p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>vaccinated</th>
      <th>T</th>
      <th>T2</th>
      <th>const</th>
      <th>Monday</th>
      <th>Saturday</th>
      <th>Sunday</th>
      <th>Thursday</th>
      <th>Tuesday</th>
      <th>Wednesday</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>2</th>
      <td>0.440153</td>
      <td>21</td>
      <td>441</td>
      <td>1.0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.431722</td>
      <td>0</td>
      <td>0</td>
      <td>1.0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>6</th>
      <td>0.435852</td>
      <td>10</td>
      <td>100</td>
      <td>1.0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>8</th>
      <td>0.441906</td>
      <td>24</td>
      <td>576</td>
      <td>1.0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>10</th>
      <td>0.437877</td>
      <td>15</td>
      <td>225</td>
      <td>1.0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
</div>




<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">mod=sm.WLS(Y_train,X_train,P_train)</span><br><span class="line">res=mod.fit()</span><br><span class="line">res.params</span><br></pre></td></tr></tbody></table></figure>




<pre><code>vaccinated   -1.510900e-04
T             1.028112e-06
T2           -6.639196e-10
const         8.186223e-05
Monday       -2.235516e-06
Saturday     -8.318830e-07
Sunday       -3.189080e-06
Thursday     -2.233662e-08
Tuesday       3.232847e-07
Wednesday    -2.672040e-07
dtype: float64</code></pre>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">res.rsquared</span><br></pre></td></tr></tbody></table></figure>




<pre><code>0.37572844250241666</code></pre>
<p>$R^2$ is 0.376, higher than the 0.373 we previously have. Day of the week provides a <strong>small improvement</strong> on $R^2$ compared to original model.</p>
</body></html>
    
    </div>
    
    
</article>




    
        <article class="article content gallery" itemscope itemprop="blogPost">
    <h1 class="article-title is-size-3 is-size-4-mobile" itemprop="name">
        
            <a href="/2020/08/02/word2vec-on-bbg/" itemprop="url">Training Word Embedding on Bloomberg News</a>
        
    </h1>
    <div class="article-meta columns is-variable is-1 is-multiline is-mobile is-size-7-mobile">
        <span class="column is-narrow">
            <time datetime="2020-08-02T16:00:00.000Z" itemprop="datePublished">Aug 2 2020</time>
        </span>
        
        <span class="column is-narrow article-category">
            <i class="far fa-folder"></i>
            <a class="article-category-link" href="/categories/NLP/">NLP</a>
        </span>
        
        
        <span class="column is-narrow">
            
            
            9 minutes read (About 1350 words)
        </span>
        
    </div>
    <div class="article-entry is-size-6-mobile" itemprop="articleBody">
    
        <html><head></head><body><p>While working from home (stuck at home indeed), I get a bit more time, so revisited some old deep learning notes last month. Would it be awesome if I can come up with a project to play with? As we all know knowledge would go rusty if lack of use!</p>
<p>Initially, I was thinking about a machine translator using Attention Models, because I was once asked in an interview. The fund showed a particular interest in developing such a tool for the purpose of distributing research reports to non-English-speaking countries. But I was completely set back by the data sets asking for literally thousands of dollars&#x2026; While if labeled dataset is that expensive, why not run some unsurprised learning? Training word2vec doesn&#x2019;t sound like a bad idea. I&#x2019;ve been writing some views on FICC (fixed income, currencies, and commodities) on my other <a target="_blank" rel="noopener" href="https://yumengsblog.com/">non-geeky-at-all blog</a>. Won&#x2019;t it be fun to run word embeddings on Bloomberg news, see if people talk about the right things when they analyze inflation and gold prices?</p>
<p>So here we go, rock and roll!</p>
<p>First it&#x2019;s a bit introduction to word embedding. The term refers to a set of techniques in natural language processing that can map words into high dimensional vectors of real numbers. The vectors/embeddings can be seen as featurized representations of words, which preserve semantic properties, hence are commonly used as inputs in NLP tasks.</p>
<p>Here is an example, </p>
<img src="http://yumeng-li.github.io/pics/embeddingVectors.png" alt="embedding vectors" width="450">

<p>While in real cases, the dimension of vectors is usually a lot higher.</p>
<p>We will use a tool called word2vec to convert words into embeddings, <a target="_blank" rel="noopener" href="https://radimrehurek.com/gensim/">gensim</a> has it implemented in a very easy-to-use way. </p>
<blockquote>
<p>Word2vec is a technique for natural language processing. The word2vec algorithm uses a neural network model to learn word associations from a large corpus of text. Once trained, such a model can detect synonymous words or suggest additional words for a partial sentence.</p>
</blockquote>
<p>More explanation on word2vec can be found <a target="_blank" rel="noopener" href="https://www.wikiwand.com/en/Word2vec">here</a>.</p>
<h2 id="Creating-Corpus"><a href="#Creating-Corpus" class="headerlink" title="Creating Corpus"></a>Creating Corpus</h2><h3 id="Scarping-Addresses-of-Bloomberg-Articles"><a href="#Scarping-Addresses-of-Bloomberg-Articles" class="headerlink" title="Scarping Addresses of Bloomberg Articles"></a>Scarping Addresses of Bloomberg Articles</h3><p>I would like to run the embeddings only on the most recent posts, so first need to grab their urls through a search page.</p>
<p>Import the libraries,</p>
<figure class="highlight bash hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">import re</span><br><span class="line">import requests</span><br><span class="line">import string</span><br><span class="line">import numpy as np</span><br><span class="line">import gensim.models.word2vec as word2vec</span><br><span class="line">from bs4 import BeautifulSoup</span><br><span class="line">from time import sleep</span><br></pre></td></tr></tbody></table></figure>
<p>A function that can scrape the urls of articles that show up in the top p pages related to a specific topic,</p>
<figure class="highlight bash hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">def scrape_bloomberg_urls(subject, maxPage, headers):</span><br><span class="line">    urls = []</span><br><span class="line">    <span class="hljs-keyword">for</span> p <span class="hljs-keyword">in</span> range(1,maxPage):</span><br><span class="line">        searchUrl = <span class="hljs-string">&apos;https://www.bloomberg.com/search?query=&apos;</span> + subject + <span class="hljs-string">&apos;&amp;sort=relevance:desc&apos;</span> + <span class="hljs-string">&apos;&amp;page=&apos;</span> + str(p)</span><br><span class="line">        response = requests.get(searchUrl, headers=headers)</span><br><span class="line">        soup = BeautifulSoup(response.content, <span class="hljs-string">&apos;html.parser&apos;</span>)</span><br><span class="line">        regex = re.compile(<span class="hljs-string">&apos;.*headline.*&apos;</span>)</span><br><span class="line">        <span class="hljs-keyword">for</span> tag <span class="hljs-keyword">in</span> soup.findAll(<span class="hljs-string">&apos;a&apos;</span>, {<span class="hljs-string">&quot;class&quot;</span> : regex}, href = True):</span><br><span class="line">            href = tag.attrs[<span class="hljs-string">&apos;href&apos;</span>]</span><br><span class="line">            <span class="hljs-keyword">if</span> <span class="hljs-string">&apos;/news/articles/&apos;</span> <span class="hljs-keyword">in</span> href and href not <span class="hljs-keyword">in</span> urls:</span><br><span class="line">                urls.append(href)            </span><br><span class="line">    <span class="hljs-built_in">return</span> urls</span><br></pre></td></tr></tbody></table></figure>

<p>Set up headers to pass to the request, websites have gone crazy about blocking robots,</p>
<figure class="highlight bash hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">headers = {</span><br><span class="line">    <span class="hljs-string">&apos;user-agent&apos;</span>: <span class="hljs-string">&apos;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36&apos;</span>,</span><br><span class="line">    <span class="hljs-string">&apos;referrer&apos;</span>: <span class="hljs-string">&apos;https://google.com&apos;</span>,</span><br><span class="line">    <span class="hljs-string">&apos;Accept&apos;</span>: <span class="hljs-string">&apos;text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8&apos;</span>,</span><br><span class="line">    <span class="hljs-string">&apos;Accept-Encoding&apos;</span>: <span class="hljs-string">&apos;gzip, deflate, br&apos;</span>,</span><br><span class="line">    <span class="hljs-string">&apos;Accept-Language&apos;</span>: <span class="hljs-string">&apos;en-US,en;q=0.9&apos;</span>,</span><br><span class="line">    <span class="hljs-string">&apos;Pragma&apos;</span>: <span class="hljs-string">&apos;no-cache&apos;</span>}</span><br></pre></td></tr></tbody></table></figure>

<p>We get 155 articles here, not a large dataset so it may give us weird results, but should be good enough for an exercise,</p>
<figure class="highlight bash hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">In: len(urls)</span><br><span class="line">Out: 155</span><br></pre></td></tr></tbody></table></figure>

<figure class="highlight bash hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">In: urls[:5]</span><br><span class="line">Out: </span><br><span class="line">[<span class="hljs-string">&apos;https://www.bloomberg.com/news/articles/2020-08-07/inflation-trend-a-friend-in-real-yield-contest-seasia-rates&apos;</span>,</span><br><span class="line"> <span class="hljs-string">&apos;https://www.bloomberg.com/news/articles/2020-08-06/blackrock-joins-crescendo-of-inflation-warnings-amid-virus-fight&apos;</span>,</span><br><span class="line"> <span class="hljs-string">&apos;https://www.bloomberg.com/news/articles/2020-08-06/china-inflation-rate-headed-for-0-on-cooling-food-prices-chart&apos;</span>,</span><br><span class="line"> <span class="hljs-string">&apos;https://www.bloomberg.com/news/articles/2020-08-06/inflation-binds-czechs-after-fast-rate-cuts-decision-day-guide&apos;</span>,</span><br><span class="line"> <span class="hljs-string">&apos;https://www.bloomberg.com/news/articles/2020-08-07/jpmorgan-rejects-threat-to-dollar-status-flagged-by-goldman&apos;</span>]</span><br></pre></td></tr></tbody></table></figure>
<h3 id="Parsing-Articles"><a href="#Parsing-Articles" class="headerlink" title="Parsing Articles"></a>Parsing Articles</h3><p>This step is to parse articles through the urls and save the corpus into a local text file.</p>
<figure class="highlight bash hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">def parse_article(urls, headers):</span><br><span class="line">    corpus = []</span><br><span class="line">    <span class="hljs-keyword">for</span> url <span class="hljs-keyword">in</span> urls:</span><br><span class="line">        response = requests.get(url, headers=headers)</span><br><span class="line">        soup = BeautifulSoup(response.content,<span class="hljs-string">&apos;lxml&apos;</span>)</span><br><span class="line">        <span class="hljs-keyword">for</span> tag <span class="hljs-keyword">in</span> soup.find_all(<span class="hljs-string">&apos;p&apos;</span>):</span><br><span class="line">            content = tag.get_text()</span><br><span class="line">            cleanedContent = content.tra nslate(str.maketrans(<span class="hljs-string">&apos;&apos;</span>, <span class="hljs-string">&apos;&apos;</span>, string.punctuation)).lower()</span><br><span class="line">            corpus.append(cleanedContent)</span><br><span class="line">        seconds = np.random.randint(low=5, high=20)</span><br><span class="line">        sleep(seconds)</span><br><span class="line">    <span class="hljs-built_in">return</span> corpus</span><br></pre></td></tr></tbody></table></figure>
<p>A function serves as a writer,</p>
<figure class="highlight bash hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">def write_to_txt(outdir, subject, contentList):</span><br><span class="line">    outputfile = f<span class="hljs-string">&apos;{outdir}/{subject}.txt&apos;</span></span><br><span class="line">    with open(outputfile, <span class="hljs-string">&apos;a&apos;</span>) as file:</span><br><span class="line">        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> contentList:</span><br><span class="line">            file.write(f<span class="hljs-string">&apos;{i}\n&apos;</span>)</span><br></pre></td></tr></tbody></table></figure>

<p>Run the functions,</p>
<figure class="highlight bash hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">corpus = parse_article(urls, headers)</span><br><span class="line">write_to_txt(<span class="hljs-string">&apos;Documents/Blog/&apos;</span>, <span class="hljs-string">&apos;corpus&apos;</span>, corpus)</span><br></pre></td></tr></tbody></table></figure>

<h2 id="Training-the-Model"><a href="#Training-the-Model" class="headerlink" title="Training the Model"></a>Training the Model</h2><p>Now that we have created the corpus, let&#x2019;s train the model using word2vec from gensim.</p>
<p>Gensim has a cool function <em>LineSentence</em> that can directly read sentences from a text file with one sentence a line. Now you probably get why I had the corpus saved this way.</p>
<figure class="highlight bash hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sentences = word2vec.LineSentence(<span class="hljs-string">&apos;corpus.txt&apos;</span>)</span><br><span class="line">model = word2vec.Word2Vec(sentences, min_count=10, workers=8, iter=500, window=15, size=300, negative=50)</span><br></pre></td></tr></tbody></table></figure>
<p>The model I choose is using the <a target="_blank" rel="noopener" href="https://www.wikiwand.com/en/N-gram">Skip-Gram</a> algorithm with Negative Sampling. Hyper-parameters above are what I find work well, see below for their definitions, just in case you would like to tune them yourself,</p>
<blockquote>
<ul>
<li>size &#x2013; Dimensionality of the word vectors.</li>
<li>window &#x2013; Maximum distance between the current and predicted word within a sentence.</li>
<li>min_count &#x2013; Ignores all words with total frequency lower than this.</li>
<li>workers &#x2013; Use these many worker threads to train the model (=faster training with multicore machines).</li>
<li>negative  &#x2013; If &gt; 0, negative sampling will be used, the int for negative specifies how many &#x201C;noise words&#x201D; should be drawn (usually between 5-20). If set to 0, no negative sampling is used.</li>
</ul>
</blockquote>
<h2 id="Checking-Out-the-Results"><a href="#Checking-Out-the-Results" class="headerlink" title="Checking Out the Results"></a>Checking Out the Results</h2><p>The goal of this exercise to find out what people talk about when they analyze inflation expectation (basically what currently drives the stock market, TIPS and metals to roar higher and higher) and gold (my favorite and probably the most promising asset in the next 10 years).</p>
<p>Word2vev model can find words that have been used in a similar context with the word we care about. Here we employ a method <em>most_similar</em> to reveal them.</p>
<p>Let&#x2019;s first check out the words people usually mention when they talk about inflation. I print out the top 20 words. The meaningful ones are,</p>
<ul>
<li>&#x201C;target&#x201D;, &#x201C;bank&#x2019;s&#x201D;, &#x201C;bank&#x201D; - of course central banks target on inflation and their monetary policies shape expectations.</li>
<li>&#x201C;nominal&#x201D;, &#x201C;negative&#x201D;, &#x201C;yields&#x201D; - words usually describing rates, make sense.</li>
<li>&#x201C;food&#x201D; - measure of inflation.</li>
<li>&#x201C;employment&#x201D; - make a lot sense if you remember the Phillips curve.</li>
</ul>
<p>The results are actually very good, authors did understand and explain inflations well.</p>
<figure class="highlight bash hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">In: model.wv.most_similar(<span class="hljs-string">&apos;inflation&apos;</span>,topn=20)</span><br><span class="line">Out:</span><br><span class="line">[(<span class="hljs-string">&apos;target&apos;</span>, 0.25702178478240967),</span><br><span class="line"> (<span class="hljs-string">&apos;strip&apos;</span>, 0.18441016972064972),</span><br><span class="line"> (<span class="hljs-string">&apos;securities&apos;</span>, 0.18246109783649445),</span><br><span class="line"> (<span class="hljs-string">&apos;full&apos;</span>, 0.16774789988994598),</span><br><span class="line"> (<span class="hljs-string">&apos;nominal&apos;</span>, 0.16606125235557556),</span><br><span class="line"> (<span class="hljs-string">&apos;bank&#x2019;s&apos;</span>, 0.162990003824234),</span><br><span class="line"> (<span class="hljs-string">&apos;yields&apos;</span>, 0.15935908257961273),</span><br><span class="line"> (<span class="hljs-string">&apos;negative&apos;</span>, 0.15806841850280762),</span><br><span class="line"> (<span class="hljs-string">&apos;remain&apos;</span>, 0.15371079742908478),</span><br><span class="line"> (<span class="hljs-string">&apos;levels&apos;</span>, 0.14426037669181824),</span><br><span class="line"> (<span class="hljs-string">&apos;well&apos;</span>, 0.1438026875257492),</span><br><span class="line"> (<span class="hljs-string">&apos;current&apos;</span>, 0.13828279078006744),</span><br><span class="line"> (<span class="hljs-string">&apos;attractive&apos;</span>, 0.13690069317817688),</span><br><span class="line"> (<span class="hljs-string">&apos;showed&apos;</span>, 0.1277044713497162),</span><br><span class="line"> (<span class="hljs-string">&apos;bank&apos;</span>, 0.12733221054077148),</span><br><span class="line"> (<span class="hljs-string">&apos;food&apos;</span>, 0.12377716600894928),</span><br><span class="line"> (<span class="hljs-string">&apos;similar&apos;</span>, 0.11955425888299942),</span><br><span class="line"> (<span class="hljs-string">&apos;the&apos;</span>, 0.11911500245332718),</span><br><span class="line"> (<span class="hljs-string">&apos;employment&apos;</span>, 0.11866464465856552),</span><br><span class="line"> (<span class="hljs-string">&apos;keep&apos;</span>, 0.11818670481443405)]</span><br></pre></td></tr></tbody></table></figure>

<p>Then let&#x2019;s try it for gold. It&#x2019;s good to see silver rank top, the two metals do hold a strong correlation despite the natures of them are very different, gold generally behaves as a bond of real rates, while silver should be viewed more as a commodity. (People make mistakes on this, see <a target="_blank" rel="noopener" href="https://www.investopedia.com/articles/optioninvestor/09/silver-thursday-hunt-brothers.asp">the story of Hunt Brothers</a>)</p>
<p>Other words are just descriptive, focusing on telling readers what&#x2019;s going on in the markets. While this is not helpful, Bloomberg! People want to know why, you may want to talk more about real rates, inflation expectations, debt and global growth in the future.</p>
<figure class="highlight bash hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">In: model.wv.most_similar(<span class="hljs-string">&apos;gold&apos;</span>,topn=30)</span><br><span class="line">Out:</span><br><span class="line">[(<span class="hljs-string">&apos;gold&#x2019;s&apos;</span>, 0.26054084300994873),</span><br><span class="line"> (<span class="hljs-string">&apos;silver&apos;</span>, 0.25277888774871826),</span><br><span class="line"> (<span class="hljs-string">&apos;ounce&apos;</span>, 0.2523342967033386),</span><br><span class="line"> (<span class="hljs-string">&apos;bullion&apos;</span>, 0.2294640988111496),</span><br><span class="line"> (<span class="hljs-string">&apos;2300&apos;</span>, 0.22592982649803162),</span><br><span class="line"> (<span class="hljs-string">&apos;spot&apos;</span>, 0.2171008288860321),</span><br><span class="line"> (<span class="hljs-string">&apos;climbing&apos;</span>, 0.19733953475952148),</span><br><span class="line"> (<span class="hljs-string">&apos;metal&apos;</span>, 0.1939847469329834),</span><br><span class="line"> (<span class="hljs-string">&apos;comex&apos;</span>, 0.19371576607227325),</span><br><span class="line"> (<span class="hljs-string">&apos;rally&apos;</span>, 0.18643531203269958),</span><br><span class="line"> (<span class="hljs-string">&apos;exchangetraded&apos;</span>, 0.1859540343284607),</span><br><span class="line"> (<span class="hljs-string">&apos;a&apos;</span>, 0.18539577722549438),</span><br><span class="line"> (<span class="hljs-string">&apos;delivery&apos;</span>, 0.17595867812633514),</span><br><span class="line"> (<span class="hljs-string">&apos;reaching&apos;</span>, 0.1702871322631836),</span><br><span class="line"> (<span class="hljs-string">&apos;strip&apos;</span>, 0.16905872523784637),</span><br><span class="line"> (<span class="hljs-string">&apos;posted&apos;</span>, 0.16247007250785828),</span><br><span class="line"> (<span class="hljs-string">&apos;lows&apos;</span>, 0.16047437489032745),</span><br><span class="line"> (<span class="hljs-string">&apos;as&apos;</span>, 0.1585429608821869),</span><br><span class="line"> (<span class="hljs-string">&apos;analysis&apos;</span>, 0.1577225774526596),</span><br><span class="line"> (<span class="hljs-string">&apos;2011&apos;</span>, 0.15711694955825806),</span><br><span class="line"> (<span class="hljs-string">&apos;precious&apos;</span>, 0.15656355023384094),</span><br><span class="line"> (<span class="hljs-string">&apos;threat&apos;</span>, 0.1542907953262329),</span><br><span class="line"> (<span class="hljs-string">&apos;more&apos;</span>, 0.1541929841041565),</span><br><span class="line"> (<span class="hljs-string">&apos;drive&apos;</span>, 0.15310749411582947),</span><br><span class="line"> (<span class="hljs-string">&apos;every&apos;</span>, 0.1524747610092163),</span><br><span class="line"> (<span class="hljs-string">&apos;analyst&apos;</span>, 0.1517343521118164),</span><br><span class="line"> (<span class="hljs-string">&apos;managing&apos;</span>, 0.14977654814720154),</span><br><span class="line"> (<span class="hljs-string">&apos;price&apos;</span>, 0.1497490406036377),</span><br><span class="line"> (<span class="hljs-string">&apos;amounts&apos;</span>, 0.14969849586486816),</span><br><span class="line"> (<span class="hljs-string">&apos;backed&apos;</span>, 0.1450338065624237)]</span><br></pre></td></tr></tbody></table></figure></body></html>
    
    </div>
    
    
</article>




    
    
    </div>
</section>
    <footer class="footer">
    <div class="container">
        <div class="columns content">
            <div class="column is-narrow has-text-centered">
                &copy; 2025 Yumeng Li&nbsp;
                Powered by <a href="http://hexo.io/" target="_blank">Hexo</a> & <a
                        target="_blank" rel="noopener" href="http://github.com/ppoffice/hexo-theme-minos">Minos</a>
            </div>
            <div class="column is-hidden-mobile"></div>

            
            <div class="column is-narrow">
                <div class="columns is-mobile is-multiline is-centered">
                
                    
                <a class="column is-narrow has-text-black" title="GitHub" target="_blank" rel="noopener" href="https://github.com/ppoffice/hexo-theme-minos">
                    
                    GitHub
                    
                </a>
                
                </div>
            </div>
            
            
        </div>
    </div>
</footer>
    <script src="//cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/moment.js/2.22.2/moment-with-locales.min.js"></script>

<!-- test if the browser is outdated -->
<div id="outdated">
    <h6>Your browser is out-of-date!</h6>
    <p>Update your browser to view this website correctly. <a id="btnUpdateBrowser" target="_blank" rel="noopener" href="http://outdatedbrowser.com/">Update my browser now </a></p>
    <p class="last"><a href="#" id="btnCloseUpdateBrowser" title="Close">&times;</a></p>
</div>
<script src="//cdnjs.cloudflare.com/ajax/libs/outdated-browser/1.1.5/outdatedbrowser.min.js"></script>
<script>
    $(document).ready(function () {
        // plugin function, place inside DOM ready function
        outdatedBrowser({
            bgColor: '#f25648',
            color: '#ffffff',
            lowerThan: 'flex'
        })
    });
</script>

<script>
    window.FontAwesomeConfig = {
        searchPseudoElements: true
    }
    moment.locale("en-AU");
</script>


    
    
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML"></script>
<script>
    MathJax.Hub.Config({
        "HTML-CSS": {
            matchFontHeight: false
        },
        SVG: {
            matchFontHeight: false
        },
        CommonHTML: {
            matchFontHeight: false
        },
        tex2jax: {
            inlineMath: [
                ['$','$'],
                ['\\(','\\)']
            ]
        }
    });
</script>

    
    
    
    
<script src="//cdnjs.cloudflare.com/ajax/libs/lightgallery/1.6.8/js/lightgallery-all.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/justifiedGallery/3.6.5/js/jquery.justifiedGallery.min.js"></script>
<script>
    (function ($) {
        $(document).ready(function () {
            if (typeof($.fn.lightGallery) === 'function') {
                $('.article.gallery').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof($.fn.justifiedGallery) === 'function') {
                $('.justified-gallery').justifiedGallery();
            }
        });
    })(jQuery);
</script>

    
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.0/clipboard.min.js"></script>
    <style>
        .hljs {
            position: relative;
        }

        .hljs .clipboard-btn {
            float: right;
            color: #9a9a9a;
            background: none;
            border: none;
            cursor: pointer;
        }

        .hljs .clipboard-btn:hover {
          color: #8a8a8a;
        }

        .hljs > .clipboard-btn {
            display: none;
            position: absolute;
            right: 4px;
            top: 4px;
        }

        .hljs:hover > .clipboard-btn {
            display: inline;
        }

        .hljs > figcaption > .clipboard-btn {
            margin-right: 4px;
        }
    </style>
    <script>
      $(document).ready(function () {
        $('figure.hljs').each(function(i, figure) {
          var codeId = 'code-' + i;
          var code = figure.querySelector('.code');
          var copyButton = $('<button>Copy <i class="far fa-clipboard"></i></button>');
          code.id = codeId;
          copyButton.addClass('clipboard-btn');
          copyButton.attr('data-clipboard-target-id', codeId);

          var figcaption = figure.querySelector('figcaption');

          if (figcaption) {
            figcaption.append(copyButton[0]);
          } else {
            figure.prepend(copyButton[0]);
          }
        })

        var clipboard = new ClipboardJS('.clipboard-btn', {
          target: function(trigger) {
            return document.getElementById(trigger.getAttribute('data-clipboard-target-id'));
          }
        });
        clipboard.on('success', function(e) {
          e.clearSelection();
        })
      })
    </script>

    
    

    



<script src="/js/script.js"></script>


    
    <div class="searchbox ins-search">
    <div class="searchbox-mask"></div>
    <div class="searchbox-container ins-search-container">
        <div class="searchbox-input-wrapper">
            <input type="text" class="searchbox-input ins-search-input" placeholder="Type something..." />
            <span class="searchbox-close ins-close ins-selectable"><i class="fa fa-times-circle"></i></span>
        </div>
        <div class="searchbox-result-wrapper ins-section-wrapper">
            <div class="ins-section-container"></div>
        </div>
    </div>
</div>
<script>
    (function (window) {
        var INSIGHT_CONFIG = {
            TRANSLATION: {
                POSTS: 'Posts',
                PAGES: 'Pages',
                CATEGORIES: 'Categories',
                TAGS: 'Tags',
                UNTITLED: '(Untitled)',
            },
            CONTENT_URL: '/content.json',
        };
        window.INSIGHT_CONFIG = INSIGHT_CONFIG;
    })(window);
</script>

<script src="/js/insight.js"></script>

    
</body>
</html>