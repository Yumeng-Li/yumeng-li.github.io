<!DOCTYPE html>
<html class="has-navbar-fixed-top">
<head>
    <meta charset="utf-8">
<title>Category: Time Series Analysis - The Practical Quant</title>
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/outdated-browser/1.1.5/outdatedbrowser.min.css">




<meta name="description" content="">





    <meta property="og:type" content="website">
<meta property="og:title" content="The Practical Quant">
<meta property="og:url" content="http://yoursite.com/categories/Time-Series-Analysis/index.html">
<meta property="og:site_name" content="The Practical Quant">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="Yumeng Li">
<meta name="twitter:card" content="summary">





<link rel="icon" href="/favicon.png">


<link rel="stylesheet" href="//fonts.googleapis.com/css?family=Ovo|Source+Code+Pro">
<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/bulma/0.6.2/css/bulma.min.css">


<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/lightgallery/1.6.8/css/lightgallery.min.css">
<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/justifiedGallery/3.6.5/css/justifiedGallery.min.css">


<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/atom-one-light.min.css">


<link rel="stylesheet" href="/css/style.css">


<script defer src="//use.fontawesome.com/releases/v5.0.8/js/all.js"></script>


    
    
    
    
    
    
    
    
    
    

    


<meta name="generator" content="Hexo 5.0.0"></head>
<body>
    
<nav class="navbar is-transparent is-fixed-top navbar-main" role="navigation" aria-label="main navigation">
    <div class="container">
        <div class="navbar-brand">
            <a class="navbar-item navbar-logo" href="/">
                
                    
                    THE PRACTICAL QUANT
                    
                
            </a>
            <div class="navbar-burger">
                <span></span>
                <span></span>
                <span></span>
            </div>
        </div>
        
        <div class="navbar-menu navbar-start">
            
            <a class="navbar-item "
               href="/archives">Archives</a>
            
            <a class="navbar-item "
               href="/categories">Category</a>
            
            <a class="navbar-item "
               target="_blank" rel="noopener" href="http://allaboutmacros.com/">Blog</a>
            
            <a class="navbar-item "
               target="_blank" rel="noopener" href="https://www.linkedin.com/in/emmelineli/">Linkedin</a>
            
        </div>
        
        <div class="navbar-menu navbar-end">
            
            <a class="navbar-item search" title="Search" href="javascript:;">
                <i class="fas fa-search"></i>
            </a>
            
            
            
            <a class="navbar-item" title="GitHub" target="_blank" rel="noopener" href="https://github.com/ppoffice/hexo-theme-minos">
                
                <i class="fab fa-github"></i>
                
            </a>
               
            
        </div>
    </div>
</nav>

    <section class="section section-heading">
    <div class="container">
        <div class="content">
            <h5><i class="far fa-folder"></i>Time Series Analysis</h5>
        </div>
    </div>
</section>
<section class="section">
    <div class="container">
    
        <article class="article content gallery" itemscope itemprop="blogPost">
    <h1 class="article-title is-size-3 is-size-4-mobile" itemprop="name">
        
            <a href="/2022/01/11/VECM2/" itemprop="url">Predicting Post-Pandemic Consumption Behaviors Given Potential Paths of Monetary Policy, Part 2 -- Cointegration, VAR/VECM and Forecast</a>
        
    </h1>
    <div class="article-meta columns is-variable is-1 is-multiline is-mobile is-size-7-mobile">
        <span class="column is-narrow">
            <time datetime="2022-01-11T17:00:00.000Z" itemprop="datePublished">Jan 11 2022</time>
        </span>
        
        <span class="column is-narrow article-category">
            <i class="far fa-folder"></i>
            <a class="article-category-link" href="/categories/Time-Series-Analysis/">Time Series Analysis</a>
        </span>
        
        
        <span class="column is-narrow">
            
            
            26 minutes read (About 3833 words)
        </span>
        
    </div>
    <div class="article-entry is-size-6-mobile" itemprop="articleBody">
    
        <html><head></head><body><p>In the last blog, we went through the economic intuitions behind this consumption model, conducted some preliminary analysis in search for structural breaks and checked the stationarity of the endogenous variables using ADF and KPSS tests. Since we have found that none of the variables can be considered stationary, we can therefore ask whether the variables form a cointegrated system with a given number of &#x201C;common trends&#x201D;. Intuitively, I would argue that there exists one cointegration equation among the three variables as in the long run, one&#x2019;s consumption and net wealth should add up to the one&#x2019;s income.</p>
<p>So let&#x2019;s keep going down to the path, and see how we can test for cointegration and include it in a VECM.</p>
<img src="http://yumeng-li.github.io/VECM_files/flowchart.png">



<h2 id="Cointegration-Test"><a href="#Cointegration-Test" class="headerlink" title="Cointegration Test"></a>Cointegration Test</h2><p>Given that all variables are consistent with the unit root hypothesis and possibly cointegrated, we can use <strong>Johansen&#x2019;s trace statistic</strong> to test for the existence of a common trend i.e., a long-run cointegration relationship among the three endogenous variables.</p>
<p>First, we should estimate the VAR in levels so as to find out the optimal lags to use for the cointegration test. You might want to do this in first differences depending on your preferences, but basically the objective here is to run a preliminary VAR without worrying about whether the variables are cointegrated, which is <strong>obviously an issue given that they all have unit roots</strong>.</p>
<h3 id="VAR-in-Levels"><a href="#VAR-in-Levels" class="headerlink" title="VAR in Levels"></a>VAR in Levels</h3><p>I&#x2019;m going to allow for 10 lags in the first instance just to get the process going, and these are the variables in the VAR as I include the dummy variables sb_1975_4, sb_2008_3 to allow for the possibility of a structural breaks.</p>
<p>The BIC and HQ statistics suggest two lags for the optimal number of lags, whereas the AIC and FPE statistics suggest three. We choose the VAR model with three lags to allow for the possibility.</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train = train.iloc[<span class="hljs-number">1</span>:]</span><br></pre></td></tr></tbody></table></figure>


<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">model = VAR(endog=train[[<span class="hljs-string">&apos;ln_rc&apos;</span>,<span class="hljs-string">&apos;ln_rdy&apos;</span>,<span class="hljs-string">&apos;ln_rnw&apos;</span>]], exog =train[[<span class="hljs-string">&apos;sb_1975_4&apos;</span>,<span class="hljs-string">&apos;sb_2008_3&apos;</span>,<span class="hljs-string">&apos;const&apos;</span>]])</span><br><span class="line">res = model.select_order(<span class="hljs-number">10</span>)</span><br><span class="line">res.summary()</span><br></pre></td></tr></tbody></table></figure>




<table class="simpletable">
<caption>VAR Order Selection (* highlights the minimums)</caption>
<tbody><tr>
   <td></td>      <th>AIC</th>         <th>BIC</th>         <th>FPE</th>        <th>HQIC</th>    
</tr>
<tr>
  <th>0</th>  <td>    -17.58</td>  <td>    -17.39</td>  <td> 2.310e-08</td>  <td>    -17.51</td>
</tr>
<tr>
  <th>1</th>  <td>    -27.99</td>  <td>    -27.66</td>  <td> 6.975e-13</td>  <td>    -27.86</td>
</tr>
<tr>
  <th>2</th>  <td>    -28.19</td>  <td>    -27.71*</td> <td> 5.696e-13</td>  <td>    -28.00*</td>
</tr>
<tr>
  <th>3</th>  <td>    -28.20*</td> <td>    -27.58</td>  <td> 5.664e-13*</td> <td>    -27.95</td>
</tr>
<tr>
  <th>4</th>  <td>    -28.17</td>  <td>    -27.40</td>  <td> 5.829e-13</td>  <td>    -27.86</td>
</tr>
<tr>
  <th>5</th>  <td>    -28.13</td>  <td>    -27.22</td>  <td> 6.067e-13</td>  <td>    -27.76</td>
</tr>
<tr>
  <th>6</th>  <td>    -28.12</td>  <td>    -27.07</td>  <td> 6.144e-13</td>  <td>    -27.69</td>
</tr>
<tr>
  <th>7</th>  <td>    -28.07</td>  <td>    -26.87</td>  <td> 6.488e-13</td>  <td>    -27.58</td>
</tr>
<tr>
  <th>8</th>  <td>    -28.02</td>  <td>    -26.68</td>  <td> 6.810e-13</td>  <td>    -27.48</td>
</tr>
<tr>
  <th>9</th>  <td>    -28.02</td>  <td>    -26.53</td>  <td> 6.807e-13</td>  <td>    -27.42</td>
</tr>
<tr>
  <th>10</th> <td>    -27.97</td>  <td>    -26.33</td>  <td> 7.217e-13</td>  <td>    -27.31</td>
</tr>
</tbody></table>



<h3 id="Johansen-Cointegration-Test"><a href="#Johansen-Cointegration-Test" class="headerlink" title="Johansen Cointegration Test"></a>Johansen Cointegration Test</h3><p>Then, we conduct the Johansen&#x2019;s test with two lags. One has to ensure that the lag interval for the<br>differenced endogenous variables is <strong>1 smaller</strong> than the number of lags used in VAR in levels by moving to the corresponding VECM form.</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">vec_rank = vecm.select_coint_rank(endog=train[[<span class="hljs-string">&apos;ln_rc&apos;</span>,<span class="hljs-string">&apos;ln_rdy&apos;</span>,<span class="hljs-string">&apos;ln_rnw&apos;</span>]], det_order = <span class="hljs-number">0</span>, k_ar_diff = <span class="hljs-number">2</span>, method = <span class="hljs-string">&apos;trace&apos;</span>, signif=<span class="hljs-number">0.05</span>)</span><br><span class="line">print(vec_rank.summary())</span><br><span class="line">print(<span class="hljs-string">&apos;The rank to choose according to the Johansen test: &apos;</span>+ str(vec_rank.rank))</span><br></pre></td></tr></tbody></table></figure>

<pre><code>Johansen cointegration test using trace test statistic with 5% significance level
=====================================
r_0 r_1 test statistic critical value
-------------------------------------
  0   3          33.75          29.80
  1   3          11.44          15.49
-------------------------------------
The rank to choose according to the Johansen test: 1</code></pre>
<p>The results suggest that we can reject the null hypothesis of no cointegration (or zero cointegrating vectors) using a 5% significance level. Moreover, we cannot reject the null hypotheses of at most 1 cointegrating vectors versus the alternative of 2. Therefore, we assume that there exists one (and only one) cointegrating vector.</p>
<h2 id="VECM-Estimation"><a href="#VECM-Estimation" class="headerlink" title="VECM Estimation"></a>VECM Estimation</h2><p>Since there is a cointegration relationship existing, we should construct an ECM equation (e.g., an error correction model) that can explain the actual behavior of consumption during the sample period based on the long-run specification.</p>
<p>In developing the ECM model, we can include additional exogenous variables in the VECM to explain short-run dynamics of consumption. Besides the structural break around 1976 and the inequality gap, here are a bunch of other variables that we should consider. Remember the the determinants we discussed at the beginning?</p>
<ul>
<li>Disposable Income (in the current period) (+)</li>
<li>Expectations (consumer confidence indexes, employment growth) (+)</li>
<li>Wealth (stock market performance, housing prices) (+)</li>
<li>Uncertainty (precautionary saving) (-)</li>
<li>Availability of Credit (+)</li>
<li>Real Interest Rate (?)<ul>
<li>Substitution effect (-)</li>
<li>Income effect (+)</li>
</ul>
</li>
</ul>
<h3 id="Explore-Exogenous-Variables"><a href="#Explore-Exogenous-Variables" class="headerlink" title="Explore Exogenous Variables"></a>Explore Exogenous Variables</h3><p>In developing the ECM model, we should include additional exogenous variables in the VECM to explain short-run dynamics of consumption.</p>
<h4 id="Expectations"><a href="#Expectations" class="headerlink" title="Expectations"></a>Expectations</h4><p>Here we use the unemployment rate and the consumer confidence index as exogenous variables. These variables are proxies for changes in the level of income uncertainty facing the household sector.</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">f, (ax1, ax2) = plt.subplots(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>,figsize=(<span class="hljs-number">14</span>,<span class="hljs-number">4</span>))</span><br><span class="line">ax1.plot(train[<span class="hljs-string">&apos;unemp&apos;</span>])</span><br><span class="line">ax2.plot(train[<span class="hljs-string">&apos;unemp&apos;</span>].diff().dropna())</span><br><span class="line">ax1.set_title(<span class="hljs-string">&apos;unemp&apos;</span>)</span><br><span class="line">ax2.set_title(<span class="hljs-string">&apos;diff_unemp&apos;</span>)</span><br></pre></td></tr></tbody></table></figure>


<img src="http://yumeng-li.github.io/VECM_files/VECM_61_1.png">


<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">f, (ax1, ax2) = plt.subplots(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>,figsize=(<span class="hljs-number">14</span>,<span class="hljs-number">4</span>))</span><br><span class="line">ax1.plot(train[<span class="hljs-string">&apos;consumer_confidence&apos;</span>])</span><br><span class="line">ax2.plot(np.log(train[<span class="hljs-string">&apos;consumer_confidence&apos;</span>]))</span><br><span class="line">ax1.set_title(<span class="hljs-string">&apos;consumer_confidence&apos;</span>)</span><br><span class="line">ax2.set_title(<span class="hljs-string">&apos;ln_consumer_confidence&apos;</span>)</span><br></pre></td></tr></tbody></table></figure>



<img src="http://yumeng-li.github.io/VECM_files/VECM_62_1.png">



<p>Let&#x2019;s then conduct the stationary tests for the change in the unemployment rate and the logarithm of the consumer confidence index, to check whether they are separately consistent with the stationarity assumption.</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">augmented_dickey_fuller_test(train[<span class="hljs-string">&apos;unemp&apos;</span>].diff().dropna())</span><br><span class="line">kpss_test(train[<span class="hljs-string">&apos;unemp&apos;</span>].diff().dropna())</span><br></pre></td></tr></tbody></table></figure>

<pre><code>ADF Statistic: -5.032319
ADF p-value: 0.000019
stationary - null hypothesis of a unit root can be rejected at a 5% significant level
KPSS Statistic: 0.048204
KPSS p-value: 0.100000
stationary - null hypothesis of stationary cannot be rejected at a 5% significant level</code></pre>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">augmented_dickey_fuller_test(np.log(train[<span class="hljs-string">&apos;consumer_confidence&apos;</span>]))</span><br><span class="line">kpss_test(np.log(train[<span class="hljs-string">&apos;consumer_confidence&apos;</span>]))</span><br></pre></td></tr></tbody></table></figure>

<pre><code>ADF Statistic: -3.450606
ADF p-value: 0.009350
stationary - null hypothesis of a unit root can be rejected at a 5% significant level
KPSS Statistic: 0.134007
KPSS p-value: 0.100000
stationary - null hypothesis of stationary cannot be rejected at a 5% significant level</code></pre>
<h4 id="Availability-of-Credit"><a href="#Availability-of-Credit" class="headerlink" title="Availability of Credit"></a>Availability of Credit</h4><p>Evidently, the availability and cost of credit will also affect decisions of households to consume. So the behavior of monetary aggregates in terms of the availability of credit and the interest rates in terms of the cost of credit will matter for our projections for private consumption.</p>
<p>(Honestly, it would be ridiculous to ignore the crazy amount of money that the fed has been pumping into the economy since 2008 and its consequential impact. The hopeless belief in the Keynesian&#x2019;s monetary policy.)</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">f, (ax1, ax2) = plt.subplots(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>,figsize=(<span class="hljs-number">14</span>,<span class="hljs-number">4</span>))</span><br><span class="line">ax1.plot(train[<span class="hljs-string">&apos;m2&apos;</span>])</span><br><span class="line">ax2.plot(np.log(train[<span class="hljs-string">&apos;m2&apos;</span>]).diff())</span><br><span class="line">ax1.set_title(<span class="hljs-string">&apos;real m2&apos;</span>)</span><br><span class="line">ax2.set_title(<span class="hljs-string">&apos;diff real m2&apos;</span>)</span><br></pre></td></tr></tbody></table></figure>



<img src="http://yumeng-li.github.io/VECM_files/VECM_67_1.png">



<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">f, (ax1, ax2) = plt.subplots(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>,figsize=(<span class="hljs-number">14</span>,<span class="hljs-number">4</span>))</span><br><span class="line">ax1.plot(train[<span class="hljs-string">&apos;dff&apos;</span>])</span><br><span class="line">ax2.plot(train[<span class="hljs-string">&apos;dff&apos;</span>].diff())</span><br><span class="line">ax1.set_title(<span class="hljs-string">&apos;Federal Funds Effective Rate&apos;</span>)</span><br><span class="line">ax2.set_title(<span class="hljs-string">&apos;Diff Federal Funds Effective Rate&apos;</span>)</span><br></pre></td></tr></tbody></table></figure>



<img src="http://yumeng-li.github.io/VECM_files/VECM_68_1.png">



<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">augmented_dickey_fuller_test(np.log(train[<span class="hljs-string">&apos;m2&apos;</span>]).diff().dropna())</span><br><span class="line">kpss_test(np.log(train[<span class="hljs-string">&apos;m2&apos;</span>]).diff().dropna())</span><br></pre></td></tr></tbody></table></figure>

<pre><code>ADF Statistic: -5.040178
ADF p-value: 0.000018
stationary - null hypothesis of a unit root can be rejected at a 5% significant level
KPSS Statistic: 0.172896
KPSS p-value: 0.100000
stationary - null hypothesis of stationary cannot be rejected at a 5% significant level</code></pre>
<p>Let&#x2019;s adding these new variables to the dataframe.</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">df[<span class="hljs-string">&apos;d_unemp&apos;</span>] = df[<span class="hljs-string">&apos;unemp&apos;</span>].diff()</span><br><span class="line">df[<span class="hljs-string">&apos;ln_consumer_confidence&apos;</span>] = np.log(df[<span class="hljs-string">&apos;consumer_confidence&apos;</span>])</span><br><span class="line">df[<span class="hljs-string">&apos;d_ln_m2&apos;</span>] = np.log(df[<span class="hljs-string">&apos;m2&apos;</span>]).diff()</span><br><span class="line">df[<span class="hljs-string">&apos;d_dff&apos;</span>] = df[<span class="hljs-string">&apos;dff&apos;</span>].diff()</span><br><span class="line">df[<span class="hljs-string">&apos;spread_10y3m&apos;</span>] = df[<span class="hljs-string">&apos;yield_10y&apos;</span>]-df[<span class="hljs-string">&apos;yield_3m&apos;</span>]</span><br><span class="line"></span><br><span class="line">train,test = df[<span class="hljs-number">1</span>:cut], df[cut:]</span><br></pre></td></tr></tbody></table></figure>

<h3 id="Model-Estimation"><a href="#Model-Estimation" class="headerlink" title="Model Estimation"></a>Model Estimation</h3><p>Based on all previous analysis, we can eventually run a VECM. Based on the long-run equation, we specify the form of the model with three endogenous variables log(rc), log(rdy) and log(rnw), and allow for five additional exogenous variables.</p>
<p>The lag intervals for the differenced endogenous variables are 2 given that we estimated the VAR with 3 lags in levels.</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">endog = [<span class="hljs-string">&apos;ln_rc&apos;</span>,<span class="hljs-string">&apos;ln_rdy&apos;</span>,<span class="hljs-string">&apos;ln_rnw&apos;</span>]</span><br><span class="line">exog = [<span class="hljs-string">&apos;sb_1975_4&apos;</span>,<span class="hljs-string">&apos;sb_2008_3&apos;</span>,<span class="hljs-string">&apos;d_unemp&apos;</span>,<span class="hljs-string">&apos;ln_consumer_confidence&apos;</span>, <span class="hljs-string">&apos;d_ln_m2&apos;</span>, <span class="hljs-string">&apos;d_dff&apos;</span>]</span><br></pre></td></tr></tbody></table></figure>


<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">vec = vecm.VECM(endog = train[endog],</span><br><span class="line">                exog =train[exog],</span><br><span class="line">                k_ar_diff = <span class="hljs-number">2</span>, coint_rank = <span class="hljs-number">1</span>, deterministic = <span class="hljs-string">&quot;co&quot;</span>, missing= <span class="hljs-string">&apos;drop&apos;</span>)</span><br><span class="line">vecm_fit = vec.fit()</span><br></pre></td></tr></tbody></table></figure>

<p>The coefficients in the long-run equation are both statistically significantly different from zero (see the first three tables below), and most importantly, consistent with our economic priors i.e., income and wealth have positive coefficients on consumption (see parameters for ln_rdy and ln_rnw in the first table).</p>
<p>For the exogenous variables, the negative coefficient on the structural breaks indicates that the increasing inequality may compress the consumption of households. Positive and significant coefficient on real m2 complies with our assumption that expanding monetary policy through quantitative ease may stimulate consumption in the short term.</p>
<p>Also, the coefficient on the ECM term is <strong>negative</strong> and significant with respect to real consumption, and significant and <strong>positive</strong> with respect to disposable income, both using a 5% level of significance, which are reasonable since both consumption and income are endogenous variables and dependent on each other, a rational household would react to disequilibrium in consumption relative to its long-run path though simultaneous adjustments - lower their spending (negative coefficient) and increase their income (positive coefficient).</p>
<p>From the &#x201C;cointegration relations&#x201D; table, we can verify our idea that in the long run, amount that one&#x2019;s consumption and wealth accumulated would be equal to the income one earns.</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vecm_fit.summary().as_latex</span><br></pre></td></tr></tbody></table></figure>




<pre><code>&lt;bound method Summary.as_latex of &lt;class &apos;statsmodels.iolib.summary.Summary&apos;&gt;
&quot;&quot;&quot;
Det. terms outside the coint. relation &amp; lagged endog. parameters for equation ln_rc
==============================================================================
                 coef    std err          z      P&gt;|z|      [0.025      0.975]
------------------------------------------------------------------------------
const         -0.6625      0.154     -4.310      0.000      -0.964      -0.361
exog1         -0.0027      0.001     -3.309      0.001      -0.004      -0.001
exog2         -0.0052      0.001     -4.940      0.000      -0.007      -0.003
exog3         -0.0069      0.001     -4.782      0.000      -0.010      -0.004
exog4          0.1386      0.032      4.279      0.000       0.075       0.202
exog5          0.1697      0.033      5.189      0.000       0.106       0.234
exog6          0.0006      0.000      1.463      0.144      -0.000       0.001
L1.ln_rc      -0.2099      0.068     -3.108      0.002      -0.342      -0.078
L1.ln_rdy      0.0742      0.043      1.739      0.082      -0.009       0.158
L1.ln_rnw      0.0485      0.022      2.248      0.025       0.006       0.091
L2.ln_rc      -0.0623      0.063     -0.986      0.324      -0.186       0.062
L2.ln_rdy      0.0077      0.042      0.184      0.854      -0.074       0.090
L2.ln_rnw      0.0039      0.022      0.176      0.860      -0.039       0.047
Det. terms outside the coint. relation &amp; lagged endog. parameters for equation ln_rdy
==============================================================================
                 coef    std err          z      P&gt;|z|      [0.025      0.975]
------------------------------------------------------------------------------
const         -0.2331      0.251     -0.928      0.353      -0.725       0.259
exog1         -0.0025      0.001     -1.879      0.060      -0.005       0.000
exog2         -0.0031      0.002     -1.777      0.076      -0.006       0.000
exog3         -0.0069      0.002     -2.937      0.003      -0.011      -0.002
exog4          0.0640      0.053      1.210      0.226      -0.040       0.168
exog5          0.1607      0.053      3.007      0.003       0.056       0.265
exog6         -0.0003      0.001     -0.445      0.656      -0.002       0.001
L1.ln_rc       0.1241      0.110      1.124      0.261      -0.092       0.340
L1.ln_rdy     -0.1826      0.070     -2.620      0.009      -0.319      -0.046
L1.ln_rnw      0.0519      0.035      1.469      0.142      -0.017       0.121
L2.ln_rc      -0.1472      0.103     -1.426      0.154      -0.350       0.055
L2.ln_rdy      0.0505      0.068      0.738      0.460      -0.084       0.184
L2.ln_rnw     -0.0868      0.036     -2.421      0.015      -0.157      -0.017
Det. terms outside the coint. relation &amp; lagged endog. parameters for equation ln_rnw
==============================================================================
                 coef    std err          z      P&gt;|z|      [0.025      0.975]
------------------------------------------------------------------------------
const         -1.4968      0.494     -3.028      0.002      -2.466      -0.528
exog1          0.0003      0.003      0.097      0.923      -0.005       0.005
exog2         -0.0009      0.003     -0.260      0.795      -0.007       0.006
exog3         -0.0024      0.005     -0.514      0.607      -0.011       0.007
exog4          0.3093      0.104      2.968      0.003       0.105       0.513
exog5          0.1839      0.105      1.748      0.080      -0.022       0.390
exog6         -0.0021      0.001     -1.576      0.115      -0.005       0.001
L1.ln_rc       0.1244      0.217      0.573      0.567      -0.301       0.550
L1.ln_rdy     -0.2364      0.137     -1.723      0.085      -0.505       0.033
L1.ln_rnw      0.0856      0.069      1.232      0.218      -0.051       0.222
L2.ln_rc       0.2163      0.203      1.065      0.287      -0.182       0.615
L2.ln_rdy     -0.2192      0.135     -1.629      0.103      -0.483       0.044
L2.ln_rnw      0.0711      0.071      1.008      0.314      -0.067       0.209
               Loading coefficients (alpha) for equation ln_rc                
==============================================================================
                 coef    std err          z      P&gt;|z|      [0.025      0.975]
------------------------------------------------------------------------------
ec1           -0.0605      0.020     -2.993      0.003      -0.100      -0.021
               Loading coefficients (alpha) for equation ln_rdy              
==============================================================================
                 coef    std err          z      P&gt;|z|      [0.025      0.975]
------------------------------------------------------------------------------
ec1            0.0887      0.033      2.684      0.007       0.024       0.153
               Loading coefficients (alpha) for equation ln_rnw              
==============================================================================
                 coef    std err          z      P&gt;|z|      [0.025      0.975]
------------------------------------------------------------------------------
ec1           -0.1382      0.065     -2.126      0.033      -0.266      -0.011
          Cointegration relations for loading-coefficients-column 1          
==============================================================================
                 coef    std err          z      P&gt;|z|      [0.025      0.975]
------------------------------------------------------------------------------
beta.1         1.0000          0          0      0.000       1.000       1.000
beta.2        -1.1175      0.073    -15.311      0.000      -1.261      -0.974
beta.3         0.0555      0.058      0.963      0.336      -0.057       0.168
==============================================================================
&quot;&quot;&quot;&gt;</code></pre>
<h2 id="VECM-Forecast"><a href="#VECM-Forecast" class="headerlink" title="VECM Forecast"></a>VECM Forecast</h2><h3 id="Out-of-sample-Forecast"><a href="#Out-of-sample-Forecast" class="headerlink" title="Out-of-sample Forecast"></a>Out-of-sample Forecast</h3><p>As we are comfortable with the model estimated, let&#x2019;s do some forecasting with the out-of-sample data and evaluate how the model works.</p>
<p>To forecast for time beyond the sample period pre-2017, we need to provide futures shocks post-2017 i.e., the exogenous variables, which includes the dramatic economic downturn during the Covid recession. Let&#x2019;s plot them out and get a feel.</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">f, (ax1, ax2, ax3) = plt.subplots(<span class="hljs-number">1</span>,<span class="hljs-number">3</span>,figsize=(<span class="hljs-number">18</span>,<span class="hljs-number">4</span>))</span><br><span class="line">ax1.plot(test[<span class="hljs-string">&apos;d_unemp&apos;</span>])</span><br><span class="line">ax2.plot(test[<span class="hljs-string">&apos;d_ln_m2&apos;</span>])</span><br><span class="line">ax3.plot(test[<span class="hljs-string">&apos;d_dff&apos;</span>])</span><br><span class="line">ax1.set_title(<span class="hljs-string">&apos;Diff Unemployment Rate&apos;</span>)</span><br><span class="line">ax2.set_title(<span class="hljs-string">&apos;Diff Real M2&apos;</span>)</span><br><span class="line">ax3.set_title(<span class="hljs-string">&apos;Diff Fed Fund Rate&apos;</span>)</span><br><span class="line">f.autofmt_xdate()</span><br></pre></td></tr></tbody></table></figure>

<img src="http://yumeng-li.github.io/VECM_files/VECM_80_0.png">


<p>When forecasting, I include the confidence intervals for the predictions as well. Remember, point predictions won&#x2019;t help you much with gauging the uncertainty of forecasts.</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">mid, lower_1se, upper_1se = vecm_fit.predict(steps=len(test),alpha = <span class="hljs-number">0.32</span>, exog_fc =test[exog])</span><br><span class="line">_, lower_2se, upper_2se = vecm_fit.predict(steps=len(test),alpha = <span class="hljs-number">0.05</span>, exog_fc =test[exog])</span><br><span class="line"></span><br><span class="line">pred_mid = pd.DataFrame(np.exp(mid), columns=[<span class="hljs-string">&apos;pred_rc&apos;</span>,<span class="hljs-string">&apos;pred_rdy&apos;</span>,<span class="hljs-string">&apos;pred_rnw&apos;</span>], index = test.index)</span><br><span class="line">pred_lower_1se = pd.DataFrame(np.exp(lower_1se), columns=[<span class="hljs-string">&apos;lower_1se_rc&apos;</span>,<span class="hljs-string">&apos;lower_1se_rdy&apos;</span>,<span class="hljs-string">&apos;lower_1se_rnw&apos;</span>], index = test.index)</span><br><span class="line">pred_upper_1se = pd.DataFrame(np.exp(upper_1se), columns=[<span class="hljs-string">&apos;upper_1se_rc&apos;</span>,<span class="hljs-string">&apos;upper_1se_rdy&apos;</span>,<span class="hljs-string">&apos;upper_1se_rnw&apos;</span>], index = test.index)</span><br><span class="line">pred_lower_2se = pd.DataFrame(np.exp(lower_2se), columns=[<span class="hljs-string">&apos;lower_2se_rc&apos;</span>,<span class="hljs-string">&apos;lower_2se_rdy&apos;</span>,<span class="hljs-string">&apos;lower_2se_rnw&apos;</span>], index = test.index)</span><br><span class="line">pred_upper_2se = pd.DataFrame(np.exp(upper_2se), columns=[<span class="hljs-string">&apos;upper_2se_rc&apos;</span>,<span class="hljs-string">&apos;upper_2se_rdy&apos;</span>,<span class="hljs-string">&apos;upper_2se_rnw&apos;</span>], index = test.index)</span><br><span class="line">ptest = pd.concat([test, pred_mid, pred_lower_1se, pred_upper_1se, pred_lower_2se, pred_upper_2se],axis = <span class="hljs-number">1</span>)</span><br></pre></td></tr></tbody></table></figure>

<p>The saving rate (percentage points) can be derived from total nominal consumption and<br>nominal household disposable income using the following formula:</p>
<p>$saving\ rate=100*(rdy - (rc + (government\ transfers + interest\ payments/gdp\ deflator))) /rdy$</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ptest[<span class="hljs-string">&apos;pred_saving_rate&apos;</span>]=<span class="hljs-number">100</span>*(ptest.pred_rdy-(ptest.pred_rc+(ptest.gov_transfers+ptest.interest_payments/ptest.c_deflator)))/ptest.pred_rdy</span><br></pre></td></tr></tbody></table></figure>

<p>Now let&#x2019;s see how the model predicts,</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">plot_point_prediction</span>(<span class="hljs-params">test</span>):</span></span><br><span class="line">    fig, axes = plt.subplots(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>, figsize=(<span class="hljs-number">12</span>,<span class="hljs-number">8</span>))</span><br><span class="line">    count=<span class="hljs-number">0</span></span><br><span class="line">    <span class="hljs-keyword">for</span> endog <span class="hljs-keyword">in</span> [<span class="hljs-string">&apos;rc&apos;</span>,<span class="hljs-string">&apos;rdy&apos;</span>,<span class="hljs-string">&apos;rnw&apos;</span>,<span class="hljs-string">&apos;saving_rate&apos;</span>]:</span><br><span class="line">        col=count %<span class="hljs-number">2</span></span><br><span class="line">        row=count //<span class="hljs-number">2</span></span><br><span class="line">        ax=axes[row][col]</span><br><span class="line">        ax.set_title(endog)</span><br><span class="line">        ax.plot(test[endog], color=<span class="hljs-string">&apos;Black&apos;</span>,linewidth=<span class="hljs-number">1</span>, label=<span class="hljs-string">&quot;Observed&quot;</span>)</span><br><span class="line">        ax.plot(test[<span class="hljs-string">&apos;pred_&apos;</span>+endog],color=<span class="hljs-string">&apos;grey&apos;</span>,linestyle=<span class="hljs-string">&apos;--&apos;</span>,linewidth=<span class="hljs-number">1</span>, label=<span class="hljs-string">&quot;Forecast&quot;</span>)  </span><br><span class="line">        ax.legend()</span><br><span class="line">        count+=<span class="hljs-number">1</span></span><br><span class="line">    fig.autofmt_xdate()</span><br></pre></td></tr></tbody></table></figure>


<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">plot_point_prediction(ptest)</span><br></pre></td></tr></tbody></table></figure>

<img src="http://yumeng-li.github.io/VECM_files/VECM_87_0.png">



<p>The results are not so bad. Actually pretty dope pre-pandemic. For a model that is trying to predict for consumption three years out, its predictive accuracy is out-of-expectation.</p>
<p>Though not surprisingly, the forecasts are over predicting actual real disposable income, real net wealth and therefore real consumption expenditures. This reflects the I(1) nature of the variables involved, the limited serial correlation in their innovation sequences, and the fact that all variables were trending upwards before the pandemic. Because of their I(1) nature, a dynamic forecast will effectively use the last known value (four years ago) of these variables to generate out-of-sample forecasts. Consequently, the forecasting model does not do good job of predicting the impact of covid19 on real consumption, and its predictions for the saving ratio appear to be systematically below the actual saving outcome.</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">plot_forecast_with_confidence_intervals</span>(<span class="hljs-params">df,test</span>):</span></span><br><span class="line">    fig, axes = plt.subplots(<span class="hljs-number">3</span>, <span class="hljs-number">1</span>, figsize=(<span class="hljs-number">10</span>,<span class="hljs-number">10</span>))</span><br><span class="line">    count=<span class="hljs-number">0</span></span><br><span class="line">    <span class="hljs-keyword">for</span> endog <span class="hljs-keyword">in</span> [<span class="hljs-string">&apos;rc&apos;</span>,<span class="hljs-string">&apos;rdy&apos;</span>,<span class="hljs-string">&apos;rnw&apos;</span>]:</span><br><span class="line">        col=count</span><br><span class="line">        ax=axes[col]</span><br><span class="line">        ax.set_title(endog)</span><br><span class="line">        ax.plot(df[endog], color=<span class="hljs-string">&apos;Black&apos;</span>,linewidth=<span class="hljs-number">1.2</span>,label=<span class="hljs-string">&quot;Observed&quot;</span>)</span><br><span class="line">        ax.plot(test[<span class="hljs-string">&apos;pred_&apos;</span>+endog],<span class="hljs-string">&apos;k--&apos;</span>,linewidth=<span class="hljs-number">0.7</span>,label=<span class="hljs-string">&quot;Forecast&quot;</span>)</span><br><span class="line">        ax.fill_between(test[<span class="hljs-string">&apos;pred_&apos;</span>+endog].index,test[<span class="hljs-string">&apos;lower_1se_&apos;</span>+endog],test[<span class="hljs-string">&apos;upper_1se_&apos;</span>+endog],alpha=<span class="hljs-number">0.07</span>,color=<span class="hljs-string">&quot;k&quot;</span>)</span><br><span class="line">        ax.fill_between(test[<span class="hljs-string">&apos;pred_&apos;</span>+endog].index,test[<span class="hljs-string">&apos;lower_2se_&apos;</span>+endog],test[<span class="hljs-string">&apos;upper_2se_&apos;</span>+endog],alpha=<span class="hljs-number">0.07</span>,color=<span class="hljs-string">&quot;k&quot;</span>)      </span><br><span class="line">        ax.legend(loc =<span class="hljs-string">&apos;upper left&apos;</span>)</span><br><span class="line">        count+=<span class="hljs-number">1</span></span><br></pre></td></tr></tbody></table></figure>

<p>With that being said, the previous assessment considers only a single point prediction using the baseline model. A fairer assessment of the model can be made considering the <strong>confidential intervals for prediction</strong>, which suggest that the underlying model is useful for predicting the outcome of the consumption and net wealth.</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">plot_forecast_with_confidence_intervals(df.iloc[<span class="hljs-number">200</span>:],ptest)</span><br></pre></td></tr></tbody></table></figure>

<img src="http://yumeng-li.github.io/VECM_files/VECM_91_0.png">



<p>While there is obvious (downward) bias in the model&#x2019;s predictions, it should be noted that this partly a reflection of the initial starting point of the forecast. If one begins the forecast after the pandemic, the resulting confidential intervals would be much closer to what observed. In this way, a <strong>static forecast</strong> which is <strong>one-period ahead forecasting</strong> would do a much better job. This is also how the model should be used in practice. As always, predicting future is difficult, trying to predict mutiple years out is meaningless.</p>
<h3 id="Long-run-Forecasts-Based-on-Potential-Paths-of-Monetary-Policy"><a href="#Long-run-Forecasts-Based-on-Potential-Paths-of-Monetary-Policy" class="headerlink" title="Long-run Forecasts Based on Potential Paths of Monetary Policy"></a>Long-run Forecasts Based on Potential Paths of Monetary Policy</h3><p>Now it has come to a very interesting point. Let&#x2019;s fit a VECM with the complete data from 1962 to 2021Q2, and use it to forecast for 2021Q3 onwards using two simulated forward paths of the Fed&#x2019;s monetary policy. Correct assumptions would help us to get more accurate estimates for the long-run consumption, disposal income and net wealth of the American households after the pandemic.</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">endog = [<span class="hljs-string">&apos;ln_rc&apos;</span>,<span class="hljs-string">&apos;ln_rdy&apos;</span>,<span class="hljs-string">&apos;ln_rnw&apos;</span>]</span><br><span class="line">exog = [<span class="hljs-string">&apos;sb_1975_4&apos;</span>,<span class="hljs-string">&apos;sb_2008_3&apos;</span>,<span class="hljs-string">&apos;d_unemp&apos;</span>,<span class="hljs-string">&apos;ln_consumer_confidence&apos;</span>, <span class="hljs-string">&apos;d_ln_m2&apos;</span>,<span class="hljs-string">&apos;d_dff&apos;</span>]</span><br><span class="line">vec = vecm.VECM(endog = df[endog],</span><br><span class="line">                exog =df[exog],</span><br><span class="line">                k_ar_diff = <span class="hljs-number">2</span>, coint_rank = <span class="hljs-number">1</span>, deterministic = <span class="hljs-string">&quot;co&quot;</span>, missing= <span class="hljs-string">&apos;drop&apos;</span>)</span><br><span class="line">vecm_fit = vec.fit()</span><br></pre></td></tr></tbody></table></figure>

<p>Let&#x2019;s generate two paths for the forward monetary shocks. One path assumes that fed keeps its current pacing of assets purchasing i.e., no tapering, the other path assumes four interest rate hikes in 2022, 2023 and 2024 assuming the same pace as the period after the financial crisis from 2014Q4 to 2017Q3, with all the other exogenous variables being the same, specifically, current value for unemployment rate and the historical average for consumer confidence.</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">forecast_range = pd.date_range(start=<span class="hljs-string">&apos;04/01/2021&apos;</span>, end =<span class="hljs-string">&apos;01/01/2024&apos;</span>,freq=<span class="hljs-string">&apos;QS&apos;</span> )</span><br><span class="line">shocks = {}</span><br><span class="line">shocks = pd.DataFrame(shocks, index = forecast_range, columns = exog)</span><br><span class="line">shocks[<span class="hljs-string">&apos;sb_1975_4&apos;</span>] = <span class="hljs-number">1</span></span><br><span class="line">shocks[<span class="hljs-string">&apos;sb_2008_3&apos;</span>] = <span class="hljs-number">1</span></span><br><span class="line">shocks[<span class="hljs-string">&apos;d_unemp&apos;</span>] = <span class="hljs-number">0</span></span><br><span class="line">shocks[<span class="hljs-string">&apos;ln_consumer_confidence&apos;</span>] = df[<span class="hljs-string">&apos;ln_consumer_confidence&apos;</span>].mean()</span><br><span class="line"></span><br><span class="line">no_hikes_shocks = shocks.copy()</span><br><span class="line">no_hikes_shocks[<span class="hljs-string">&apos;d_ln_m2&apos;</span>] = df.iloc[<span class="hljs-number">-4</span>:][<span class="hljs-string">&apos;d_ln_m2&apos;</span>].mean()</span><br><span class="line">no_hikes_shocks[<span class="hljs-string">&apos;d_dff&apos;</span>] = <span class="hljs-number">0.0</span></span><br><span class="line"></span><br><span class="line">four_hikes_shocks =shocks.copy()</span><br><span class="line">four_hikes_shocks[[<span class="hljs-string">&apos;d_ln_m2&apos;</span>,<span class="hljs-string">&apos;d_dff&apos;</span>]] = df.loc[<span class="hljs-string">&apos;2014-10-01&apos;</span>:<span class="hljs-string">&apos;2017-07-01&apos;</span>,[<span class="hljs-string">&apos;d_ln_m2&apos;</span>,<span class="hljs-string">&apos;d_dff&apos;</span>]].values</span><br></pre></td></tr></tbody></table></figure>

<p>Here&#x2019;s how the money aggregates is going to be like if assuming no tapering until 2024, which would be of course totally insane&#x2026;</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-comment"># change of log(real m2) and change of fed fund rate</span></span><br><span class="line">no_hikes_shocks[[<span class="hljs-string">&apos;d_ln_m2&apos;</span>,<span class="hljs-string">&apos;d_dff&apos;</span>]]</span><br></pre></td></tr></tbody></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;</code></pre>
<p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>d_ln_m2</th>
      <th>d_dff</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>2021-04-01</th>
      <td>0.02268</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>2021-07-01</th>
      <td>0.02268</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>2021-10-01</th>
      <td>0.02268</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>2022-01-01</th>
      <td>0.02268</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>2022-04-01</th>
      <td>0.02268</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>2022-07-01</th>
      <td>0.02268</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>2022-10-01</th>
      <td>0.02268</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>2023-01-01</th>
      <td>0.02268</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>2023-04-01</th>
      <td>0.02268</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>2023-07-01</th>
      <td>0.02268</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>2023-10-01</th>
      <td>0.02268</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>2024-01-01</th>
      <td>0.02268</td>
      <td>0.0</td>
    </tr>
  </tbody>
</table>
</div>



<p>Here&#x2019;s a more likely path assumes that the fed would stop expanding its balance sheet from the forth quarter of 2021 and hike once in 2022 and three times at the end 2023 and beginning of 2024. This path is quite aggressive.</p>
<p>(By the way, when central banks expend their balance sheets i.e., increase their net asset position, they literally buy assets or reduce their liability through money creation, which means writing checks to themselves.)</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-comment"># hike schedule - historical simulation using 2014-2017 data</span></span><br><span class="line"><span class="hljs-comment">## change of log(real m2) and change of fed fund rate</span></span><br><span class="line">four_hikes_shocks[[<span class="hljs-string">&apos;d_ln_m2&apos;</span>,<span class="hljs-string">&apos;d_dff&apos;</span>]]</span><br></pre></td></tr></tbody></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;</code></pre>
<p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>d_ln_m2</th>
      <th>d_dff</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>2021-04-01</th>
      <td>0.015029</td>
      <td>0.01</td>
    </tr>
    <tr>
      <th>2021-07-01</th>
      <td>0.025704</td>
      <td>0.01</td>
    </tr>
    <tr>
      <th>2021-10-01</th>
      <td>0.003989</td>
      <td>0.02</td>
    </tr>
    <tr>
      <th>2022-01-01</th>
      <td>0.007853</td>
      <td>0.00</td>
    </tr>
    <tr>
      <th>2022-04-01</th>
      <td>0.013799</td>
      <td>0.03</td>
    </tr>
    <tr>
      <th>2022-07-01</th>
      <td>0.022938</td>
      <td>0.20</td>
    </tr>
    <tr>
      <th>2022-10-01</th>
      <td>0.009184</td>
      <td>0.01</td>
    </tr>
    <tr>
      <th>2023-01-01</th>
      <td>0.011144</td>
      <td>0.02</td>
    </tr>
    <tr>
      <th>2023-04-01</th>
      <td>0.009073</td>
      <td>0.06</td>
    </tr>
    <tr>
      <th>2023-07-01</th>
      <td>0.008134</td>
      <td>0.25</td>
    </tr>
    <tr>
      <th>2023-10-01</th>
      <td>0.010906</td>
      <td>0.25</td>
    </tr>
    <tr>
      <th>2024-01-01</th>
      <td>0.005544</td>
      <td>0.20</td>
    </tr>
  </tbody>
</table>
</div>




<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">mid_no_hikes = vecm_fit.predict(steps=<span class="hljs-number">12</span>,exog_fc =no_hikes_shocks[exog])</span><br><span class="line">pred_mid_no_hikes = pd.DataFrame(np.exp(mid_no_hikes), columns=[<span class="hljs-string">&apos;pred_rc&apos;</span>,<span class="hljs-string">&apos;pred_rdy&apos;</span>,<span class="hljs-string">&apos;pred_rnw&apos;</span>], index = forecast_range)</span><br><span class="line"></span><br><span class="line">mid, lower_1se, upper_1se = vecm_fit.predict(steps=<span class="hljs-number">12</span>,alpha = <span class="hljs-number">0.32</span>, exog_fc =four_hikes_shocks[exog])</span><br><span class="line">_, lower_2se, upper_2se = vecm_fit.predict(steps=<span class="hljs-number">12</span>,alpha = <span class="hljs-number">0.05</span>, exog_fc =four_hikes_shocks[exog])</span><br><span class="line"></span><br><span class="line">pred_mid = pd.DataFrame(np.exp(mid), columns=[<span class="hljs-string">&apos;pred_rc&apos;</span>,<span class="hljs-string">&apos;pred_rdy&apos;</span>,<span class="hljs-string">&apos;pred_rnw&apos;</span>], index = forecast_range)</span><br><span class="line">pred_lower_1se = pd.DataFrame(np.exp(lower_1se), columns=[<span class="hljs-string">&apos;lower_1se_rc&apos;</span>,<span class="hljs-string">&apos;lower_1se_rdy&apos;</span>,<span class="hljs-string">&apos;lower_1se_rnw&apos;</span>], index = forecast_range)</span><br><span class="line">pred_upper_1se = pd.DataFrame(np.exp(upper_1se), columns=[<span class="hljs-string">&apos;upper_1se_rc&apos;</span>,<span class="hljs-string">&apos;upper_1se_rdy&apos;</span>,<span class="hljs-string">&apos;upper_1se_rnw&apos;</span>], index = forecast_range)</span><br><span class="line">pred_lower_2se = pd.DataFrame(np.exp(lower_2se), columns=[<span class="hljs-string">&apos;lower_2se_rc&apos;</span>,<span class="hljs-string">&apos;lower_2se_rdy&apos;</span>,<span class="hljs-string">&apos;lower_2se_rnw&apos;</span>], index = forecast_range)</span><br><span class="line">pred_upper_2se = pd.DataFrame(np.exp(upper_2se), columns=[<span class="hljs-string">&apos;upper_2se_rc&apos;</span>,<span class="hljs-string">&apos;upper_2se_rdy&apos;</span>,<span class="hljs-string">&apos;upper_2se_rnw&apos;</span>], index = forecast_range)</span><br><span class="line">pred = pd.concat([pred_mid, pred_lower_1se, pred_upper_1se, pred_lower_2se, pred_upper_2se],axis = <span class="hljs-number">1</span>)</span><br></pre></td></tr></tbody></table></figure>


<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">plot_forecasts_given_different_paths</span>(<span class="hljs-params">df,pred1,pred2</span>):</span></span><br><span class="line">    fig, axes = plt.subplots(<span class="hljs-number">3</span>, <span class="hljs-number">1</span>, figsize=(<span class="hljs-number">10</span>,<span class="hljs-number">10</span>))</span><br><span class="line">    count=<span class="hljs-number">0</span></span><br><span class="line">    <span class="hljs-keyword">for</span> endog <span class="hljs-keyword">in</span> [<span class="hljs-string">&apos;rc&apos;</span>,<span class="hljs-string">&apos;rdy&apos;</span>,<span class="hljs-string">&apos;rnw&apos;</span>]:</span><br><span class="line">        col=count</span><br><span class="line">        ax=axes[col]</span><br><span class="line">        ax.set_title(endog)</span><br><span class="line">        ax.plot(df[endog], color=<span class="hljs-string">&apos;Black&apos;</span>,linewidth=<span class="hljs-number">1.2</span>,label=<span class="hljs-string">&quot;Observed&quot;</span>)</span><br><span class="line">        ax.plot(pred1[<span class="hljs-string">&apos;pred_&apos;</span>+endog],<span class="hljs-string">&apos;--&apos;</span>,color=<span class="hljs-string">&apos;tab:blue&apos;</span>, linewidth=<span class="hljs-number">1.5</span>,label=<span class="hljs-string">&quot;Forecast - no hikes&quot;</span>)</span><br><span class="line">        ax.plot(pred2[<span class="hljs-string">&apos;pred_&apos;</span>+endog],<span class="hljs-string">&apos;--&apos;</span>,color=<span class="hljs-string">&apos;tab:red&apos;</span>, linewidth=<span class="hljs-number">1.5</span>,label=<span class="hljs-string">&quot;Forecast - four hikes&quot;</span>)</span><br><span class="line">        ax.fill_between(pred2[<span class="hljs-string">&apos;pred_&apos;</span>+endog].index,pred2[<span class="hljs-string">&apos;lower_1se_&apos;</span>+endog],pred2[<span class="hljs-string">&apos;upper_1se_&apos;</span>+endog],alpha=<span class="hljs-number">0.07</span>,color=<span class="hljs-string">&quot;tab:red&quot;</span>)</span><br><span class="line">        ax.fill_between(pred2[<span class="hljs-string">&apos;pred_&apos;</span>+endog].index,pred2[<span class="hljs-string">&apos;lower_2se_&apos;</span>+endog],pred2[<span class="hljs-string">&apos;upper_2se_&apos;</span>+endog],alpha=<span class="hljs-number">0.07</span>,color=<span class="hljs-string">&quot;tab:red&quot;</span>)</span><br><span class="line">        ax.legend(loc =<span class="hljs-string">&apos;upper left&apos;</span>)</span><br><span class="line">        ax.axvline(x=datetime(<span class="hljs-number">2022</span>, <span class="hljs-number">7</span>, <span class="hljs-number">1</span>), color=<span class="hljs-string">&apos;tab:red&apos;</span>)</span><br><span class="line">        ax.axvline(x=datetime(<span class="hljs-number">2023</span>, <span class="hljs-number">7</span>, <span class="hljs-number">1</span>), color=<span class="hljs-string">&apos;tab:red&apos;</span>)</span><br><span class="line">        ax.axvline(x=datetime(<span class="hljs-number">2023</span>, <span class="hljs-number">10</span>, <span class="hljs-number">1</span>), color=<span class="hljs-string">&apos;tab:red&apos;</span>)</span><br><span class="line">        ax.axvline(x=datetime(<span class="hljs-number">2024</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>), color=<span class="hljs-string">&apos;tab:red&apos;</span>)</span><br><span class="line">        count+=<span class="hljs-number">1</span></span><br></pre></td></tr></tbody></table></figure>

<p>In March 2020, in response to the economic impact of the pandemic, the Fed acted quickly having learned its lesson from the financial crisis and cut short-term interest rates to zero. It also restarted its large-scale asset purchases and since June 2020, the Fed has been buying 80 billion of Treasury securities and 40 billion of mortgage-backed securities each month, with its balance sheet swelling from 4.4 trillion to 8.6 trillion. The extreme monetary support combined with a huge fiscal stimulus package holds the U.S economy and shields American household incomes. Though plots won&#x2019;t be shown here, the model forecasts a much lower rdy and rnw if not taking into account the rapid increase of money base. Therefore, in the last 20 months, we all see the quick recovery of consumption and a rapid growth of asset prices, which is also reflected in the steeper slope of the third graph. However, as the economy rebounded, it may no longer need such extreme measures of support and keeping them in place could do more harm than good. For example, low mortgage rates have fueled a boom in house prices, but the problems that now afflict the economy are mostly supply issues while demand, which the bond buys most directly affects. Therefore, the policy makers may start to hit the brakes. That&#x2019;s also why we are seeing Fed officials talking about tapering the pace of its bond purchases and lately laying the groundwork for higher rates.</p>
<h3 id="1-Growth-rates-return-to-normal"><a href="#1-Growth-rates-return-to-normal" class="headerlink" title="1. Growth rates return to normal"></a>1. Growth rates return to normal</h3><p><strong>First it&#x2019;s not surprising that real consumption, disposable income and household net wealth are projected to grow at milder rates comparing to those when the economy was bottoming out from the slump. As the stimulus aids wear off over time, one would expect the growth rates to normalize to pre-pandemic levels.</strong></p>
<h3 id="2-Growth-differences-taking-expansionary-and-tight-monetary-policy"><a href="#2-Growth-differences-taking-expansionary-and-tight-monetary-policy" class="headerlink" title="2. Growth differences taking expansionary and tight monetary policy"></a>2. Growth differences taking expansionary and tight monetary policy</h3><p><strong>Secondly, assuming a tighter monetary policy to come, we project that the growth rate of real consumption throughout 2023 would slow down to 2.2% from 3.0%. The real income and net wealth of households would decrease from 3.9% and 9.1% to 2.4% and 6.5% respectively. At the end of 2023, all three measures would be lower than those come from the first path - that the Fed sticks to its current rate target and asset buying pace. In fact the amounts would be lowered by 1.8%, 3.3% and 4.8% respectively.</strong></p>
<h3 id="3-Foreseeable-shallow-decline-in-mid-2022"><a href="#3-Foreseeable-shallow-decline-in-mid-2022" class="headerlink" title="3. Foreseeable shallow decline in mid 2022"></a>3. Foreseeable shallow decline in mid 2022</h3><p><strong>Thirdly, we also notice that in the middle of 2022, the private consumption and household income would show a temporary and shallow decline and then grow at an annual rate again coming close to the path observed just before the pandemic.</strong> Likely this is caused by the tightening of policy in 2022. So far, Powell has announced that the fed will begin tapering this month, the first step toward pulling back on the massive amount of help it had been providing markets and the economy. One the fiscal side, though public programs to support households and businesses have been the most powerful engine of recovery from the hit, governments of the major economies now are hitting the brakes. The money they&#x2019; ll pull out of their economies in 2022 amounts to 2.5 percentage points of the world&#x2019;s GDP, five times bigger than anything that happened during the turn to austerity after the 2008 crisis, according to UBS estimates. The fiscal tightening - the withdrawal of pandemic spending will likely have more impact on the global economy next year.</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">plot_forecasts_given_different_paths(df.iloc[<span class="hljs-number">220</span>:],pred_mid_no_hikes,pred)</span><br></pre></td></tr></tbody></table></figure>

<img src="http://yumeng-li.github.io/VECM_files/VECM_105_0.png">

</body></html>
    
    </div>
    
    
</article>




    
        <article class="article content gallery" itemscope itemprop="blogPost">
    <h1 class="article-title is-size-3 is-size-4-mobile" itemprop="name">
        
            <a href="/2021/11/16/VECM/" itemprop="url">Predicting Post-Pandemic Consumption Behaviors Given Potential Paths of Monetary Policy, Part 1 -- Economic Theories, Structural Breaks and Stationarity</a>
        
    </h1>
    <div class="article-meta columns is-variable is-1 is-multiline is-mobile is-size-7-mobile">
        <span class="column is-narrow">
            <time datetime="2021-11-16T17:00:00.000Z" itemprop="datePublished">Nov 16 2021</time>
        </span>
        
        <span class="column is-narrow article-category">
            <i class="far fa-folder"></i>
            <a class="article-category-link" href="/categories/Time-Series-Analysis/">Time Series Analysis</a>
        </span>
        
        
        <span class="column is-narrow">
            
            
            29 minutes read (About 4355 words)
        </span>
        
    </div>
    <div class="article-entry is-size-6-mobile" itemprop="articleBody">
    
        <html><head></head><body><p>It&#x2019;s official on Nov.3rd that the Federal Reserve is winding down its aggressive pandemic-era asset purchasing, or as what we call &#x201C;tapering&#x201D;. The announcement actually didn&#x2019;t come out as a surprise since Fed officials had been laying the groundwork for policy tightening as the economy gradually recovered from the Covid19 slump. Furthermore, due to the lately surging inflation readings, we are seeing traders start to bet on faster Fed hikes. So it seems interesting to get some ideas of how the post-pandemic consumption behaviors of the American households would be, and quantify the potential impact of policy shifting on these behaviors using two classic time series models - the <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Vector_autoregression">Vector Autoregression (VAR)</a> and the <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Error_correction_model#VECM">Vector Error Correction Model (VECM)</a>. Tribute to Christopher Sims who first promoted the use of VAR in empirical macroeconomics. </p>
<p>We will build the models following the steps below, </p>
<img src="http://yumeng-li.github.io/VECM_files/flowchart.png">

<p>And before we get onto that, I will introduce some of the economic theories behind predicting real consumption.</p>
<h3 id="Economic-Intuitions-Behind"><a href="#Economic-Intuitions-Behind" class="headerlink" title="Economic Intuitions Behind"></a>Economic Intuitions Behind</h3><p>Let&#x2019;s first recap some of the main determinants of private consumption,</p>
<ul>
<li>Disposable Income (in the current period) (+)</li>
<li>Expectations (consumer confidence indexes, employment growth) (+)</li>
<li>Wealth (stock market performance, housing prices) (+)</li>
<li>Uncertainty (precautionary saving) (-)</li>
<li>Availability of Credit (+)</li>
<li>Real Interest Rate (?)<br>&#xA0; &#xA0; - Substitution effect (-)<br>&#xA0; &#xA0; - Income effect (+)</li>
</ul>
<p>You might remember that perhaps the simplest theories of consumption, Keynesian theories of consumption, would link private consumption to <strong>disposable income</strong> in the current period, and of course positively. So as disposable income goes up, private consumption is expected to go up as well. More sophisticated, forward looking models would give more prominence or link consumption not to disposable income today but to <strong>expectations</strong> about the evolution of disposable income in the future. A number of indicators that can be used to gauge these expectations, for example, consumer confidence index, employment growth or my personally preferred - yield spreads. These can be used as a proxy for expectations for future income.</p>
<p>Another variable thought to be important by economists in terms of explaining the behavior of private consumption is <strong>wealth</strong>. In that sense, for this wealth channel, the performance of the stock market or even the behavior of housing prices, would have an impact on private consumption.</p>
<p>Another important factor is <strong>uncertainty</strong>, which would actually have a negative association with private consumption because it would create incentives for what the economists called precautionary saving. So agents will try to create a buffer to be able to fall back on in case a bad shock hits.</p>
<p><strong>The availability of credit</strong> is also thought to be an important determinant with, obviously, a positive relationship.</p>
<p>And finally, the <strong>real interest rate</strong> is also thought to be one of the important determinants of private consumption.<br>The effect of real interest on consumption can be a little bit ambiguous. Perhaps the most straightforward or direct effect<br>would be what economists call a substitution effect where, as interest rates go up, the incentives to consume are reduced. So agents will have, on the contrary, incentives to save more. And therefore, an increase in rates will be linked to a decrease in private consumption. But on the other hand, it is also possible that if consumers are net savers the increase in interest rates<br>might generate some additional income&#x2013; what economists call an income effect&#x2013;and this might actually lead to an increase in private consumption.</p>
<h2 id="Getting-Data"><a href="#Getting-Data" class="headerlink" title="Getting Data"></a>Getting Data</h2><p>Based on the economic intuitions, I grabbed the following well-defined and organized economic data from the <a target="_blank" rel="noopener" href="https://fred.stlouisfed.org/">Federal Reserve Economic Data</a> of the St. Louis Fed. The FRED allows modelers to write programs and build applications that retrieve data through an API provided. There is a list of <a target="_blank" rel="noopener" href="https://fred.stlouisfed.org/docs/api/fred/">packages/libraries</a> that one can turn to, depending on one&#x2019;s programming language. Since I have to model everything in Python as it&#x2019;s compatible with the github&#xA0;blog, I use the library <a target="_blank" rel="noopener" href="https://github.com/mortada/fredapi">fredapi</a> to request the data. However, in practice, I would probably go for EViews as most economists prefer and do. </p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd</span><br><span class="line"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np</span><br><span class="line"><span class="hljs-keyword">from</span> fredapi <span class="hljs-keyword">import</span> Fred</span><br><span class="line"><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt</span><br><span class="line"><span class="hljs-keyword">from</span> datetime <span class="hljs-keyword">import</span> datetime</span><br><span class="line">%matplotlib inline</span><br><span class="line"></span><br><span class="line"><span class="hljs-keyword">import</span> statsmodels.api <span class="hljs-keyword">as</span> sm</span><br><span class="line"><span class="hljs-keyword">from</span> statsmodels.tsa.stattools <span class="hljs-keyword">import</span> adfuller</span><br><span class="line"><span class="hljs-keyword">from</span> statsmodels.tsa.stattools <span class="hljs-keyword">import</span> kpss</span><br><span class="line"><span class="hljs-keyword">from</span> statsmodels.tsa.api <span class="hljs-keyword">import</span> VAR</span><br><span class="line"><span class="hljs-keyword">from</span> statsmodels.tsa.vector_ar <span class="hljs-keyword">import</span> vecm</span><br><span class="line"><span class="hljs-keyword">from</span> statsmodels.tsa <span class="hljs-keyword">import</span> filters</span><br><span class="line"></span><br><span class="line">pd.options.display.max_colwidth = <span class="hljs-number">50</span></span><br><span class="line">pd.options.display.max_rows = <span class="hljs-number">400</span></span><br></pre></td></tr></tbody></table></figure>


<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">fred = Fred(api_key=your_key_string)</span><br></pre></td></tr></tbody></table></figure>

<p>The period I study is from 1962Q1 to 2021Q2, which ensures that the economic data that I am interested in all has a value. </p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">startDate = <span class="hljs-string">&apos;1962-01-01&apos;</span></span><br><span class="line">endDate = <span class="hljs-string">&apos;2021-04-01&apos;</span></span><br></pre></td></tr></tbody></table></figure>


<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">get_series_and_print_info</span>(<span class="hljs-params">fred, series_id, startDate, endDate, name</span>):</span></span><br><span class="line">&#xA0; &#xA0; series = fred.get_series(series_id, observation_start=startDate, observation_end=endDate, frequency=<span class="hljs-string">&apos;q&apos;</span>)</span><br><span class="line">&#xA0; &#xA0; print(name+<span class="hljs-string">&apos;: &apos;</span>+fred.get_series_info(series_id).title)</span><br><span class="line">&#xA0; &#xA0; <span class="hljs-keyword">return</span> series</span><br></pre></td></tr></tbody></table></figure>


<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">data = {}</span><br><span class="line">data[<span class="hljs-string">&apos;saving_rate&apos;</span>] = get_series_and_print_info(fred, <span class="hljs-string">&apos;a072rc1q156sbea&apos;</span>,startDate, endDate, <span class="hljs-string">&apos;saving_rate&apos;</span>)</span><br><span class="line">data[<span class="hljs-string">&apos;ns&apos;</span>] = get_series_and_print_info(fred, <span class="hljs-string">&apos;w986rc1q027sbea&apos;</span>,startDate, endDate, <span class="hljs-string">&apos;ns&apos;</span>)</span><br><span class="line">data[<span class="hljs-string">&apos;nnw&apos;</span>] = get_series_and_print_info(fred, <span class="hljs-string">&apos;TNWBSHNO&apos;</span>,startDate, endDate, <span class="hljs-string">&apos;nnw&apos;</span>)</span><br><span class="line">data[<span class="hljs-string">&apos;nl&apos;</span>] = get_series_and_print_info(fred, <span class="hljs-string">&apos;TCMILBSHNO&apos;</span>,startDate, endDate, <span class="hljs-string">&apos;nl&apos;</span>)</span><br><span class="line">data[<span class="hljs-string">&apos;na&apos;</span>] = get_series_and_print_info(fred, <span class="hljs-string">&apos;TABSHNO&apos;</span>,startDate, endDate, <span class="hljs-string">&apos;na&apos;</span>)</span><br><span class="line">data[<span class="hljs-string">&apos;nfa&apos;</span>] = get_series_and_print_info(fred, <span class="hljs-string">&apos;TFAABSHNO&apos;</span>,startDate, endDate, <span class="hljs-string">&apos;nfa&apos;</span>)</span><br><span class="line">data[<span class="hljs-string">&apos;interest_payments&apos;</span>] = get_series_and_print_info(fred, <span class="hljs-string">&apos;b069rc1q027sbea&apos;</span>,startDate, endDate, <span class="hljs-string">&apos;interest_payments&apos;</span>)</span><br><span class="line">data[<span class="hljs-string">&apos;cpi&apos;</span>] = get_series_and_print_info(fred, <span class="hljs-string">&apos;cpiaucsl&apos;</span>,startDate, endDate, <span class="hljs-string">&apos;cpi&apos;</span>)</span><br><span class="line">data[<span class="hljs-string">&apos;c_deflator&apos;</span>] = get_series_and_print_info(fred, <span class="hljs-string">&apos;dpcerd3q086sbea&apos;</span>,startDate, endDate, <span class="hljs-string">&apos;c_deflator&apos;</span>)/<span class="hljs-number">100</span></span><br><span class="line">data[<span class="hljs-string">&apos;dff&apos;</span>] = get_series_and_print_info(fred, <span class="hljs-string">&apos;DFF&apos;</span>,startDate, endDate, <span class="hljs-string">&apos;dff&apos;</span>)</span><br><span class="line">data[<span class="hljs-string">&apos;yield_3m&apos;</span>] = get_series_and_print_info(fred, <span class="hljs-string">&apos;dgs3&apos;</span>,startDate, endDate, <span class="hljs-string">&apos;yield_3m&apos;</span>)</span><br><span class="line">data[<span class="hljs-string">&apos;yield_10y&apos;</span>] = get_series_and_print_info(fred, <span class="hljs-string">&apos;irltlt01usq156n&apos;</span>,startDate, endDate, <span class="hljs-string">&apos;yield_10y&apos;</span>)</span><br><span class="line">data[<span class="hljs-string">&apos;prime_rate&apos;</span>] = get_series_and_print_info(fred, <span class="hljs-string">&apos;dprime&apos;</span>,startDate, endDate, <span class="hljs-string">&apos;prime_rate&apos;</span>)</span><br><span class="line">data[<span class="hljs-string">&apos;m2&apos;</span>] = get_series_and_print_info(fred, <span class="hljs-string">&apos;M2REAL&apos;</span>,startDate, endDate, <span class="hljs-string">&apos;m2&apos;</span>)</span><br><span class="line">data[<span class="hljs-string">&apos;ndy&apos;</span>] = get_series_and_print_info(fred, <span class="hljs-string">&apos;dpi&apos;</span>,startDate, endDate, <span class="hljs-string">&apos;ndy&apos;</span>)</span><br><span class="line">data[<span class="hljs-string">&apos;rdy&apos;</span>] = get_series_and_print_info(fred, <span class="hljs-string">&apos;dpic96&apos;</span>,startDate, endDate, <span class="hljs-string">&apos;rdy&apos;</span>)</span><br><span class="line">data[<span class="hljs-string">&apos;npiy&apos;</span>] = get_series_and_print_info(fred, <span class="hljs-string">&apos;b703rc1q027sbea&apos;</span>,startDate, endDate, <span class="hljs-string">&apos;npiy&apos;</span>)</span><br><span class="line">data[<span class="hljs-string">&apos;ny&apos;</span>] = get_series_and_print_info(fred, <span class="hljs-string">&apos;pincome&apos;</span>,startDate, endDate, <span class="hljs-string">&apos;ny&apos;</span>)</span><br><span class="line">data[<span class="hljs-string">&apos;ry&apos;</span>] = get_series_and_print_info(fred, <span class="hljs-string">&apos;rpi&apos;</span>,startDate, endDate, <span class="hljs-string">&apos;ry&apos;</span>)</span><br><span class="line">data[<span class="hljs-string">&apos;nc&apos;</span>] = get_series_and_print_info(fred, <span class="hljs-string">&apos;pcec&apos;</span>,startDate, endDate, <span class="hljs-string">&apos;nc&apos;</span>)</span><br><span class="line">data[<span class="hljs-string">&apos;rc&apos;</span>] = get_series_and_print_info(fred, <span class="hljs-string">&apos;pcecc96&apos;</span>,startDate, endDate, <span class="hljs-string">&apos;rc&apos;</span>)</span><br><span class="line">data[<span class="hljs-string">&apos;nc_services&apos;</span>] = get_series_and_print_info(fred, <span class="hljs-string">&apos;pcesv&apos;</span>,startDate, endDate, <span class="hljs-string">&apos;nc_services&apos;</span>)</span><br><span class="line">data[<span class="hljs-string">&apos;nc_durable_goods&apos;</span>] = get_series_and_print_info(fred, <span class="hljs-string">&apos;pcdg&apos;</span>,startDate, endDate, <span class="hljs-string">&apos;nc_durable_goods&apos;</span>)</span><br><span class="line">data[<span class="hljs-string">&apos;unemp&apos;</span>] = get_series_and_print_info(fred, <span class="hljs-string">&apos;unrate&apos;</span>,startDate, endDate, <span class="hljs-string">&apos;unemp&apos;</span>)</span><br><span class="line">data[<span class="hljs-string">&apos;house_prices&apos;</span>] = get_series_and_print_info(fred, <span class="hljs-string">&apos;ussthpi&apos;</span>,startDate, endDate, <span class="hljs-string">&apos;house_prices&apos;</span>)</span><br><span class="line">data[<span class="hljs-string">&apos;mortgage_30y&apos;</span>] = get_series_and_print_info(fred, <span class="hljs-string">&apos;MORTGAGE30US&apos;</span>,startDate, endDate, <span class="hljs-string">&apos;mortgage_30y&apos;</span>)</span><br><span class="line">data[<span class="hljs-string">&apos;consumer_confidence&apos;</span>] = get_series_and_print_info(fred, <span class="hljs-string">&apos;CSCICP03USM665S&apos;</span>,startDate, endDate, <span class="hljs-string">&apos;consumer_confidence&apos;</span>)</span><br><span class="line">data[<span class="hljs-string">&apos;gov_debt&apos;</span>] = get_series_and_print_info(fred, <span class="hljs-string">&apos;fygfdpun&apos;</span>,startDate, endDate, <span class="hljs-string">&apos;gov_debt&apos;</span>)</span><br><span class="line">data[<span class="hljs-string">&apos;gov_transfers&apos;</span>] = get_series_and_print_info(fred, <span class="hljs-string">&apos;w211rc1q027sbea&apos;</span>,startDate, endDate, <span class="hljs-string">&apos;gov_transfers&apos;</span>)</span><br></pre></td></tr></tbody></table></figure>

<p>&#xA0; &#xA0; saving_rate: Personal saving as a percentage of disposable personal income<br>&#xA0; &#xA0; ns: Net private saving: Households and institutions<br>&#xA0; &#xA0; nnw: Households and Nonprofit Organizations; Net Worth, Level<br>&#xA0; &#xA0; nl: Households and Nonprofit Organizations; Debt Securities and Loans; Liability, Level<br>&#xA0; &#xA0; na: Households and Nonprofit Organizations; Total Assets, Level<br>&#xA0; &#xA0; nfa: Households and Nonprofit Organizations; Total Financial Assets, Level<br>&#xA0; &#xA0; interest_payments: Personal interest payments<br>&#xA0; &#xA0; cpi: Consumer Price Index for All Urban Consumers: All Items in U.S. City Average<br>&#xA0; &#xA0; c_deflator: Personal consumption expenditures (implicit price deflator)<br>&#xA0; &#xA0; dff: Federal Funds Effective Rate<br>&#xA0; &#xA0; yield_3m: Market Yield on U.S. Treasury Securities at 3-Year Constant Maturity<br>&#xA0; &#xA0; yield_10y: Long-Term Government Bond Yields: 10-year: Main (Including Benchmark) for the United States<br>&#xA0; &#xA0; prime_rate: Bank Prime Loan Rate<br>&#xA0; &#xA0; m2: Real M2 Money Stock<br>&#xA0; &#xA0; ndy: Disposable Personal Income<br>&#xA0; &#xA0; rdy: Real Disposable Personal Income<br>&#xA0; &#xA0; npiy: Personal income receipts on assets: Personal dividend income<br>&#xA0; &#xA0; ny: Personal Income<br>&#xA0; &#xA0; ry: Real Personal Income<br>&#xA0; &#xA0; nc: Personal Consumption Expenditures<br>&#xA0; &#xA0; rc: Real Personal Consumption Expenditures<br>&#xA0; &#xA0; nc_services: Personal Consumption Expenditures: Services<br>&#xA0; &#xA0; nc_durable_goods: Personal Consumption Expenditures: Durable Goods<br>&#xA0; &#xA0; unemp: Unemployment Rate<br>&#xA0; &#xA0; house_prices: All-Transactions House Price Index for the United States<br>&#xA0; &#xA0; mortgage_30y: 30-Year Fixed Rate Mortgage Average in the United States<br>&#xA0; &#xA0; consumer_confidence: Consumer Opinion Surveys: Confidence Indicators: Composite Indicators: OECD Indicator for the United States<br>&#xA0; &#xA0; gov_debt: Federal Debt Held by the Public<br>&#xA0; &#xA0; gov_transfers: Personal current transfer payments</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df = pd.DataFrame(data)</span><br></pre></td></tr></tbody></table></figure>

<p>Since we are interested in predicting the <strong>real consumption</strong>, which means eliminating the effect of prices, nominal variables including consumption, disposable income and net wealth need to be converted into real terms. Here they are deflated by the price deflator for consumption (c_deflator). </p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">df[<span class="hljs-string">&apos;rnw&apos;</span>] = df[<span class="hljs-string">&apos;nnw&apos;</span>]/df[<span class="hljs-string">&apos;c_deflator&apos;</span>]</span><br><span class="line">df[<span class="hljs-string">&apos;rl&apos;</span>] = df[<span class="hljs-string">&apos;nl&apos;</span>]/df[<span class="hljs-string">&apos;c_deflator&apos;</span>]</span><br><span class="line">df[<span class="hljs-string">&apos;ra&apos;</span>] = df[<span class="hljs-string">&apos;na&apos;</span>]/df[<span class="hljs-string">&apos;c_deflator&apos;</span>]</span><br><span class="line">df[<span class="hljs-string">&apos;rfa&apos;</span>] = df[<span class="hljs-string">&apos;nfa&apos;</span>]/df[<span class="hljs-string">&apos;c_deflator&apos;</span>]</span><br></pre></td></tr></tbody></table></figure>

<h2 id="Data-Visualization-and-Preliminary-Analysis-Some-Fun-Ideas"><a href="#Data-Visualization-and-Preliminary-Analysis-Some-Fun-Ideas" class="headerlink" title="Data Visualization and Preliminary Analysis (Some Fun Ideas)"></a>Data Visualization and Preliminary Analysis (Some Fun Ideas)</h2><h3 id="Structural-Breaks"><a href="#Structural-Breaks" class="headerlink" title="Structural Breaks"></a>Structural Breaks</h3><p>To get a feel for the data, let&#x2019;s plot log(rc/rdy) against log(rnw/rdy). Also regress log(rc/rdy) on a constant and log(rnw/rdy) and then examine the fitted residuals, looking for evidence of any structural changes in the relationship between consumption, disposable income and net wealth. </p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">fig, ax1 = plt.subplots()</span><br><span class="line">ax1.set_xlabel(<span class="hljs-string">&apos;Year&apos;</span>)</span><br><span class="line">ax1.set_ylabel(<span class="hljs-string">&apos;rdy rl&apos;</span>)</span><br><span class="line">ax1.plot(df[<span class="hljs-string">&apos;rc&apos;</span>], color=<span class="hljs-string">&apos;tab:grey&apos;</span>,label=<span class="hljs-string">&apos;real consumption&apos;</span>)</span><br><span class="line">ax1.plot(df[<span class="hljs-string">&apos;rdy&apos;</span>], color=<span class="hljs-string">&apos;tab:blue&apos;</span>,label=<span class="hljs-string">&apos;real disposable income&apos;</span>)</span><br><span class="line">ax1.plot(df[<span class="hljs-string">&apos;rl&apos;</span>], color=<span class="hljs-string">&apos;tab:green&apos;</span>,label=<span class="hljs-string">&apos;real liability&apos;</span>)</span><br><span class="line">ax1.legend(loc=<span class="hljs-string">&apos;upper left&apos;</span>)</span><br><span class="line">ax2 = ax1.twinx() &#xA0;</span><br><span class="line">ax2.set_ylabel(<span class="hljs-string">&apos;rnw ra rfa&apos;</span>) </span><br><span class="line">ax2.plot(df[<span class="hljs-string">&apos;rnw&apos;</span>], color=<span class="hljs-string">&apos;tab:red&apos;</span>,label=<span class="hljs-string">&apos;real net worth&apos;</span>)</span><br><span class="line">ax2.plot(df[<span class="hljs-string">&apos;ra&apos;</span>], color=<span class="hljs-string">&apos;tab:orange&apos;</span>,label=<span class="hljs-string">&apos;real asset&apos;</span>)</span><br><span class="line">ax2.plot(df[<span class="hljs-string">&apos;rfa&apos;</span>], color=<span class="hljs-string">&apos;tab:purple&apos;</span>,label=<span class="hljs-string">&apos;real fincl asset&apos;</span>)</span><br><span class="line">ax2.legend(loc=<span class="hljs-string">&apos;lower right&apos;</span>)</span><br><span class="line">fig.tight_layout() </span><br><span class="line">plt.show()</span><br></pre></td></tr></tbody></table></figure>

<img src="http://yumeng-li.github.io/VECM_files/VECM_14_0.png">



<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">data1 = df[<span class="hljs-string">&apos;rc&apos;</span>]/df[<span class="hljs-string">&apos;rdy&apos;</span>]</span><br><span class="line">data2 = df[<span class="hljs-string">&apos;rc&apos;</span>]/df[<span class="hljs-string">&apos;rnw&apos;</span>]</span><br><span class="line">data3 = df[<span class="hljs-string">&apos;rc&apos;</span>]/df[<span class="hljs-string">&apos;rl&apos;</span>]</span><br><span class="line">data4 = df[<span class="hljs-string">&apos;rc&apos;</span>]/df[<span class="hljs-string">&apos;ra&apos;</span>]</span><br><span class="line">data5 = df[<span class="hljs-string">&apos;rc&apos;</span>]/df[<span class="hljs-string">&apos;rfa&apos;</span>]</span><br><span class="line">fig, ax1 = plt.subplots()</span><br><span class="line">ax1.set_xlabel(<span class="hljs-string">&apos;Year&apos;</span>)</span><br><span class="line">ax1.set_ylabel(<span class="hljs-string">&apos;rdy&apos;</span>)</span><br><span class="line">ax1.plot(data1, color=<span class="hljs-string">&apos;tab:blue&apos;</span>,label=<span class="hljs-string">&apos;rc/rdy&apos;</span>)</span><br><span class="line">ax1.legend(loc=<span class="hljs-string">&apos;upper left&apos;</span>)</span><br><span class="line">ax2 = ax1.twinx() &#xA0;</span><br><span class="line">ax2.set_ylabel(<span class="hljs-string">&apos;rnw ra rfa&apos;</span>) </span><br><span class="line">ax2.plot(data2, color=<span class="hljs-string">&apos;tab:red&apos;</span>,label=<span class="hljs-string">&apos;rc/rnw&apos;</span>)</span><br><span class="line">ax2.plot(data4, color=<span class="hljs-string">&apos;tab:orange&apos;</span>,label=<span class="hljs-string">&apos;rc/ra&apos;</span>)</span><br><span class="line">ax2.plot(data5, color=<span class="hljs-string">&apos;tab:purple&apos;</span>,label=<span class="hljs-string">&apos;rc/rfa&apos;</span>)</span><br><span class="line">ax2.legend(loc=<span class="hljs-string">&apos;upper right&apos;</span>)</span><br><span class="line">fig.tight_layout() </span><br><span class="line">plt.show()</span><br></pre></td></tr></tbody></table></figure>
<img src="http://yumeng-li.github.io/VECM_files/VECM_15_0.png">



<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">data1 = np.log(df[<span class="hljs-string">&apos;rc&apos;</span>]/df[<span class="hljs-string">&apos;rdy&apos;</span>])</span><br><span class="line">data2 = np.log(df[<span class="hljs-string">&apos;rnw&apos;</span>]/df[<span class="hljs-string">&apos;rdy&apos;</span>])</span><br><span class="line">fig, ax1 = plt.subplots()</span><br><span class="line">color = <span class="hljs-string">&apos;tab:green&apos;</span></span><br><span class="line">ax1.set_xlabel(<span class="hljs-string">&apos;Year&apos;</span>)</span><br><span class="line">ax1.set_ylabel(<span class="hljs-string">&apos;rc/rdy&apos;</span>, color=color)</span><br><span class="line">ax1.plot(data1, color=color)</span><br><span class="line">ax1.tick_params(axis=<span class="hljs-string">&apos;y&apos;</span>, labelcolor=color)</span><br><span class="line">ax2 = ax1.twinx() &#xA0;</span><br><span class="line">color = <span class="hljs-string">&apos;tab:blue&apos;</span></span><br><span class="line">ax2.set_ylabel(<span class="hljs-string">&apos;rnw/rdy&apos;</span>, color=color) </span><br><span class="line">ax2.plot(data2, color=color)</span><br><span class="line">ax2.tick_params(axis=<span class="hljs-string">&apos;y&apos;</span>, labelcolor=color)</span><br><span class="line">fig.tight_layout()</span><br><span class="line">ax1.axvline(x=datetime(<span class="hljs-number">1975</span>, <span class="hljs-number">10</span>, <span class="hljs-number">1</span>), color=<span class="hljs-string">&apos;k&apos;</span>)</span><br><span class="line">ax1.axvline(x=datetime(<span class="hljs-number">2008</span>, <span class="hljs-number">7</span>, <span class="hljs-number">1</span>), color=<span class="hljs-string">&apos;k&apos;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></tbody></table></figure>

<img src="http://yumeng-li.github.io/VECM_files/VECM_16_0.png">


<p>Are there any structural breaks in the relationship before 1975Q4 and after 2008Q3? If so, we have to create dummies to allow for them. Let&#x2019;s run a regression to further confirm it.</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Y = np.log(df[<span class="hljs-string">&apos;rc&apos;</span>]/df[<span class="hljs-string">&apos;rdy&apos;</span>])</span><br><span class="line">X = np.log(df[<span class="hljs-string">&apos;rnw&apos;</span>]/df[<span class="hljs-string">&apos;rdy&apos;</span>])</span><br><span class="line">X = sm.add_constant(X)</span><br><span class="line">model = sm.OLS(Y,X)</span><br><span class="line">res = model.fit()</span><br></pre></td></tr></tbody></table></figure>


<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">ax = res.resid[:<span class="hljs-number">-8</span>].plot(style=<span class="hljs-string">&apos;.&apos;</span>,grid=<span class="hljs-literal">True</span>)</span><br><span class="line">ax.xaxis.grid(<span class="hljs-literal">True</span>, linestyle=<span class="hljs-string">&apos;--&apos;</span>, linewidth=<span class="hljs-number">0.25</span>)</span><br><span class="line">ax.axvline(x=datetime(<span class="hljs-number">1975</span>, <span class="hljs-number">10</span>, <span class="hljs-number">1</span>), color=<span class="hljs-string">&apos;k&apos;</span>)</span><br><span class="line">ax.axvline(x=datetime(<span class="hljs-number">2008</span>, <span class="hljs-number">7</span>, <span class="hljs-number">1</span>), color=<span class="hljs-string">&apos;k&apos;</span>)</span><br></pre></td></tr></tbody></table></figure>




<img src="http://yumeng-li.github.io/VECM_files/VECM_19_1.png">



<p>Both the time plot and the regression of log(rc/rdy) on a constant and log(rnw/rdy) suggests that there were structural breaks around 1975Q4 and 2008Q3. Notice the consistently negative residuals till 1975Q4 and the wider gap between log(rc/rdy) and log(rnw/rdy) before 1976 compared to the gap after 1976. Another one is that household net worth and assets have increased at an accelerating rate after the 2008 global financial crisis (increasing rnw/rdy ratio). While at the same time, we don&#x2019;t see a rising trend of consumption compared to income even though households have accumulated higher real net wealth benefited from the booming asset prices post-crisis, which is a quite different behavior compared to pre-crisis.</p>
<p>These are strong suggestive evidence that structural breaks occurred around 1975Q4 and 2008Q3. <strong>But why? What are the drivers?</strong></p>
<h3 id="Inequality-and-the-Kondratiev-theory"><a href="#Inequality-and-the-Kondratiev-theory" class="headerlink" title="Inequality and the Kondratiev theory"></a>Inequality and the Kondratiev theory</h3><p>Let&#x2019;s now take a look at the inequality data, which will give us better clues to our questions. </p>
<p>Data is downloaded from the World Inequality Database <a target="_blank" rel="noopener" href="https://wid.world/data/">WID</a>. Buckets I selected are top 10%, middle 40% and the bottom 50% of the US population.</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">income_inequality = pd.read_csv(<span class="hljs-string">&apos;income_inequality.csv&apos;</span>,index_col=<span class="hljs-number">0</span>)</span><br><span class="line">wealth_inequality = pd.read_csv(<span class="hljs-string">&apos;wealth_inequality.csv&apos;</span>,index_col=<span class="hljs-number">0</span>)</span><br><span class="line">pretax_income = pd.read_csv(<span class="hljs-string">&apos;pretax_income.csv&apos;</span>,index_col=<span class="hljs-number">0</span>)</span><br><span class="line">net_wealth = pd.read_csv(<span class="hljs-string">&apos;net_wealth.csv&apos;</span>,index_col=<span class="hljs-number">0</span>)</span><br></pre></td></tr></tbody></table></figure>

<p>As we can tell from the following graphs, income inequality in the US has risen dramatically since the mid 1970s, see how the income of the top 10% grows whereas that of the middle and bottom classes remains nearly flat. This issue has drawn heightened attention in recent years (including me :D). </p>
<p>In the past decade, economic observers have also become increasingly worried about &#x201C;secular stagnation&#x201D; - or a chronic shortfall of aggregate demand, fearing that this shortfall will constrain American economic growth in coming years. These two phenomena&#x2014;<strong>rising inequality</strong> and <strong>chronic weakness of demand</strong> - are related. Specifically, since the <strong>marginal propensity to consume (MPC)</strong> is much lower at the higher wealth quintiles (for low-wealth households, the MPC is <strong>10 times larger</strong> than it is for wealthy households, see this <a target="_blank" rel="noopener" href="https://www.bostonfed.org/publications/research-department-working-paper/2019/estimating-the-marginal-propensity-to-consume-using-the-distributions-income-consumption-wealth.aspx">paper</a> elaborating it), rising inequality transfers income from high MPC households in the bottom and middle of the income distribution to lower MPC households at the top. All else equal, this redistribution away from low- to high-saving households reduces consumption spending, which drags on demand growth.</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pretax_income.iloc[:,<span class="hljs-number">1</span>:].plot(title=<span class="hljs-string">&apos;Pre-tax Income&apos;</span>)</span><br></pre></td></tr></tbody></table></figure>




<img src="http://yumeng-li.github.io/VECM_files/VECM_26_1.png">




<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">net_wealth.iloc[:,<span class="hljs-number">1</span>:].plot(title=<span class="hljs-string">&apos;Net Wealth&apos;</span>)</span><br></pre></td></tr></tbody></table></figure>




<p>&#xA0; &#xA0; &lt;matplotlib.axes._subplots.AxesSubplot at 0x23c76c39e48&gt;</p>
<img src="http://yumeng-li.github.io/VECM_files/VECM_27_1.png">



<p>Due to the differences in income growth, we see a larger and larger wealth gap between groups.</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">data1 = wealth_inequality[<span class="hljs-string">&apos;sw_p99p100&apos;</span>]</span><br><span class="line">data2 = wealth_inequality[<span class="hljs-string">&apos;sw_p90p100&apos;</span>]</span><br><span class="line">data3 = wealth_inequality[<span class="hljs-string">&apos;sw_p50p90&apos;</span>]</span><br><span class="line">data4 = wealth_inequality[<span class="hljs-string">&apos;sw_p0p50&apos;</span>]</span><br><span class="line"></span><br><span class="line">fig, ax1 = plt.subplots()</span><br><span class="line">color = <span class="hljs-string">&apos;tab:blue&apos;</span></span><br><span class="line">ax1.set_xlabel(<span class="hljs-string">&apos;Year&apos;</span>)</span><br><span class="line">ax1.set_ylabel(<span class="hljs-string">&apos;Rich&apos;</span>, color=color)</span><br><span class="line">ax1.plot(data1, color=<span class="hljs-string">&apos;royalblue&apos;</span>,label=<span class="hljs-string">&apos;Share of p99p100&apos;</span>)</span><br><span class="line">ax1.plot(data2, color=<span class="hljs-string">&apos;dodgerblue&apos;</span>,label=<span class="hljs-string">&apos;Share of p90p100&apos;</span>)</span><br><span class="line">ax1.tick_params(axis=<span class="hljs-string">&apos;y&apos;</span>, labelcolor=color)</span><br><span class="line">ax1.legend(loc =<span class="hljs-string">&apos;center left&apos;</span>)</span><br><span class="line">ax1.set_title(<span class="hljs-string">&apos;Wealth Inequality&apos;</span>)</span><br><span class="line">ax2 = ax1.twinx() &#xA0;</span><br><span class="line">color = <span class="hljs-string">&apos;tab:green&apos;</span></span><br><span class="line">ax2.set_ylabel(<span class="hljs-string">&apos;Poor&apos;</span>, color=color) </span><br><span class="line">ax2.plot(data3, color=<span class="hljs-string">&apos;limegreen&apos;</span>,label=<span class="hljs-string">&apos;Share of p50p90&apos;</span>)</span><br><span class="line">ax2.plot(data4, color=<span class="hljs-string">&apos;lightgreen&apos;</span>,label=<span class="hljs-string">&apos;Share of p0p50&apos;</span>)</span><br><span class="line">ax2.tick_params(axis=<span class="hljs-string">&apos;y&apos;</span>, labelcolor=color)</span><br><span class="line">fig.tight_layout() </span><br><span class="line">ax2.legend(loc =<span class="hljs-string">&apos;center right&apos;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></tbody></table></figure>

<img src="http://yumeng-li.github.io/VECM_files/VECM_29_0.png">


<p>Based on the theory, we can assume that there is a negative correlation between income inequality and consumption. Here I created an inequality indicator using the difference in shares of wealth of the top 10% and the bottom 50%, so that we can further look into the change of inequality over the last half century. The long-term trend is estimated using the <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Hodrick%E2%80%93Prescott_filter"><strong>Hodrick&#x2013;Prescott filter</strong></a>, which can smooth the data by getting rid of the short-term fluctuations (shorter cycles).</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">wealth_inequality.index = pd.to_datetime(wealth_inequality.index, format=<span class="hljs-string">&apos;%Y&apos;</span>)</span><br><span class="line"><span class="hljs-comment"># create a variable nw_inequlity for the wealth gap between top 10% and bottom 50%</span></span><br><span class="line">wealth_inequality[<span class="hljs-string">&apos;nw_inequality&apos;</span>] = wealth_inequality[<span class="hljs-string">&apos;sw_p90p100&apos;</span>]-wealth_inequality[<span class="hljs-string">&apos;sw_p50p90&apos;</span>]</span><br><span class="line"><span class="hljs-comment"># apply the Hodrick-Prescott filter to smooth the inequality data and estimate the long-term trend</span></span><br><span class="line">wealth_inequality[<span class="hljs-string">&apos;f_nw_inequality&apos;</span>] = pd.DataFrame(filters.hp_filter.hpfilter(wealth_inequality[<span class="hljs-string">&apos;nw_inequality&apos;</span>],lamb=<span class="hljs-number">6.25</span>)[<span class="hljs-number">1</span>])</span><br></pre></td></tr></tbody></table></figure>


<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">f, (ax1, ax2) = plt.subplots(<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,figsize=(<span class="hljs-number">15</span>,<span class="hljs-number">4</span>))</span><br><span class="line">ax1.plot(wealth_inequality[<span class="hljs-string">&apos;nw_inequality&apos;</span>])</span><br><span class="line">ax2.plot(wealth_inequality[<span class="hljs-string">&apos;f_nw_inequality&apos;</span>])</span><br><span class="line">ax1.set_title(<span class="hljs-string">&apos;nw inequality&apos;</span>)</span><br><span class="line">ax2.set_title(<span class="hljs-string">&apos;filtered nw inequality (trend)&apos;</span>)</span><br></pre></td></tr></tbody></table></figure>


<img src="http://yumeng-li.github.io/VECM_files/VECM_32_1.png">



<p>Seeing the long-term inequity trend, it&#x2019;s no wonder why we have identified two structural breaks. </p>
<p>Actually up to now, if you are familiar with the theory of Kondratieff cycles, this probably starts to look quite interesting to you. <strong>Kondratiev waves</strong> (also called super-cycles, great surges, long waves, K-waves or the long economic cycle) are hypothesized cycle-like phenomena in the modern world economy. The phenomenon is closely connected with the technology life cycle. It is stated that the period of a wave ranges from forty to sixty years, the cycles consist of alternating intervals of high sectoral growth and intervals of relatively slow growth. </p>
<p>Though there is a lack of agreement about both the cause of the waves and the start and end years of particular waves, <strong>inequity appears to be the most obvious driver</strong>, and yet some researchers have presented a technological and credit cycle explanation as well. However, among several modern timing versions of the cycles based on any of the causes, the consensus is that the start of our current cycle - Wave of the Information and Telecommunications Revolution falls into the range of early 1970s to mid 1980s, which happens to be the first break point we identified.</p>
<p>Every wave of innovations lasts approximately until the profits from the new innovation or sector fall to the level of other, older, more traditional sectors. It is a situation when the new technology, which originally increased a capacity to utilize new sources from nature, reached its limits and it is not possible to overcome this limit without an application of another new technology. For the end of an application phase of any wave there is typically an economic crisis and economic stagnation. The financial crisis of 2007&#x2013;2008 is viewed by some as a result of the coming end of the Wave of the Information and Telecommunications Revolution, our second break point.</p>
<p>So after all, I create two dummy variables below to represent the turning points of the cycle.</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">df[<span class="hljs-string">&apos;sb_1975_4&apos;</span>] = np.where(df.index &lt; <span class="hljs-string">&apos;1976-10-1&apos;</span>,<span class="hljs-number">0</span>,<span class="hljs-number">1</span>)</span><br><span class="line">df[<span class="hljs-string">&apos;sb_2008_3&apos;</span>] = np.where(df.index &lt; <span class="hljs-string">&apos;2008-07-1&apos;</span>,<span class="hljs-number">0</span>,<span class="hljs-number">1</span>)</span><br></pre></td></tr></tbody></table></figure>

<h2 id="Long-run-Model-Specification"><a href="#Long-run-Model-Specification" class="headerlink" title="Long-run Model Specification"></a>Long-run Model Specification</h2><p>Now let&#x2019;s put aside the fun ideas and start model development with specifying the long-run predictive equation for real consumption.</p>
<p>Consider the following long-run model for U.S. real consumption:</p>
<p>\begin{align}<br>\log rc_t &amp;= \beta_0 + \beta_1 \log rdy_t + \beta_1 \log rnw_t + \epsilon_t<br>\end{align}</p>
<p>It&#x2019;s helpful to express our model in logarithmic form, so that the parameter estimates are elasticities.</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">df[<span class="hljs-string">&apos;ln_rc&apos;</span>] = np.log(df[<span class="hljs-string">&apos;rc&apos;</span>])</span><br><span class="line">df[<span class="hljs-string">&apos;ln_rdy&apos;</span>] = np.log(df[<span class="hljs-string">&apos;rdy&apos;</span>])</span><br><span class="line">df[<span class="hljs-string">&apos;ln_rnw&apos;</span>] = np.log(df[<span class="hljs-string">&apos;rnw&apos;</span>])</span><br><span class="line">df[<span class="hljs-string">&apos;const&apos;</span>] = <span class="hljs-number">1</span></span><br></pre></td></tr></tbody></table></figure>

<h3 id="Sample-and-Forecast-Period"><a href="#Sample-and-Forecast-Period" class="headerlink" title="Sample and Forecast Period"></a>Sample and Forecast Period</h3><p>The sample period for model specification i.e., specifying the preferred model for forecasting purposes can be 1962:1 to 2016:4 (and no later than 2007:4). Remember we have to stick with the train data from now on for the model development. The test set will be preserved for testing the predictive power of the model.</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cut = <span class="hljs-number">220</span></span><br><span class="line">train,test = df[:cut], df[cut:]</span><br><span class="line">train.shape,test.shape</span><br></pre></td></tr></tbody></table></figure>




<p>&#xA0; &#xA0; ((220, 39), (18, 39))</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train.tail()</span><br></pre></td></tr></tbody></table></figure>




<div>
<style scoped>
  .dataframe tbody tr th:only-of-type {
    vertical-align: middle;
  }

<p>  .dataframe tbody tr th {<br>    vertical-align: top;<br>  }</p>
<p>  .dataframe thead th {<br>    text-align: right;<br>  }<br></style><p></p>

&#xA0; 
&#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; 
&#xA0; 
&#xA0; 
&#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; 
&#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; 
&#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; 
&#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; 
&#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; &#xA0; 
&#xA0; &#xA0; 
&#xA0; <table border="1" class="dataframe"><thead><tr style="text-align: right;"><th></th><th>saving_rate</th><th>ns</th><th>nnw</th><th>nl</th><th>na</th><th>nfa</th><th>interest_payments</th><th>cpi</th><th>c_deflator</th><th>dff</th><th>...</th><th>rnw</th><th>rl</th><th>ra</th><th>rfa</th><th>sb_1975_4</th><th>sb_2008_3</th><th>ln_rc</th><th>ln_rdy</th><th>ln_rnw</th><th>const</th></tr></thead><tbody><tr><th>2015-10-01</th><td>7.4</td><td>1026.972</td><td>89638.832</td><td>14191.141</td><td>104202.900</td><td>72619.083</td><td>271.856</td><td>237.837</td><td>1.03286</td><td>0.16</td><td>...</td><td>86787.010824</td><td>13739.655907</td><td>100887.729218</td><td>70308.737873</td><td>1</td><td>1</td><td>9.392924</td><td>9.505978</td><td>11.371212</td><td>1</td></tr><tr><th>2016-01-01</th><td>7.5</td><td>1046.172</td><td>90792.562</td><td>14181.444</td><td>105350.157</td><td>73464.821</td><td>266.871</td><td>237.689</td><td>1.03340</td><td>0.36</td><td>...</td><td>87858.101413</td><td>13723.092704</td><td>101945.187730</td><td>71090.401587</td><td>1</td><td>1</td><td>9.400231</td><td>9.513525</td><td>11.383478</td><td>1</td></tr><tr><th>2016-04-01</th><td>6.9</td><td>969.635</td><td>92170.184</td><td>14313.771</td><td>106862.978</td><td>74353.858</td><td>271.273</td><td>239.590</td><td>1.03989</td><td>0.37</td><td>...</td><td>88634.551731</td><td>13764.697228</td><td>102763.732702</td><td>71501.656906</td><td>1</td><td>1</td><td>9.405301</td><td>9.511885</td><td>11.392277</td><td>1</td></tr><tr><th>2016-07-01</th><td>6.8</td><td>961.218</td><td>93991.425</td><td>14497.803</td><td>108869.331</td><td>75700.352</td><td>274.959</td><td>240.607</td><td>1.04379</td><td>0.39</td><td>...</td><td>90048.213721</td><td>13889.578363</td><td>104301.948668</td><td>72524.503971</td><td>1</td><td>1</td><td>9.411142</td><td>9.516705</td><td>11.408101</td><td>1</td></tr><tr><th>2016-10-01</th><td>6.8</td><td>974.320</td><td>94743.726</td><td>14601.767</td><td>109726.272</td><td>76056.719</td><td>277.958</td><td>242.135</td><td>1.04872</td><td>0.45</td><td>...</td><td>90342.251507</td><td>13923.418072</td><td>104628.758868</td><td>72523.379930</td><td>1</td><td>1</td><td>9.415978</td><td>9.522002</td><td>11.411361</td><td>1</td></tr></tbody>
</table>
<p>5 rows &#xD7; 39 columns</p>
</div>



<h2 id="Check-for-Stationary"><a href="#Check-for-Stationary" class="headerlink" title="Check for Stationary"></a>Check for Stationary</h2><p>I&#x2019;m going to pre-test the data to determine whether if it has a unit root, that is, whether it&#x2019;s stationary or non-stationary.</p>
<h3 id="1-What-is-stationarity"><a href="#1-What-is-stationarity" class="headerlink" title="1. What is stationarity?"></a>1. What is stationarity?</h3><p>A stationary process is a stochastic process whose probability distribution does not change over time. For our work in time series analysis, we mostly care about stationarity in its weak form - covariance stationary.</p>
<h4 id="Covariance-stationary"><a href="#Covariance-stationary" class="headerlink" title="Covariance stationary:"></a>Covariance stationary:</h4><ul>
<li>Constant mean:<br>\begin{align}<br>&amp;E(y_t) = E(y_{t+1}) = \mu<br>\end{align}</li>
<li>Constant variance:<br>\begin{align}<br>&amp;Var(y_t) = Var(y_{t+1}) = \sigma^2<br>\end{align}</li>
<li>Covariance depends on time that has elapsed between observations, not on reference period:<br>\begin{align}<br>&amp;Cov(y_t,y_{t+j}) = Cov(y_s,y_{s+j}) = \gamma_j<br>\end{align}</li>
</ul>
<p>To get an intuition of stationarity, one can imagine a frictionless pendulum. It swings back and forth in an oscillatory motion, yet the amplitude and frequency remain constant. Although the pendulum is moving, the process is stationary as its &#x201C;statistics&#x201D; are constant (frequency and amplitude). However, if a force were to be applied to the pendulum, either the frequency or amplitude would change, thus making the process non-stationary. </p>
<h3 id="2-Why-is-stationarity-important"><a href="#2-Why-is-stationarity-important" class="headerlink" title="2. Why is stationarity important?"></a>2. Why is stationarity important?</h3><p>Stationarity is one important assumption underlying in time series analysis as if the series is non-stationary, the shocks do not die out, the impact of disturbances become more influential over time and so gives a poor forecast accuracy.</p>
<h4 id="Consequences-of-non-stationarity"><a href="#Consequences-of-non-stationarity" class="headerlink" title="Consequences of non-stationarity"></a>Consequences of non-stationarity</h4><ul>
<li>The effect of a shock will diminish as time elapses</li>
<li>Statistical consequences<br>&#xA0; &#xA0; &#xA0;- Non-normal distribution of test statistics<br>&#xA0; &#xA0; &#xA0;- Bias in autoregressive coefficients; we might mistakenly estimate an AR(1), deficient forecast<br>&#xA0; &#xA0; &#xA0;- Usual confidence intervals for coefficients not valid, poor forecast ability</li>
</ul>
<h3 id="3-How-do-we-determine-whether-a-time-series-is-nonstationary"><a href="#3-How-do-we-determine-whether-a-time-series-is-nonstationary" class="headerlink" title="3. How do we determine whether a time series is nonstationary?"></a>3. How do we determine whether a time series is nonstationary?</h3><p>As always, visual inspection is one of the most useful ways to identify non stationary series. It looks that all of the curves obviously have a trend, which is definitely against the definition of stationarity.</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train[[<span class="hljs-string">&apos;ln_rc&apos;</span>,<span class="hljs-string">&apos;ln_rdy&apos;</span>,<span class="hljs-string">&apos;ln_rnw&apos;</span>]].plot()</span><br></pre></td></tr></tbody></table></figure>




<p>&#xA0; &#xA0; &lt;matplotlib.axes._subplots.AxesSubplot at 0x23c0007c248&gt;</p>
<img src="http://yumeng-li.github.io/VECM_files/VECM_44_1.png">



<p>Then we can turn to statistical tests. The first would be <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Augmented_Dickey%E2%80%93Fuller_test"><strong>Augmented Dickey-Fuller (ADF) Test</strong></a>, which tests for non-stationarity. </p>
<p>Recall AR(1) model:<br>\begin{align}<br>y_t = &#xA0;by{t_1} + \epsilon_t<br>\end{align}</p>
<p>A special case is the random walk when $b = 1$. However, for stationarity, it requires $b&lt;1$. </p>
<p>If generalizing to AR(p), it means that roots of the polynomial below must all be &gt;1 in abs value,<br>\begin{align}<br>1 - b_1z -b_2z^2-b_3z^3-&#x2026;-b_pz^p<br>\end{align}</p>
<p>If one of the roots = 1, then it is said to have a <strong>unit root</strong> i.e., non-stationary.</p>
<p>The ADF Test can test for the coefficient on $y_{t-1}$. Basically, it regresses y on its lag testing for significance of coefficient. I won&#x2019;t go into great mathematical details, if anyone&#x2019;s interested, check out this <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Augmented_Dickey%E2%80%93Fuller_test">wiki page</a>.</p>
<p>However, ADF has been found to have low power in certain circumstances:</p>
<ul>
<li>stationary processes with near-unit roots. For example, it has difficulty distinguishing between b = 1 and b = 0.95, especially with small samples.</li>
<li>Trend stationary processes.</li>
</ul>
<p>So alternative tests have been designed, which are <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Phillips%E2%80%93Perron_test"><strong>Phillips&#x2013;Perron (PP) Test</strong></a> and <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/KPSS_test"><strong>Kwiatkowski&#x2013;Phillips&#x2013;Schmidt&#x2013;Shin (KPSS)Test</strong></a>. I usually prefer to use all three of them together, but here I would focus on ADF and KPSS as they have been implemented in statsmodels.</p>
<p>A few notes:<br>&#xA0;- ADF and PP are called <strong>unit root tests</strong>; the null hypothesis is that $y_t$ has a unit root; is I(1) or higher.<br>&#xA0;- KPSS, on the other hand, is a <strong>stationarity test</strong>; the null hypothesis is that $y_t$ is I(0).<br>&#xA0;- Correct specification is key. Intercept and trend should be included when appropriate as critical values for the t-statistics will vary depending on whether they are included.<br>&#xA0;- Structural breaks can complicate matters further. That&#x2019;s why we start this blog with detecting structural breaks.</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">augmented_dickey_fuller_test</span>(<span class="hljs-params">time_series</span>):</span></span><br><span class="line">&#xA0; &#xA0; result = adfuller(time_series.values)</span><br><span class="line">&#xA0; &#xA0; print(<span class="hljs-string">&apos;ADF Statistic: %f&apos;</span> % result[<span class="hljs-number">0</span>])</span><br><span class="line">&#xA0; &#xA0; print(<span class="hljs-string">&apos;ADF p-value: %f&apos;</span> % result[<span class="hljs-number">1</span>])</span><br><span class="line">&#xA0; &#xA0; <span class="hljs-keyword">if</span> result[<span class="hljs-number">1</span>]&lt;<span class="hljs-number">0.05</span>:</span><br><span class="line">&#xA0; &#xA0; &#xA0; &#xA0; print(<span class="hljs-string">&apos;stationary - null hypothesis of a unit root can be rejected at a 5% significance level&apos;</span>)</span><br><span class="line">&#xA0; &#xA0; <span class="hljs-keyword">else</span>:</span><br><span class="line">&#xA0; &#xA0; &#xA0; &#xA0; print(<span class="hljs-string">&apos;non-stationary - null hypothesis of a unit root cannot be rejected at a 5% significance level&apos;</span>)</span><br><span class="line"></span><br><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">kpss_test</span>(<span class="hljs-params">timeseries</span>):</span></span><br><span class="line">&#xA0; &#xA0; result = kpss(timeseries, regression=<span class="hljs-string">&quot;c&quot;</span>)</span><br><span class="line">&#xA0; &#xA0; print(<span class="hljs-string">&apos;KPSS Statistic: %f&apos;</span> % result[<span class="hljs-number">0</span>])</span><br><span class="line">&#xA0; &#xA0; print(<span class="hljs-string">&apos;KPSS p-value: %f&apos;</span> % result[<span class="hljs-number">1</span>])</span><br><span class="line">&#xA0; &#xA0; <span class="hljs-keyword">if</span> result[<span class="hljs-number">1</span>]&gt;=<span class="hljs-number">0.05</span>:</span><br><span class="line">&#xA0; &#xA0; &#xA0; &#xA0; print(<span class="hljs-string">&apos;stationary - null hypothesis of stationary cannot be rejected at a 5% significance level&apos;</span>)</span><br><span class="line">&#xA0; &#xA0; <span class="hljs-keyword">else</span>:</span><br><span class="line">&#xA0; &#xA0; &#xA0; &#xA0; print(<span class="hljs-string">&apos;non-stationary - null hypothesis of stationary can be rejected at a 5% significance level&apos;</span>)</span><br></pre></td></tr></tbody></table></figure>


<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">augmented_dickey_fuller_test(train[<span class="hljs-string">&apos;ln_rc&apos;</span>])</span><br><span class="line">kpss_test(train[<span class="hljs-string">&apos;ln_rc&apos;</span>])</span><br></pre></td></tr></tbody></table></figure>

<p>&#xA0; &#xA0; ADF Statistic: -1.949803<br>&#xA0; &#xA0; ADF p-value: 0.309007<br>&#xA0; &#xA0; non-stationary - null hypothesis of a unit root cannot be rejected at a 5% significance level<br>&#xA0; &#xA0; KPSS Statistic: 2.297233<br>&#xA0; &#xA0; KPSS p-value: 0.010000<br>&#xA0; &#xA0; non-stationary - null hypothesis of stationary can be rejected at a 5% significance level</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">augmented_dickey_fuller_test(train[<span class="hljs-string">&apos;ln_rdy&apos;</span>])</span><br><span class="line">kpss_test(train[<span class="hljs-string">&apos;ln_rdy&apos;</span>])</span><br></pre></td></tr></tbody></table></figure>

<p>&#xA0; &#xA0; ADF Statistic: -2.970069<br>&#xA0; &#xA0; ADF p-value: 0.037786<br>&#xA0; &#xA0; stationary - null hypothesis of a unit root can be rejected at a 5% significance level<br>&#xA0; &#xA0; KPSS Statistic: 2.292161<br>&#xA0; &#xA0; KPSS p-value: 0.010000<br>&#xA0; &#xA0; non-stationary - null hypothesis of stationary can be rejected at a 5% significance level</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">augmented_dickey_fuller_test(train[<span class="hljs-string">&apos;ln_rnw&apos;</span>])</span><br><span class="line">kpss_test(train[<span class="hljs-string">&apos;ln_rnw&apos;</span>])</span><br></pre></td></tr></tbody></table></figure>

<p>&#xA0; &#xA0; ADF Statistic: -0.266563<br>&#xA0; &#xA0; ADF p-value: 0.930108<br>&#xA0; &#xA0; non-stationary - null hypothesis of a unit root cannot be rejected at a 5% significance level<br>&#xA0; &#xA0; KPSS Statistic: 2.300497<br>&#xA0; &#xA0; KPSS p-value: 0.010000<br>&#xA0; &#xA0; non-stationary - null hypothesis of stationary can be rejected at a 5% significance level</p>
<p>None of the variables can be considered stationary with the two tests. We can therefore ask whether the variables form a cointegrated system with a given number of &#x201C;common trends&#x201D;. <strong>Intuitively, I would argue that there exists one cointegration equation among the three variables as in the long run, one&#x2019;s amount consumed and wealth accumulated should be equal to the income that one earns.</strong> </p>
<p>We will discuss how to test for cointegration relationships and how to estimate them using a vector error correction model in the next blog.</p>
</body></html>
    
    </div>
    
    
</article>




    
    
    </div>
</section>
    <footer class="footer">
    <div class="container">
        <div class="columns content">
            <div class="column is-narrow has-text-centered">
                &copy; 2025 Yumeng Li&nbsp;
                Powered by <a href="http://hexo.io/" target="_blank">Hexo</a> & <a
                        target="_blank" rel="noopener" href="http://github.com/ppoffice/hexo-theme-minos">Minos</a>
            </div>
            <div class="column is-hidden-mobile"></div>

            
            <div class="column is-narrow">
                <div class="columns is-mobile is-multiline is-centered">
                
                    
                <a class="column is-narrow has-text-black" title="GitHub" target="_blank" rel="noopener" href="https://github.com/ppoffice/hexo-theme-minos">
                    
                    GitHub
                    
                </a>
                
                </div>
            </div>
            
            
        </div>
    </div>
</footer>
    <script src="//cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/moment.js/2.22.2/moment-with-locales.min.js"></script>

<!-- test if the browser is outdated -->
<div id="outdated">
    <h6>Your browser is out-of-date!</h6>
    <p>Update your browser to view this website correctly. <a id="btnUpdateBrowser" target="_blank" rel="noopener" href="http://outdatedbrowser.com/">Update my browser now </a></p>
    <p class="last"><a href="#" id="btnCloseUpdateBrowser" title="Close">&times;</a></p>
</div>
<script src="//cdnjs.cloudflare.com/ajax/libs/outdated-browser/1.1.5/outdatedbrowser.min.js"></script>
<script>
    $(document).ready(function () {
        // plugin function, place inside DOM ready function
        outdatedBrowser({
            bgColor: '#f25648',
            color: '#ffffff',
            lowerThan: 'flex'
        })
    });
</script>

<script>
    window.FontAwesomeConfig = {
        searchPseudoElements: true
    }
    moment.locale("en-AU");
</script>


    
    
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML"></script>
<script>
    MathJax.Hub.Config({
        "HTML-CSS": {
            matchFontHeight: false
        },
        SVG: {
            matchFontHeight: false
        },
        CommonHTML: {
            matchFontHeight: false
        },
        tex2jax: {
            inlineMath: [
                ['$','$'],
                ['\\(','\\)']
            ]
        }
    });
</script>

    
    
    
    
<script src="//cdnjs.cloudflare.com/ajax/libs/lightgallery/1.6.8/js/lightgallery-all.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/justifiedGallery/3.6.5/js/jquery.justifiedGallery.min.js"></script>
<script>
    (function ($) {
        $(document).ready(function () {
            if (typeof($.fn.lightGallery) === 'function') {
                $('.article.gallery').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof($.fn.justifiedGallery) === 'function') {
                $('.justified-gallery').justifiedGallery();
            }
        });
    })(jQuery);
</script>

    
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.0/clipboard.min.js"></script>
    <style>
        .hljs {
            position: relative;
        }

        .hljs .clipboard-btn {
            float: right;
            color: #9a9a9a;
            background: none;
            border: none;
            cursor: pointer;
        }

        .hljs .clipboard-btn:hover {
          color: #8a8a8a;
        }

        .hljs > .clipboard-btn {
            display: none;
            position: absolute;
            right: 4px;
            top: 4px;
        }

        .hljs:hover > .clipboard-btn {
            display: inline;
        }

        .hljs > figcaption > .clipboard-btn {
            margin-right: 4px;
        }
    </style>
    <script>
      $(document).ready(function () {
        $('figure.hljs').each(function(i, figure) {
          var codeId = 'code-' + i;
          var code = figure.querySelector('.code');
          var copyButton = $('<button>Copy <i class="far fa-clipboard"></i></button>');
          code.id = codeId;
          copyButton.addClass('clipboard-btn');
          copyButton.attr('data-clipboard-target-id', codeId);

          var figcaption = figure.querySelector('figcaption');

          if (figcaption) {
            figcaption.append(copyButton[0]);
          } else {
            figure.prepend(copyButton[0]);
          }
        })

        var clipboard = new ClipboardJS('.clipboard-btn', {
          target: function(trigger) {
            return document.getElementById(trigger.getAttribute('data-clipboard-target-id'));
          }
        });
        clipboard.on('success', function(e) {
          e.clearSelection();
        })
      })
    </script>

    
    

    



<script src="/js/script.js"></script>


    
    <div class="searchbox ins-search">
    <div class="searchbox-mask"></div>
    <div class="searchbox-container ins-search-container">
        <div class="searchbox-input-wrapper">
            <input type="text" class="searchbox-input ins-search-input" placeholder="Type something..." />
            <span class="searchbox-close ins-close ins-selectable"><i class="fa fa-times-circle"></i></span>
        </div>
        <div class="searchbox-result-wrapper ins-section-wrapper">
            <div class="ins-section-container"></div>
        </div>
    </div>
</div>
<script>
    (function (window) {
        var INSIGHT_CONFIG = {
            TRANSLATION: {
                POSTS: 'Posts',
                PAGES: 'Pages',
                CATEGORIES: 'Categories',
                TAGS: 'Tags',
                UNTITLED: '(Untitled)',
            },
            CONTENT_URL: '/content.json',
        };
        window.INSIGHT_CONFIG = INSIGHT_CONFIG;
    })(window);
</script>

<script src="/js/insight.js"></script>

    
</body>
</html>